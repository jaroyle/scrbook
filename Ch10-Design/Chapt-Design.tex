\chapter{
Sampling Design
}
\markboth{Sampling design}{}
\label{chapt.design}

\vspace{.3in}


Statistical design is recognized as an important component of animal
population studies \citep{morrison_etal:2008,
  williams_etal:2002}. There are probably few to no field biologists
who have never been in the situation where a problem with data could
be traced back to some flaw in study design.  Commonly, design is
thought of in terms of number of samples to take, when to sample,
methods of capture, desired sample size (of individuals), power of
tests, and related considerations.  In the context of spatial sampling
problems, where populations of mobile animals are sampled by an array
of traps or devices, there are a number of critical design elements.
Two of the most important ones are the spacing and configuration of
traps (or sampling devices) within the array.  While conceptual and
heuristic design considerations have been addressed by a number of
authors (e.g., \citealp{nichols_karanth:2002}, Chapt. 11), little
formal analysis focused on spatial design of arrays has been carried
out. \citet{bondrup-nielsen:1983} investigated the effect of trapping
grid size (relative to animal home range area) on density estimates using a simulation
study and some authors have addressed trap spacing and configuration
by sensitivity ``re-analysis'' (deleting traps and reanalyzing;
\citealp{wegge_etal:2004, tobler_etal:2008}). The scarcity of
simulation-based studies looking at study design issues is surprising,
as it seems natural to evaluate prescribed designs by Monte Carlo
simulation in terms of their accuracy and precision.

Theoretically optimal (or extremely good) designs are useful to study
in order to understand how factors influence the design
problem. However, often field studies face logistic difficulties,
especially when dealing with wide-ranging, rare and cryptic species
like large carnivores. The need to sample large areas with limited
resources often forces researchers to compromise between a study
design that is optimized for data analysis and a study design that is
logistically viable. In such studies, traps are often placed along
roads or rivers where a large number of traps can be accessed with
relative ease. Unless the study site is crossed by a network of roads
or waterways, this will lead to a sampling design that is much more
'linear' compared to an area-based design where traps are spread out
uniformly over the study site.

In this chapter we recommend a general framework for evaluating
specific design choices for SCR studies based on Monte Carlo
simulation of specific design scenarios based on trade-offs between
available effort, funding, logistics and other practical
considerations -- what we call {\it scenario analysis}.  Many study
design-related issues such as how long to survey in order to obtain
sufficient data, can be addressed with preliminary field studies that
will give you an idea of how much data you can expect to collect
within a unit of effort (a camera trap day or a point count survey,
for example).  But it is also always useful to perform scenario
analysis based on simulation before conducting the actual field survey
not only to evaluate the design in terms of its ability to generate
useful estimates of things, but also so that you have an expectation
of what the data will look like as they are being collected. This
gives you the ability to recognize some pathologies and possibly
intervene to resolve issues before they render a whole study
worthless. Suppose you design a study to place 40 camera traps based
on your expectations of parameter values you obtained from a careful
review of the literature, and simulation studies suggest that you
should get 3-5 captures of individuals per night of sampling. In the
field you find that you're realizing 0 or 1 captures per night and
therefore you have the ability to sit down and immediately question
your initial assumptions and possibly take some remedial
action. Simulation evaluation of design {\it a priori} is therefore a
critical element of any field study.

%Yet another design that has been
%proposed for regular capture-recapture studies is that of a trapping
%web (DESCRIBE). {\bf XXXX Maybe not? XXXX} In the second part of this
%chapter we look into the performance of these trap arrangements.


While we recommend scenario analysis as a general tool to understand
your {\it expected data} before carrying out a spatial
capture-recapture study, it is possible to develop some heuristics and
even analytic results related to the
broader problem of model-based spatial design
\citep{muller:2007} using an explicit objective function based on the
inference objective.



\section{General Considerations}

Many biologists have experience with the design of natural resource
surveys from a classical perspective
\citep{thompson:2002,cochran:2007}, a key feature of which involves
sampling space. That is, we identify a sample frame comprised of
spatial units and we sample randomly (or by some other method, such as
generalized random tesselation stratified (GRTS) sampling \citep{stevens_olsen:2004})
those units and measure some attribute. The resulting inference
applies to the attribute of the sample frame. There are some distinct
aspects of the design of SCR studies which many people struggle with
in their attempts to reconcile SCR design with classical survey design
problems.

\begin{comment}
\subsection{Model-based not design-based}

{\bf XXX ANDY: I find this little subsection hard to understand; maybe it needs some mroe elaborating. I really like the following one though. XXXXXX

XXX Reply: Andy sez: I agree, not worth going here.
}
We take a model-based approach to the design of SCR studies. That is,
we seek design features -- spatial arrangements of samples -- which we
expect to produce estimators of model parameters that have a
satisfactory desired level of precision.

Classical finite-population sampling is often ``design-based'' which
means properties of estimators (bias, variance) are evaluated over
repeated realizations of the {\it sample}. The sample is random, but
the attribute being observed is not. However, in the SCR modeling
framework properties of our estimators are distinctly model-based.  We
evaluate estimators (usually) or care only about a {\it fixed} sample,
averaged over realizations of the underlying process and data we might
generate. This is a classical parametric frequentist idea which we
think makes as much sense as a Bayesian too.
\end{comment}



\subsection{Sampling space or sampling individuals?}

A fundamental question in any sampling problem is what is the sample
frame -- or the population we are hoping to extrapolate too? In the
context of capture-recapture studies, it is tempting to think of the
sample frame as being spatial (the space within ``the study area'',
tiled into quadrats perhaps).  Clearly SCR models involve a type of
spatial sampling -- we have to identify spatial locations for traps,
or arrays of traps.  However, unlike conventional natural resource
sampling the attribute we measure is {\it not} directly relevant to
the {\it sample location}, such as where we place a trap and,
therefore, it many not be sensible to think of the sample frame as
being comprised of spatial units.  On the other hand,
capture-recapture studies clearly obtain a sample of {\it individuals}
and SCR models are models of {\it individual} encounter and space use.
Therefore, it is more natural to think of the sample frame as a list
of $N$ individuals, deterimined by the definition of the state-space,
or a subset of the state-space, i.e., the study-area, but the number
$N$ is unknown.
%You can think of it this way: the study
%area deterimines the collection of individuals that we care about for
%management or whatever, but additional individuals might be exposed to
%sampling.

Spatial sampling in SCR studies is important, but only as a device for
accumulating individuals in the sample from which we can learn about
their inclusion probability. That is, we're not interested in any
sample unit attribute directly but, rather, we use spatial units as a
means for sampling individuals and obtaining individual level
encounter histories that indicate the different sample locations at
which each individual is encountered.  It makes sense in this context
that we should want to choose a set of spatial sample units that
provides an adequate sample size of individuals, perhaps as many as
possible. The key technical consideration as it relates to spatial
sampling and SCR is that arbitrary selection of sample units has a
side-effect that it induces unequal probabilities of inclusion into
the sample and so we must also learn about these unequal probabilities
of sample inclusion as we obtain our sample.

The fact that SCR sampling induces unequal probabilities of sampling
is consistent with the classical sampling idea of Horvitz-Thompson
estimation (see
\url{http://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator})
  which has motivated capture-recapture models similar to SCR
  \citep{huggins:1989, alho:1990}.  In the Horvitz-Thompson framework,
  the sample inclusion probabilities are usually fixed and
  known. However, in all real animal sampling problems they are
  unknown because we never know precisely where each individual lives
  and therefore cannot characterize its encounter probability.
  Therefore, we have to estimate the sample inclusion probabilities
  using a model.  SCR models achieve this effect formally, using a
  fully model based approach based on a model that accounts for the
  organization of individual activity centers and trap locations.
  This notion of Horvitz-Thompson estimation suggests that perhaps we
  should consider designing SCR studies based on the H-T variance
  estimator as a design criterion. We discuss this a little bit later
  in this chapter.

\subsection{Scope of inference vs. state-space}

In SCR models we make a distinction between the scope of inference --
the population we care about -- and the state-space of the point
process, which we are required to prescribe in order to fit models.
These are not the same thing. The geographic scope of inference is the
region within which animals live that you care about in your study --
let's call this ``the study area''.  This is often prescribed for
political reasons or legal reasons (e.g. a National Park). To intitiate a study, or perhaps
motivating the study, you have to draw a line on a map to delineate a
study area, although often it is difficult to draw this line, and
where you draw it is not so much a statistical/SCR issue.  On the
other hand, you need to prescribe the state-space to define and fit an
SCR model. This is the region that contains individuals that you {\it
  might} capture. This is different from the study area in most cases.

It is helpful to think about this distinction operationally. We define
our study area a priori.  As a conceptual device, we might think of
this as the area that, given an infinite amount of resources, we might
wall-off so that we can study a real closed population. This 'study
area' should exist independent of any model or estimator of some
population quantity. i.e., the subject-matter context should determine
what the study area is. Given a well-defined study area, we use some
method to arrange data collecting devices within this study area. The
method of arrangement can be completely arbitrary but, naturally, we
want to choose arrangements of traps that are better in terms of
obtaining statistical information from the data we wind up collecting.

Lets face it -- It's quite a nuisance that animals move around and
this makes the idea of a spatial study area kind of meaningless in
terms of management in most cases. Wherever you draw a line on a map,
there will be animals who live mostly beyond that line that will
sometimes be subjected to your study.  One of the benefits of SCR
models is they formalize the exposure and contribution of these
individuals to your study. That is a good thing. Thus, you can
probably be a bit sloppy or practical in your definition of ``the
study area'' and not worry too much.



\section{Study design for (spatial) capture-recapture}

The importance of adequate trap spacing and overall configuration of
the trapping array has long been discussed in the capture-recapture
literature.  A heuristic based on recognizing the importance of
typical home range sizes \citep{dice:1938, dice:1941} and thus being
able to obtain information about home range size is that traps should
be spaced such that the array of available traps exposes as many
individuals as possible but, at the same time, individuals should be
captureable in multiple traps. Thus, good designs should generate a
high sample size $n$ and a large number of spatial recaptures.  These
two considerations trade-off in building designs.  On one hand, having
a lot of traps very close together should produce the most spatial
recaptures but produce very few unique individuals captured (assuming
that studies are limited in the total number of sampling devices they
can deploy). On the other hand, spreading the traps out as much as
possible, in a nearly systematic or regular design, should yield the
most unique individuals.  We will formalize this trade-off later, when
we consider formal model-based design of SCR studies.




Traditional CR models require that all individuals in the study area
have a probability $>$ 0 of being captured, which means that the trap
array must not contain holes large enough to contain an animal's
entire home range \citep{otis_etal:1978}. As a consequence, trap
spacing should be on the same order as the radius of a typical home
range (e.g., \citealp{dillon_kelly:2007}).  For example, imagine a
camera trap study implemented in South America with the objective to
survey populations of both jaguars and the much smaller
ocelots. Ocelots also have much smaller home ranges and therefore
should require closer trap spacing than the large wide-ranging
jaguars.  Where approaches such as MMDM are used in combination with
traditional CR models to obtain density estimates (see
Chapt. \ref{chapt.closed}), trap spacing also has a major effect on
movement estimates, since it determines the resolution of the
information on individual movement \citep{parmenter_etal:2003,
  wilson_anderson:1985b}. If trap spacing is too wide, there is little
to no information on animal movement because most animals will only be
captured at one trap \citep{dillon_kelly:2007}. In addition, only a
trapping grid that is large relative to individual movement can
capture the full extent of such movements, and researchers have
suggested that the grid size should be at least four times that of
individual home ranges to avoid positive bias in estimates of density
\citep{bondrup-nielsen:1983}.  This recommendation originated in small
mammal trapping, and it should be relatively easy to follow when
dealing with species covering home ranges $<$ 1ha. However, translated
to large mammal research, this can entail having to cover several
thousands of square kilometers -- a logistical and financial challenge
probably few projects could realistically tackle.

Holes in the study area are of no concern in SCR studies.  As a
practical matter, some animals within the study area might have
vanishingly small probability of being included in the sample, i.e.,
$p \approx 0$, the nice thing about SCR models is that $N$ is
explicitly tied to the state-space, and not the traps which expose
them to encounter.  Within an SCR model, extending inference from the
sample to individuals that live in these holes represents an
extrapolation (prediction of the model outside the range of the data),
but one that the model is capable of producing because we have
explicit declarations, in the model, that it applies to any area
within the state-space (the state-space is a part of the model!), even
to areas where we can't capture individuals because we happened to not
put a trap near them. Conversely, classical capture-recapture models
only apply to individuals that have encounter probability that is
consistent with the model being considered. Presumably, the existence
of a hole in the trap array would introduce individuals with $p=0$,
which is not accommodated in those models.





Whereas traditional CR studies are concerned with the number of
individuals and recaptures and with satisfying the model assumption of
all individuals having some probability of being captured, in spatial
capture-recapture we are looking at an additional level of
information: We need spatially spread out captures and
recaptures. That means, it is not enough to recapture an individual,
but we need to recapture at least some individuals at several
traps. Therefore, in general, design of SCR studies boils down to
obtaining three bits of information: total captures of unique
individuals, gross recaptures informative about baseline encounter
rate, and spatial recaptures, informative about $\sigma$. Most SCR
design choices wind up trading these three things against each other
to achieve some optimal (or good) mix. So for example if we sample a
very small number of sites a huge number of times then we can get a
lot of recaptures but only very few spatial ones, and few unique
individuals etc.  This need for spatial recaptures may appear as an
additional constraint on study design, but actually, SCR studies are
much less restricted than traditional CR studies, because of the way
animal movement is incorporated into the model: $\sigma$ is estimated
as a specified function of the ancillary spatial information collected
in the survey and the capture frequencies at those locations and this
function is able to make a prediction across distances even when these
are latent, including distances larger than the extent of the trap
array. When there is enough data across at least some range of
distances, the model will do well at making predictions at unobserved
distances. The key here is that there needs to be 'enough data across
some range of distances', which induces some constraint on how large
our overall trap array must be to provide this range of distances
(e.g., \citealp{marques_etal:2011}). We will review the flexibility of
SCR models in terms of trap spacing and trapping grid size in the
following section.



\section{Trap spacing and array size relative to animal movement}

{\bf
XXXX Does Efford have anything out there with discussions of trap
spacing? XXXXX
}

Using a simulation study, \citet{sollmann_etal:2012} investigated how
trap spacing and array size relative to animal movement influence SCR
parameter estimates and we will summarize this study here. They
simulated encounter histories on an $8 \times 8$ trap array with
regular spacing of 2 units, using a Binomial encounter model with
Gaussian hazard encounter model (complementary log-log link), across a
range of values for the movement parameter $\sigma^*$. We refer to the
movement parameter as $\sigma^*$ here, because
\citet{sollmann_etal:2012} use a slightly different parametrization of
SCR models, in which $\sigma^*$ corresponds to $\sigma\times\sqrt{2}$.

{\bf XXXX ANDY: I am totally blanking here: We formulated the model so
  that the distance function was d2/sig2. That means $\sigma^*$ =
  $\sigma\times\sqrt{2}$, right? It was different before but I think
  now it's right. If I got it wrong, let me know and I'll fix
  it. Sorry!! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX }
In Sec. \ref{scr0.sec.implied} we pointed out that under the bivariate normal (or half-normal) detection model $\sigma$ can be converted into an estimate of the 95 \% home range or ``use area'' around $s_i$. Based on this transformation, values for $\sigma^*$ were chosen so that there was a scenario where
the trap array was smaller than a single individual's home range, i.e. trap spacing was narrow relative to individual movements
($\sigma^*$ = 5), a scenario where spaces between traps were large
enough to contain entire home ranges ($\sigma^*$ = 0.5), and two
intermediate scenarios and where sigma was smaller ($\sigma^*$ =1
unit) and larger ($\sigma^*$ = 2.5 units) than the trap spacing,
respectively.  $N$ was 100 and the baseline trap encounter rate
$\lambda_0$ was 0.5 for all four scenarios, and trap encounters were
generated over 4 occasions. Table \ref{design.tab.simres} shows the
results as the average over 100 simulations.

 \begin{table}[ht]
  \centering
  \caption{
Mean, relative root mean squared error (rrmse) of the mean, mode, 2.5 \% and 97.5 \% quantiles, relative bias of mean (RB) and 95BCI coverage (BCI) for spatial capture-recapture parameters across 100 simulations for four simulation scenarios, define by the input value of movement parameter $\sigma^*$. $N$ = number of individuals in the state space; $\lambda_0$ = baseline trap encounter rate}
    \begin{tabular}{l*{7}{c}}
    \hline
    Scenario & Mean  & rrmse & Mode  & 2.5\% & 97.5\% & RB    & BCI \\     \hline
    \multicolumn{2}{ l }{\bf $\sigma^*$ = 1 ($\sigma$ = 0.71)}  & {\it } & {\it } & {\it } & {\it } & {\it } & {\it } \\
    $N$ & 108.497 & 0.172 & 104.099 & 78.977 & 143.406 & 0.085 & 96 \\
    $\lambda_0$ & 0.518 & 0.248 & 0.477 & 0.303 & 0.752 & 0.035 & 94 \\
    $\sigma^*$ & 1.008 & 0.093 & 0.990 & 0.857 & 1.195 & 0.008 & 94 \\
    \multicolumn{2}{ l }{\bf $\sigma^*$ = 2.5 ($\sigma$ = 1.77)} & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } \\
    $N$ & 100.267 & 0.105 & 98.456 & 82.086 & 121.878 & 0.003 & 97 \\
    $\lambda_0$ & 0.507 & 0.118 & 0.500 & 0.409 & 0.623 & 0.014 & 92 \\
    $\sigma^*$ & 2.501 & 0.046 & 2.491 & 2.267 & 2.690 & $<$ 0.001 & 92 \\
    \multicolumn{2}{ l }{\bf $\sigma^*$ = 5 ($\sigma$ = 3.54)} & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } \\
    $N$ & 102.859 & 0.137 & 100.756 & 77.399 & 130.020 & 0.029 & 88 \\
    $\lambda_0$ & 0.505 & 0.075 & 0.501 & 0.435 & 0.580 & 0.011 & 93 \\
    $\sigma^*$ & 5.023 & 0.039 & 5.001 & 4.687 & 5.431 & 0.005 & 97 \\ \hline
    \end{tabular}
  \label{design.tab.simres}
\end{table}


\begin{table}[ht]
  \centering
  \caption{Summary statistics of 100 simulated data sets for four simulation scenarios, defined by the input value of movement parameter $\sigma$. Individual detection histories were simulated on an 8 x 8 trap array with regular trap spacing of 2 units.}
    \begin{tabular}{l p{2.1cm} p{2.3cm}p{2.1cm}p{2.3cm}}
    \hline
    Scenario & Inds. \newline captured & Total \newline captures & Inds. \newline recaptured & Inds. captured  \newline at $>$ 1 trap \\ \hline
    {$\sigma^*$ = 0.5} & 18.29 (3.84) & 25.38 (5.86) & 5.52 (2.03) & 0.72 (0.95) \\
    {$\sigma^*$ = 1.0} & 37.70 (13.44) & 69.35 (26.05) & 19.48 (7.68) & 11.87 (5.43) \\
    {$\sigma^*$ = 2.5} & 44.19 (4.67) & 231.78 (33.98) & 36.60 (4.76) & 35.21 (4.73) \\
    {$\sigma^*$ = 5.0} & 40.51 (5.15) & 427.77 (79.09) & 33.09 (4.63)
    & 32.60 (4.76) \\ \hline
    \end{tabular}
  \label{design.tab.simdat}
\end{table}

All model parameters were identifiable and estimated with relatively
low bias ($<$ 10 \%) and high to moderate precision (rrmse $<$ 25 \%)
for all scenarios of $\sigma^*$, except $\sigma^*$ = 0.5 units
(therefore excluded from Table \ref{design.tab.simres}). Data for the
latter case mostly differed from the other scenarios in that fewer
animals were captured and very few of the captured animals were
recorded at more than 1 trap (Table \ref{design.tab.simdat}). For
$\sigma^*$ = 0.5, abundance ($N$) was not identifiable in 88 \% of the
simulations, and when identifiable, was underestimated by
approximately 50 \%. This shows that a trap spacing that is
considerably too large may be problematic in SCR studies.


Estimates of $N$ were least biased and most precise under the
$\sigma^*$ = 2.5 scenario, and in general, all parameters were
estimated best under the $\sigma^*$ = 2.5 or the $\sigma^*$ = 5
scenario. All estimates had the highest relative bias and the lowest
precision under the $\sigma^*$ = 1 scenario.  These results clearly
demonstrate that SCR models can successfully handle a range of trap
spacing to animal movement ratios, and even when using a trapping
array smaller than an average home range: at $\sigma^*$ = 5, the home
range of an individual was approximately 235 $units^2$, while the
trapping grid only covered 196 $units^2$. Still, the model performed
very well.

An important consideration in this simulation study is that all but
the $\sigma^*$ = 0.5 units scenarios provided reasonably large amounts
of data, including 20 + individuals being captured on the trapping
grid. When dealing with real-life animals that are often territorial
and may have lower trap encounter rates, a very small grid compared to
an individual's home range may result in the capture of few to no
individuals. In that case, the sparse data will limit the ability of
the model to estimate parameters (Marques et al. 2011), which is true
of most models.

In summary, SCR models performed best when $\sigma^*$ was slightly
larger than trap spacing (or in other words, when $\sigma$ was
slightly smaller) and did well as long as $\sigma^*$ was at least 0.5
times the average distance between traps (which corresponds to
$\sigma$ being 0.35 times the average distance between
traps). Although at this trap spacing to movement ratio, most
individuals are captured at one trap only (see
Tab. \ref{design.tab.simdat}), parameter estimates exhibited low bias
and remained relatively precise (see simulation results for $\sigma^*$
= 1 in Tab. \ref{design.tab.simres}). Below this trap spacing to
movement ratio the spatial information in the simulated data
apparently was not sufficient to inform SCR model parameters.


\subsection{Example: Black bears from Pictured Rocks National Lakeshore }

To see how trap array size influences parameter estimates from spatial
capture-recapture models in the real world, \citet{sollmann_etal:2012}
also looked at a black bear data set from Pictured Rocks National
Lakeshore, Michigan, collected using 123 hair snares distributed over
an area of 440 $km^2$ along the shore of Lake Superior in May-July
2005 \citep{belant_etal:2005}.  The SCR model for the bear data
allowed for differences in the baseline trap encounter rate and
$\sigma^*$ between males and females, and $\lambda_0$ varied across
occasions. This was motivated by a) the lower average number of
detections for male bears and b) the decreasing number of detections
over time in the raw data, and c) the fact that male black bears are
known to move over larger areas than females (e.g.,
\citealp{gardner_etal:2010jwm, koehler_pierce:2003}).

To address the impact of a smaller trap array on the parameter
estimates, the full data set and data subsets were analyzed with SCR
models. The first subset retained only those 50 \% of the traps
closest to the grid center. In the second, only the southern 20 \% of
the traps were retained \ref{design.tab.bears}.

\begin{table}[ht]
  \centering
  \caption{Posterior summaries of SCR model parameters for black bears.}
    \begin{tabular}{lcccc}
%    \addlinespace
	\hline
          & Mean (SE) & Mode  & 2.5\% & 97.5\% \\ \hline
    {\bf Full data set} &       &       &       &       \\
    {\it D } & 10.556 (1.076) & 10.448 & 8.594 & 12.792 \\
    $\sigma^*$ (males) & 7.451 (0.496) & 7.323 & 6.579 & 8.495 \\
    $\sigma^*$ (females) & 2.935 (0.143) & 2.939 & 2.671 & 3.226 \\
    {\bf 50\% of traps} &       &       &       &      \\
    {\it D } & 12.648 (1.838) & 12.205 & 9.307 & 16.713 \\
    $\sigma^*$ (males)  & 5.354 (0.511) & 5.248 & 4.472 & 6.473  \\
    $\sigma^*$ (females) & 3.318  (0.277) & 3.262 & 2.841 & 3.910 \\
    {\bf 20\% of traps} &       &       &       &        \\
    {\it D } & 6.752 (1.611) & 5.953 & 4.000 & 10.218  \\
    $\sigma^*$(males)  & 9.881 (3.572) & 7.566 & 5.121 & 18.447 \\
    $\sigma^*$ (females)  & 2.686 (0.391) & 2.657 & 2.121 & 3.404  \\
    \end{tabular}
  \label{design.tab.bears}
\end{table}

Reducing the area of the trap array by 50 \% created a grid polygon of
144 $km^2$, which was smaller than an estimated male black bear home
range and only 50 \% larger than a female black bear home range -
approximately 260 $km^2$ and 100 $km^2$, respectively, when converting
estimates of $\sigma^*$ to home range size. Table
\ref{design.tab.bears} shows that this did not greatly influence model
results, compared to the full data set. The observed smaller
differences in parameter estimates may be due to individual
differences in detection and movement that manifest themselves when
only a smaller portion of the overall population is sampled. By
reducing the number of traps we effectively reduced the size of the
overall data set estimates were based on (both in terms of individuals
captured and recaptures). This was reflected in overall higher SE and
wider confidence intervals. In spite of these differences, density
estimates -– the main objective of applying SCR models -–
remained largely constant. Removing 80 \% of the traps and thereby
reducing the area of the trap array to 64 $km^2$ - well below the
average black bear home range - had a great effect on sample size
(only 25 of the original 83 individuals sampled) and parameter
estimates. Particularly, male black bear movement was overestimated
and imprecise. The combination of the low baseline trap encounter rate
of males and the considerable reduction in sample size led to a low
level of information on male movement: 5 of the 12 males were captured
at one trap only. Although they moved over smaller areas, owing to
their higher trap encounter rate females were, on average, captured at
more traps (3.4 traps per individual compared to 2.6 for males) so
that their movement estimate remained relatively
accurate. Overestimated male movements and female trap encounter rates
resulted in an underestimate of density of almost 40 \%. This effect
is contrary to what we would expect to see in non-spatial CR models,
where too small an area leads to underestimated movement and
overestimated density \citep{bondrup-nielsen:1983, dillon_kelly:2007,
  maffei_noss:2008}. While this example again demonstrates the ability
of SCR models to deal with a range of trapping grid sizes, it also
clearly shows that your study design needs to consider the amount of
data you can expect to collect.

\subsection {Final musings: SCR models, trap spacing and array size}

When designing a capture-recapture study for a single species, trap
spacing and the size of the array can (and should) be tailored to the
spatial behavior of that species to ensure adequate data
collection. However, some trapping devices like camera traps may
collect data on more than one species and researchers may want to
analyze these data, too. Independent of the trapping device used,
study design will in most cases face a limit in terms of the number of
traps available or logistically manageable. As a consequence,
researchers need to find the best compromise between trap spacing and
the overall grid area and the following section will develop an
approach to derive such an optimal study design.

Particularly for large mammal research SCR models have much more
realistic requirements in terms of area coverage than non-spatial CR
models, under which density estimates can be largely inflated with
small trapping grids relative to individual movement
\citep{maffei_noss:2008}. How large the spatial survey effort needs to
be does not only depend on the extent of movement of the target
species, but also on the temporal effort, density and detection
probability \citep{marques_etal:2011} -- in summary, the amount of
data that can be collected with any given trap array. For low-density
species, like the black bears in the above example, small trapping
grids bear the risk of not collecting enough data for parameter
estimation. Simulation studies can help you assess how effective a
certain study design is given a set of parameters. Alternatively,
\citet{efford_etal:2009ecol} provide a mathematical procedure to
determine the expected number of individuals captured and recaptures
for a given detector array and set of model parameters.

Overall, while there are limits to the flexibility in spatial trap
array design for SCR modeling, the method is fairly robust to changes
in trap array size and spacing relative to animal movement. Trapping
grids with an extent of approximately a home range diameter can –
in theory - adequately estimated density and home range size. However,
these results should not encourage researchers to design non-invasive
trap arrays based on minimum area and spacing requirements. Study
design should still strive to expose as many individuals as possible
to sampling and obtain adequate data on individual movement. Large
amounts of data do not only improve precision of parameter estimates
(the density estimate for the full black bear data set has narrower
confidence intervals than estimates from the reduced data sets), they
also allow including potentially important covariates (such as gender
or time effects in the black bear example) into SCR models to obtain
density estimates that reflect the actual state of the studied
population.

\section{ Spacing of traps with telemetered individuals}
\label{design.sec.telemetry}

In Chapt. \ref{chapt.rsf} we discussed SCR models that integrate
 auxiliary information on resource selection obtained by
 telemetry. Telemetry data are directly informative about the distance
 parameter ($\sigma$ or $\alpha_{1}$ as the case may be). It stands to
 reason that, when telemetry data are available, it should affect
 considerations related to trap spacing. Conceivably even, one might
 be able to build SCR designs that don't yield any formal spatial
 recaptures because all of the information about $\sigma$ is provided
 by the telemetry data. 



\section {Sampling Over Large Areas}

Trap spacing is an essential aspect of design of SCR studies. However,
it is only the most important aspect if one can uniformly cover a
study area with traps.   In many practical situations where the study
area is large relative to effort that can be expended, one has to
consider other strategies which deviate from a strict focus on trap
spacing. There are two general strategies that have been suggested
which we think are useful in practice, either by themselves or
combined: Sampling based on {\it clusters} of traps and sampling based
on {\it rotating} groups of traps over the landscape.

Karanth and Nichols talk about moving traps around in the green book........

Efford (unpublished) looked at clusters.....

In practice, employing both of these strategies might be necessary.

Work on formalizing and generalizing these ideas is needed.  We
believe the model-based spatial design approach, which we introduce
below, is the way to do that.






\section{Model-Based Spatial Design}

A point we have stressed in previous chapters is that SCR models are
basically glorified versions of generalized linear models (GLMs) with
a random effect that represents a latent spatial attribute of
individuals, the activity center or home
range center.  This formulation makes analysis of the models readily
accessible in freely available software and also allows us to adapt
and use concepts from this broad class of models to solve problems in
spatial capture recapture. In particular, we can exploit
well-established model-based design concepts (Kiefer 1959; Box and
Draper 1959, 1987; Fedorov 1972; Sacks et al. 1989; Hardin and Sloane
1993; Fedorov and Hackl 1997) to develop a framework for designing
spatial trapping arrays for capture-recapture studies.
M$\ddot{u}$ller (2007) provides a recent monograph level treatment of the subject
that is very readable.

In the following sections, we adapt these classical methods for
constructing optimal designs to obtain the configuration of traps (or
sampling devices) in some region (the design space, ${\cal X}$), that
minimizes some appropriate objective function based on a compromise
between the variance of estimating $N$ for a prescribed
state-sdpace. We show that this criterion -- based on the variance of
an estimator of $N$ -- represents a formal compromise between
minimizing the variance of the MLEs of the detection model parameters
and obtaining a {\it high} expected probability of capture.
Intuitively, if our only objective was to minimize the variance of
paramter estimates than all of our traps should be in one or a small
number of clusters where we can recapture a small number of
individuals many times each. Conversely, if our objective was only to
maxinimize the expected probability of encounter then the array should
be highly uniform so as to maximize the number of individuals being
exposed to capture.  By seeking to minimize the variance of of an
estimator of $N$, our objective function is, formally, a compromise
between these two objectives and the resulting designs are not always
highly regular nor clustered.

% Existing theory (Sanathanan 1972) suggests that such designs
%should also be optimal for estimating density or abundance.

\subsection{Formalization of the Design Problem for SCR Studies}

Let ${\cal X}$, the {\it design space}, denote some region within
which sampling could occur and let ${\bf X} = {\bf x}_{1},\ldots, {\bf
  x}_{J}$ denote the {\it design}, the set of sample locations (e.g.,
of camera traps), normally we just call these ``traps.''
The design space ${\cal X}$ must be prescribed (a priori).
Operationally, we could equate ${\cal X}$ to the study area itself
(which is of management interest) but, in practical cases, there will
be parts of the study area that we cannot sample. Those areas need to
be excluded from ${\cal X}$.
While ${\cal X}$ may be
continuous, in practice it will be sufficient to represent ${\cal X}$
by a discrete collection of points which is what we do here.  This is especially convenient
when the geometry of ${\cal X}$ is complicated and irregular, which
would be the case 
in most practical applications).
The
technical problem addressed subsequently is how do we choose
the locations ${\bf X}$ in a manner that is statistically efficient for
estimating abundance or density, or perhaps some other variance-based
criterion?  

As usual, we regard the population of $N$ such individual ``activity
centers'' as the outcome of a point process distributed independently
over the state-space ${\cal S}$.  The importance of ${\cal S}$ is
obvious as it defines a population of individuals (i.e., activity
centers) and, in practice, it is not usually the same as ${\cal X}$
due to the fact that animals move freely over the landscape and the
location of traps is typically restricted by policies, ownership and
other considerations.  The objective we pursue here is: Given (1)
${\cal X}$, (2) a number of design points, $J$; (3) the state-space
${\cal S}$, and (4) an SCR model, and (5) a design criterion $Q({\bf
  X})$, we want to choose {\it which} $J$ design points we should
select in order to obtain the {\it optimal} design under the chosen
model, where the optimality is with respect to $Q({\bf X})$. We will
describe some possible choices for $Q({\bf X})$ below, but it makes
sense that they should relate to the variance of estimators of one or
more parameters of the SCR model.


To motivate the approach we're going to take in a simplified
suppose we know ${\bf
  s}$ for an individual. In this case, its vector of counts of
encounter in each trap ${\bf y}$ are either binomial or Poisson
counts, and the model has
\[
g( \mathbb{E}({\bf y})  ) =  \alpha_0  + \alpha_1 ||{\bf x}-{\bf s}||
\]
Lets think about this in the context of a normal linear model then:
\[
{\bf y} = {\bf M}({\bf X},{\bf s})'{\bm \alpha} + \mbox{error}
\]
We could analyze the design problem for the binomial or Poisson case
but to establish basic ideas here lets just look at the normal model.
The variance-covariance matrix of $\hat{\bm \alpha}$ is, supressing
the dependence on ${\bf X}$, is:
\[
 \mbox{Var}( {\bm \alpha}) = ({\bold M}({\bf s})'{\bold M}({\bf s}))^{-1}
\]
Therefore if we know {\it all} $N$ values of ${\bf s}$ we could now
easily find the design ${\bf X}$ that optimizes some function of the
variance-covariance matrix, whatever function we want.  If we don't
know ${\bf s}$ then we might as well minimize the expected variance:
\[
  E_{{\bf s}}\left\{ \mbox{Var}({\bm \alpha}) \right\} = \sum_{s \in
    {\cal S}}  ({\bf M}'({\bf s}){\bf M}({\bf s}))^{-1}
\]
This can be done for any number of design points ${\bf x}_{1},\ldots,
{\bf x}_{J}$ using imperfect exchange algorithms (Sec. \ref{design.sec.exchange})
which always improve the criterion but will not necessarily yield {\it
  the} optimal design. But it is usually good enough for practice.

It is worth noting that asymptotic formulae for
$\mbox{Var}( {\bm \alpha})$ can be cooked up fro any type of GLM
(e.g., see Mccullaugh and Nelder. .XXXXX p. XXXX) and we will see that
design for SCR models is closely related to the basic GLMs such as
binomial or poisson regression.



\subsection{An Optimal Design Criterion for SCR}

There are a number of appealing directions to pursue for deriving a
variance-based criterion upon which to devise designs for capture
recapture studies.  For one, we could formulate the problem as a
Huggins-Alho type of problem and follow along their analysis for
computing variances, and that might be fruitful. But the calculus is
a bit tedious for that.  Instead, a more fruitful area is to consider
either the Poisson or Binomial-integrated likelihoods, and then we
could possibly compute the variance-covariance matrix of the MLEs
directly.  This merits further investigation.   The easiest thing so
far has been to devise variance criteria based on 
a conditional estimator of $N$ of the form
\[
  \tilde{N}  =  \frac{n}{\bar{p}}
\]
where $\bar{p}$ is the probability that an individual appears in the
sample of $n$ unique individuals. In SCR models an individual with activity center
${\bf s}_{i}$ is captured if it is
captured in {\it any} trap and therefore, under the Bernoulli model,
\[
 \bar{p}({\bf s}_{i}, {\bf X}) = 1 - \prod_{j=1}^{J} (1- p_{ij}({\bf
   x}_{j}, {\bf s}_{i}))
\]
and, under the Poisson model, we have:
\[
 \bar{p}({\bf s}_{i},{\bf X}) = 1 -  \exp(-\lambda_{0} \sum_{j}
 \exp(\beta* d({\bf x}_{j}, {\bf s}_{i})^2 ))
\]
where here we emphasized that this is conditional on ${\bf s}_{i}$ and
also the design -- the trap locations ${\bf x}_{j}$. 
 The {\it
  marginal} probability of encounter, averaging over all possible
locations of ${\bf s}$ is:
\begin{equation}
 \bar{p}({\bf X}) = 1 - \int_{\bf s}    \bar{p}({\bf s}_{i},{\bf X})    d{\bf s}.
\label{design.eq.pbar}
\end{equation}
It is important to note that this can be calculated directly {\it
  given} the design ${\bf X}$. This is handy because we see that it is
used in the variance formulae given subsequently.

The approach we take here is we develop the variance of $\tilde{N}$
conditional on knowing the locations of all $N$ individuals and then
we suggest to unconditon on the realized point process by taking a
Monte Carlo average over realizations of ${\bf s}$ under a suitable
model for ${\bf s}$. The variance criterion we propose here is based
on a delta approximation $Var(n/\bar{p})$:
\begin{equation}
 Var(\tilde{N}(\alpha) ) = 
\frac{N^{2} Var(\bar{p})}{\bar{p}^{2}}  + N\frac{(1-\bar{p})}{\bar{p}}
\label{design.eq.theQ}
\end{equation}
This is a really intuitive-looking criterion upon which to base
designs. In particular, it is the sum of two components
which are 
essentially those due to (1) estimation of $\bar{p}$ from the sample
and (2) the variance of $n$. We see that generally this criterion is improved
(decreases) as we do a better job estimating $\bar{p}$ and also as $n$
approaches $N$, i.e., as $\bar{p}$ increases to 1. Thus, good designs
should generate information about detection probability {\it and}
produce large samples of individuals.  The other way to look at this
is that the variance of estimating $N$ is due to the variance of
estimating $\bar{p}$ with a {\it penalty} due to have a low value of
$\bar{p}$ (the 2nd term being the penalty, which increases as
$\bar{p}$ goes to 0).  We suggest, therefore, that designs for
capture-recapture studies should seek to minimize
Eq. \ref{design.eq.theQ}, or perhaps generalizations of this to
account for other features of the model. 



In order to work with this expersssion we will
have to do some analysis of $\mbox{Var}(\bar{p})$ which we take up now.
We note that $\bar{p}$ is itself a deterministic function of the
parameters that we need to estimate, ${\bm \alpha}$. Therefore, as a
general rule,  we could use
a delta approximation to  express $\mbox{Var}(\bar{p})$ in terms of the variance of
the MLE $\hat{\bm \alpha}$.  However, we stated above that our intent
is to work with the Poisson observation model, and we did that because
the technical argument that follows is somewhat easier for that
case. In particular, we first express the integral in Eq. 
\ref{design.eq.pbar} by a summation over a fine mesh of points so that:
\[
 \bar{p}({\bf X}) = \sum_{{\bf s}} 1 -    \bar{p}({\bf s}_{i},{\bf X})   
\]
which under the Poisson model is, in a simplified notation:
\[
 \bar{p}({\bf X}) = \sum_{{\bf s}} \left\{  
1 -  \exp(-\lambda_{0} \sum_{j}
 exp(\beta* d({\bf x}_{j}, {\bf s})^2))
\right\}
\]
To compute the variance of this expression, we note that the variance
operature can move inside the summation over ${\bf s}$, and the
subtraction from 1 doesn't count anything, so we have
\[
Var( \bar{p}({\bf X}) )  = \sum_{{\bf s}} 
 Var \left(  
 \exp(-     \sum_{j}   exp(\alpha_{0} + \alpha_{1}  d({\bf x}_{j}, {\bf s})^2))
   \right)
\]
At this point we apply the Delta approximation to produce
\[
Var( \bar{p}({\bf X}) )  = \sum_{{\bf s}}
\left( exp( - \sum_{j} exp(\alpha_{0} + \alpha_{1}  d({\bf x}_{j}, {\bf  s})^2) ) 
\right)
 Var \left(    \sum_{j}   exp(\alpha_{0} + \alpha_{1}  d({\bf x}_{j}, {\bf     s})^2)    \right)
\]
we're going to simplify things a bit and write $\lambda({\bf
  x}_{j},{\bf s}) =   exp(\alpha_{0} + \alpha_{1}  d({\bf x}_{j}, {\bf
  s})^2)$
and also $\lambda_{\bf s} = \sum_{j=1}^{J} \lambda({\bf x}_{j},{\bf
  s})$. Then
\[
Var( \bar{p}({\bf X}) )  = \sum_{{\bf s}}
\left( exp( - \lambda_{\bf s} )  \right)
 Var \left(    \sum_{j}   exp(\alpha_{0} + \alpha_{1}  d({\bf x}_{j}, {\bf     s})^2)    \right)
\]
We have to do a 2nd application of the Delta approximation to find
that:
\[
Var( \bar{p}({\bf X}) )  = 
\sum_{{\bf s}} \left( exp( - \lambda_{\bf s} )  \right)
\left(    \sum_{j}  \lambda({\bf x}_{j},{\bf s})^{2} Var( \hat{\alpha}_{0} + \hat{\alpha}_{1}  d({\bf x}_{j}, {\bf    s})^2)
  \right)
\]
(note: need to comment on why $\alpha$ has hats on it somewhere but
not elsewhere)
The final step is we assume $\hat{\alpha}_{0}$ and $\hat{\alpha}_{1}$
are independent which is not true in practice but makes life slightly
easier here (but is not necessary, in general). The variance of
$\bar{p}$ becomes:
\[
Var( \bar{p}({\bf X}) )  = 
\sum_{{\bf s}} \left( exp( - \lambda_{\bf s} )  \right)
\left(    \sum_{j}  \lambda({\bf x}_{j},{\bf s})^{2} (
 Var( \hat{\alpha}_{0}) +  
  d({\bf x}_{j}, {\bf    s})^4
Var( \hat{\alpha}_{1})  )
  \right)
\]



\begin{comment}
This produces:
\begin{equation}
Var(\bar{p}) =
\left( \frac{ \delta \bar{p}}{\delta \alpha_{0}},
 \frac{ \delta \bar{p}}{\delta \alpha_{1}} \right)
Var(\hat{\bm \alpha})
\left( \begin{array}{c} \frac{ \delta \bar{p}}{\delta \alpha_{0}} \\
 \frac{ \delta \bar{p}}{\delta \alpha_{1}} \end{array} \right)
\label{design.eq.varpbar}
\end{equation}
We need to break this down into its constituent pieces:
\end{comment}


The big picture is this: For a given design ${\bf X}$, we can compute
the $Var(\bar{p}({\bf X}))$ -- its just a calculation involving sum's
over all points in the state-space and design points -- provided we
know the variance of the estimator of ${\bm \alpha}$, 
$Var(\hat{\bm \alpha})$. 
However, it is not so easy to write down the analytic form of this matrix.
 Some calculus would have to be done on the
conditional likelihood (e.g., from Borchers and Efford 2008) to figure
out the asymptotic form of this matrix.  For our purposes,  a good heuristic is
to approximate the matrix, using the analogous result from a Poisson or Binomial GLM assuming
that $N$ is known, 
since we have conveneint formulas for those.  In particular, if we knew the
activity centers of all individuals then the resulting data $y(x,s)$
are Poisson counts. The asymptotic variance-covariance matrix of ${\bm
  \alpha}$ in that case is:
\begin{equation}
  \mbox{Var}(\hat{\bm \alpha}|{\bf X},{\bf s})
=  ({\bf M}({\bf s})' {\bf D}({\bm \alpha},{\bf s}){\bf M}({\bf s}))^{-1}.
\label{eq.varbeta}
\end{equation}
where ${\bf M}$ is a matrix which has a column of 1's and a column of
$N \times J$ entries that are the distances between each individual
and each trap and the matrix ${\bf D}$ is a diagonal matrix having
elements $\mbox{Var}( y_{j}|{\bf s}) = exp({\bf m}'{\bm \alpha})$ for
$y_{ij}$ the frequency of encounter in trap $j$.  Thus, the variance
is a function of the design ${\bf X}$ as well as ${\bf s}$ both of
which are balled-up in ${\bf M}$ -- the regression design matrix and
the matrix ${\bf D}$.  

This is conditional on ${\bf s}$...... what do we do?  Well , we look
at it this way: What is the {\it expected} information obtained from a
particular realization of $N$ individuals?  Clearly that should be:
\[
I(N) =  M_{1}' D_{1} M_{1} + .... M_{N}D_{N} M_{N} 
\]
so we can average this over all possible collections of $N$ values of
${\bf s}$. clearly, if {\it individual activity centers are
  independent}
 this is exactly the same as taking a single MC
average over {\it all} elements of the state-space weighted by $N$:
\[
E[{\bf I} ] = N  \sum_{{\bf s}}  M({\bf s})'D({\bf s}) M({\bf s})
\]





Therefore we have a design criterion which is obtained by plugging
$\bar{p}$ from Eq. XXXX {\it and} the variance expression
xxxx   nto Eq. \ref{design.eq.theQ}.
 This is a function of the design ${\bf  X}$ and also the ballpark
 guess of the model parameters................



\begin{comment}
{\flushleft \bf (2)}
 The derivative terms: multiple applications of the chain rule can
be used (see Huggins (1989) and Alho (1990) for relevant examples).
Under the Poisson model we have
\[
 \bar{p} = 1 - \sum_{s} exp(-\lambda_{0} \sum_{j} exp(\beta* d(x_{j}, s)))
\]
where the summation over ${\bf s}$ arises as a result of approximating the integral in Eq. XXXXX by a summation.
If we differentiate this with respect to $\lambda_{0}$ and $\beta$ we
have the following:
\[
\frac{\delta \bar{p}}{\delta \lambda_{0}} =
1- \sum_{s}
\left\{
\left( -\sum_{j} exp( \beta d_{ij}^{2} ) \right)
exp( - \lambda_{0} \sum_{j} exp( \beta d_{ij}^{2} )  )
\right\}
\]
and
\[
\frac{\delta \bar{p}}{\delta \beta} =
\left\{
\sum_{s} -\lambda_{0} \left( \sum_{j} exp( \beta d_{ij}^{2} ) \right)
\right\}
\left\{
1-
\sum_{s}  \left( -\lambda_{0} \sum_{j} exp( \beta d_{ij}^{2} ) \right)
exp\left( -\lambda_{0} \sum_{j} exp( \beta d_{ij}^{2} ) \right)
\right\}
\]
\end{comment}




















\begin{comment}
Furthermore, we emphasize that the above variance expression is
{\it conditional} on the realization ${\bf s}_{1},\ldots, {\bf s}_{N}$
which is, in the context of design, not observable.  We will therefore
develop design criteria which are unconditional on $\{ {\bf s} \}$.
The total variance expression is unconditional on ${\bf s}$ is:
\[
\mbox{Var}(\tilde{N}) = \mbox{E}_{s} \mbox{Var}(\tilde{N}|{\bf s}) +
\mbox{Var}_{s} \mbox{E}(\tilde{N}|{\bf s})
\]
if we assert that sample sizes will be large enough so that our
estimator is unbiased, then the 2nd term will be close to 0 and we can
ignore it.
Therefore to evaluate the unconditional variance we need to solve an
$N-$fold integration to average over ${\bf s}_{1},\ldots, {\bf
  s}_{N}$, or we can do this by taking
a monte carlo average.
\end{comment}





\subsection{Optimization of the criterion}
\label{design.sec.exchange}

To build spatial designs that optimize the criterion, 
we need to come up with a ballpark guess of the model
parameters so that the criterion can be evaluated. i.e., what are ${\bm
  \alpha}$ and $N$? If we do that, and specify
the state-space ${\cal S}$ then, we can, in 
theory, optimize the variance criterion over all possible
configurations of $J$ traps.

In formulating the optimization problem note that we have $J$ sample
locations corresponding to rows of ${\bf X}$.  The problem is a $2J$
dimensional optimization problem which, for $J$ small, could be solved
using standard numerical optimization algorithms as exist in almost
every statistical computation environment.  However, $J$ will almost
always be large enough so as to preclude effective use of such
algorithms. This is a common problem in experimental design, design
for response surface estimation, computer experiments, spatial
sampling designs and other disciplines for which sequential exchange
or swapping algorithms can be used (e.g., Wynn 1970;
Fedorov 1972; Mitchell 1974; Meyer and
Nachtsheim 1995). The basic idea is to pose the problem as a sequence
of 1-dimensional optimization problems in which the objective function
is optimized over 1 or several coordinates at a time.
In the present case, we consider swapping out ${\bf x}_{j}$ for some
point in ${\cal X}$ that is nearby ${\bf x}_{j}$ (e.g., a 1st order
neighbor). The objective function is evaluated for all possible swaps
(at most 4 in the case of 1st order neighbors) and whichever point
yields the biggest improvement is swapped for the current value.  The
algorithm is iterated over all $J$ design points and this continues
until convergence is achieved. Such algorithms may yield local optima
and optimization for a number of random initial designs can yield
incremental improvements. We implemented this swapping algorithm in
{\bf R}, using the basic strategy employed elsewhere (e.g., Nychka et
al. 1997; Royle and Nychka 1998).  A version of a swapping
algorithm used to optimize a space-filling criterion is implemented
in the {\bf R} package {\bf
  fields} (Fields Development Team 2006).  We developed an
  implementation that operates on 
a discrete representation of ${\cal S}$ (an aribtrary matrix of
coordinates).
% and an indicator of which elements of ${\cal S}$ are
%members of the design space ${\cal X}$. 
For each point in ${\bf X}$, only
the nearest neighbors (the number is specified) are considered for
swapping into the design during each iteration.

While swapping algorithms are convenient to implement, and efficient
at reducing the criterion in very high dimensional problems, they do
not always yield the global optimum.  In practice, as in the examples
below, it is advisable to apply the algorithm to a large number of
random starting designs.  Our experience is that essentially
meaningless improvements are realized after searching through a few
dozen random starts.



\subsection{Illustration}

Because the algorithm operates on a discrete version of ${\cal S}$, 
it is trivial to apply to situations in which the
state-space is arbitrary in extent and geometry. However, we consider
a simplified situation here in order to illustrate the calculation of
optimal designs and how they look for an intuitive situation. 


Consider designing a study for camera traps in a square region defined
by the square $[10,20] \times [10, 20]$ and with ${\cal X} = {\cal
  S}$.  For this illustration I assumed $\beta_{0} = log(\lambda_{0})
= -2.7$ and $\beta_{1} = 1/(\sigma^{2}) = 1/4$, $1/9$ and $1/16$, so
$\sigma = 2,3,4$. (this was dumb - note that $\sigma$ is really 2
times the standard deviation of a normal distribution. Oh well!).
Designs of size 9 and 10 were computed for each value of $\sigma$
using many random starting designs.  The putative optimal designs
(henceforth ``best'') are shown\footnote{My intention is to provide
  many of these results in an Appendix in order to reduce the length
  of the paper.} in Figure \ref{fig.fig1}.  For J=9, $\sigma =2$, the
best design was produced in 180 out of 1000 random starts.  For
$\sigma = 3$ (row 2, left panel) the best design was produced in about
88\% of all optimizations from random starting values.
%reason is that there are a lot of points in the interior that interact
%relatively little with the design and these ``holes'' tend to cause
%the algorithm to get caught in a local optimum (my interpretation) of
%the objective function.  Or, consider this, with sigma = small the
%design can probably be translated a little bit in space .... this is
%what I think happens.
For $J=10$, and $\sigma =2$ (row 1, right panel), the best design was
found about 24\% of the time (from random starts).
The $\sigma = 3$ best design (row 2, right
panel; 14\% of random starts) clusters 2 points in the center.
Finally, consider the $\sigma =4$ case (last row of
Fig. \ref{fig.fig1}).  We have two irregular looking designs and
the design points cluster in various ways.
% For $J=9$ this was produced
%only 1 time whereas only 8 instances of 1000 produced the
%best design for $J=10$.  We might thus have little confidence in that
%result\footnote{subsequent analyses have failed to find a better
%  design.}.

I computed the best designs using the same settings but inreasing the
size of ${\cal S}$ relative to ${\cal X}$.  In particular, I nested
${\cal X}$ into $[9,21] \times [9,21]$ (Figure \ref{fig.fig2})
 and then $[8,22]^{2}$ (Figure \ref{fig.fig3}).
The obvious effect of this is
that the best designs move points toward the edge of the design space
${\cal X}$ so as to provide more exposure to points in ${\cal S}$.
The effect is more pronounced, obviously, as you provide more area
outside of ${\cal X}$ that is allowed to influence the design.

As a final example,
consider placing 20 camera traps in this region. Where do they
go? Look at the 3 buffers, 3 values of sigma, thats 9 total designs
(use a single panel).
An interesting feature of the designs is that they are not
regular. Traps occur in clusters of several traps close together
with the clusters more widely spaced.



\section{Covariate models}

Many capture-recapture studies will involve one or more landscape or
habitat covariates that are thought to affect density, with the idea
of using the methods described in Chapt. \ref{chapt.state-space} for
modeling and inference.  We imagine that it should be possible to
extend the model-based framework described previously to accomodate
uncertainty due to having to estimate ${\bm \beta}$, and this could be
included as a feature of the design criterion. 

In this case, we can think 
of the captures in a trap being Poisson random variables with mean
$\lambda({\bf x},{\bf s})*D({\bf s})$ and we think the same arguments
as given above can be used to devise design criteria and optimzie
them. However, in this case we might not only care about estimating
$N$ but also (or instead) inference about the parameters ${\bm
  \beta}$. Thus, we might choose designs that are good for $N$ or
perhaps only good for estimating ${\bm \beta}$ or perhaps both. 
Intutively, we think these two design objectives conflict with one
another to some extent.  Model-based approaches should favor areas of
higher density, but the design pionts need to realize variation in the
landscape covariates too. 

This might be a  really good  dissertation topic. 





\section {Summary and Outlook}

Design of capture-recapture studies in the context of {\it spatial}
models is kind of an emerging field. There isn't much out there. As a
general rule, we always recommend {\it scenario analysis} by Monte
Carlo simulation. This takes a lot of time but it guarantees forward
progress.  We looked at some examples of that to assess trap
spacing.....
What is the answer?
There are other good appliations of this: Cluster type sampling. Use
of telemetered individuals.   These are good topics for an ambitious
PhD student to pursue.

Conceptually -- the information in SCR studies comes in two parts:
Recaptures of individuals at different traps (spatial recaptures) and
the total sample size of individuals. 
Maximizing both of these things as 
objectives induces a sort of trade-off. We need designs that are good for estimating
$\bar{p}$ and also designs that obtain a high sample size of
$n$. Designs that are only good for one or the other will produce bad
SCR designs, or designs in which $N$ is not estimable.
One exception is when telemetry is available. These provide
information on $\sigma$ or other parameters of the detection model and
this changes the whole situation so that trap arrays should be more
spread out.

Things to think about:
If you can saturate an area ....RMSE for estimating N as a function of
trap density..... this is important..... and trap
spacing........ should be in units of $\sigma$. We don't know anything
about trap density.
Other things that are important: In large landscapes you cannot
achieve saturation and so you have to do other things. It is necessary
to do some kind of clustering.....
Having RSF data from telemetry should affect the design problem but we
don't have a good understanding of this.
And when sampling is restricted by landscape features.....


We should always do a simulation study. this allows us to learn what
to expect as we start collecting real data.  plus we can simulate for
any complex situation that we desire.

However formal model-based design of SCR models has great potential
and we think this is where things will be going. SCR models are
amenable to some degree of analytic study using classical spatial
design ideas. We have just barely scratched the surface here, showing
how to formulate a criterion that is a function of the design, and
then  optimizing the criterion over all possible designs. We beliee
this approach merits more attention.








