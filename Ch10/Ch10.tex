\chapter{
%Modeling Animal space-usage with 
%Detection Models based on Ecological Distance
%Ecological Distance Models in Spatial Capture-Recapture
Modeling Space Usage: Ecological Distance in Spatial Capture-Recapture Models
}
\markboth{Chapter XXX}{}
\label{chapt.implicit}


\vspace{.3in}

%% this material is a general introduction for a manuscript
Spatial capture-recapture models are a relatively new class of models
for estimating animal density from capture-recapture data with
auxiliary information about individual capture locations
\citep{efford:2004,borchers_efford:2008, royle_young:2008, efford_etal:2009,
  royle_etal:2009ecol}.
The basic idea is to express encounter probability of
individuals as a function of the distance between individual center of
activity, say ${\bf s}_{i}$, and trap location, say ${\bf x}_{j}$.  
% Do we need to be clear about the i and j subscripts? ie, i=1,...,N
In these models ${\bf s}_{i}$ is regarded as a latent variable and
conventional methods of statistical inference either based on marginal
likelihood \citep{borchers_efford:2008} or Bayesian analysis by MCMC
\citep{royle_young:2008}.

While the models are a relatively recent innovation, their use is
already becoming widespread \citep{efford_etal:2009,
  gardner_etal:2010jwm, gardner_etal:2010ecol,kery_etal:2010, 
  gopalaswamy_etal:2012, foster_harmsen:2012} because they resolve
critical problems with using ordinary non-spatial capture-recapture
methods such as ill-defined area sampled, and heterogeneity in
encounter probability due to the juxtaposition of individuals with
traps, and they provide a framework for modeling of trap-specific
covariates.  Furthermore, essentially all real capture-recapture
studies produce auxiliary spatial information and therefore SCR models
are directly relevant to standard data collected from such studies.
% Indeed, the use of ordinary
%capture-recapture models essentially admits a model misspecification
%(i.e. homogeneous encounter probability) by ignoring the explicit
%spatial information.

Every application of SCR models so far has been based on encounter
probability models in which distance between individual activity
center and trap location is defined by a function of simple Euclidean
distance.  While these will often be sufficient for practical
purposes, especially in small data sets, sometimes developing more
complex models of the detection process as it relates to space usage
of individuals will be useful.  Animals may not judge distance in
terms of Euclidean distance but, rather, according to quality of local
habitat, landscape connectivity, perceived mortality risk, and other
considerations that affect movement behavior.
As an example of the potential problem of parameterizing SCR models
using Euclidean distance, imagine a study area bisected by a large
semi-permeable barrier. In traditional SCR models, the probability of
capturing an animal in a trap located on the opposite side of the
barrier would simply be a function of distance, whereas in reality it
should be a function of both distance and the permeability of the
barrier. Such situations are extremely common in capture-recapture
studies where multiple habitats occur in the study area or when
animals use linear features such as trails, corridors, or rivers.
 Moreover, because encounter probability and the distance
metric upon which it is based represent outcomes of individual
movements about their home range, ecologists might have explicit
hypotheses about how environmental variables affect the distance
metric, and it is therefore desirable to incorporate these hypotheses
directly into SCR models so that they may be formally evaluated
statistically.

In this chapter we develop a framework for modeling animal space
usage in SCR models, by parameterizing
models for encounter probability based on ``ecological distance.'' 
In particular, 
following \citet{royle_etal:2012ecol}.
we adopt a cost-weighted distance metric (the least-cost path)
used widely in landscape ecology for modeling connectivity,
movement and gene flow
\citep{adriaensen_etal:2003,manel_etal:2003,mcrae_etal:2008}. In the
context of SCR models we can use this as the basis for computing the
distance between traps and individuals activity centers. In this way
we can explicitly accommodate landscape structure and
account for how animals use space in SCR models. We develop a
likelihood-based inference framework for SCR model parameters using
this new distance metric when the ecological distance function is
known.  We show that the MLEs are approximately unbiased in moderate
sample sizes, as expected, but also that the misspecified model based
on Euclidean distance can produce substantial bias in estimates of $N$
and hence density.  Further, we extend the model to allow for likelihood
estimation of parameters of the cost function, so that direct inference
about movement and connectivity can be made from capture-recapture data without subjective prescription
of resistance or cost surfaces.

{\bf
Connection to RSPFs or something esteimated from telemetry data. Big
idea is estimating those from sparse individual encounter history data.
Say that here.
}

\section{Distance Models}


In the standard SCR model we model encounter probability as a function
of Euclidean distance. For example, using the binomial observation model
as an example (Chapt. \ref{chapt.scr0}), let 
$y_{ij}$ be individual- and trap specific counts $y_{ij}$
which are binomial counts with sample size $K$ and probabilities
$p_{ij}$. The Gaussian or ``half-normal'' model is 
\[
log(p_{ij})= \theta_{0} + \theta_{1} dist({\bf x}_{j} - {\bf s}_{i})^{2}
\]
or, equivalently, 
\[
p_{ij} = \lambda_{0} exp(-  dist({\bf x}_{j} - {\bf s}_{i})^{2}
/(2\sigma^{2}) )
\]
where $\theta_{0} = log(\lambda_{0})$ and $\theta_{1} =
-1/(2\sigma^2)$. 


In all previous applications of SCR models in this book, as well as in
the literature, the normal Euclidean
distance has been used, i.e., $ dist({\bf x}_{j} - {\bf s}_{i}) =
||{\bf x}_{j} - {\bf s}_{i}||$,
and the parameters $\theta_0$ and $\theta_1$
have been estimated using standard methods (likelihood or
Bayesian). 
The main problem with this approach is that 
the Euclidean distance metric is unaffected by habitat or landscape
structure, and it implies that the space used by individuals is 
stationary, and symmetric which may be unreasonable in most
applications. By stationary here we mean in the formal sense of invariance
to translation. That is, the properties of
an individual home range centered at some point
${\bf s}$ are exactly the same as any other point say ${\bf s}'$.

As an example, if the 
common detection model based on a bivariate normal probability
distribution function is used, then the implied space usage by {\it
  all} individuals, no matter their location in space or local habitat
conditions, is symmetric with circular contours of usage intensity
(density contours of the pdf).  

\citet{royle_etal:2012ecol} extended this class of SCR models that
accommodates alternative distance metrics that explicitly incorporate
information about the landscape so that a unit of distance is variable
depending on identified covariates. 
Thus, ``where'' an individual lives on the landscape, and
the state of the surrounding landscape, will determine the character
of its usage of space. In particular, we suggest distance metrics that
imply irregular, asymmetric and non-stationary home ranges of
individuals. An example of this is shown in Fig. \ref{fig.homeranges}
which shows home ranges of 6 individuals under a specific model, which
is described below.


\section{Cost-Weighted Distance}

We adopt the use of a cost-weighted distance metric here which defines
the distance between points by accumulating pixel-specific costs
determined under a cost function defined by the user.  The idea of
cost-weighted distance to characterize animal use of landscapes is
widely used in landscape ecology for modeling connectivity, movement
and gene flow \citep{beier_etal:2008}. As is customary for reasons of
computational tractability we consider a discrete landscape defined by
a raster of some prescribed resolution. The distance between any two
points ${\bf x}$ and ${\bf x}'$ can be represented by a sequence of
line segments connecting neighboring pixels say ${\bf l}_{1},{\bf
  l}_{2},\ldots,{\bf l}_{m}$. Then the cost-weighted distance between
${\bf x}$ and ${\bf x}'$ is

\begin{equation}
 d({\bf x},{\bf x}')
  =  \sum_{i=1}^{m-1} cost({\bf l}_{i},{\bf l}_{i+1})||{\bf l}_{i} - {\bf l}_{i+1}||
\label{eq.costweighted}
\end{equation}

{\flushleft
where } $cost({\bf l}_{i},{\bf l}_{i+1})$ is the user-defined cost function
to move
from pixel ${\bf l}_{i}$ to neighboring pixel ${\bf l}_{i}$ in the sequence.
Given the ``cost'' of each pixel, it is a simple matter to compute the
cost-weighted distance between any two pixels, along {\it any} path,
simply by accumulating the incremental  costs weighted by
distances.
In the context of
spatial capture-recapture models (and, more generally, landscape
connectivity) we are concerned with the {\it minimum} cost-weighted
distance, or the {\it least-cost path}, between any two points which
we will denote by $d_{lcp}$, which is
the
sequence ${\bf l}_{1},{\bf l}_{2},\ldots,{\bf l}_{m}$ that minimizes
the objective function defined by Eq. \ref{eq.costweighted}. That is,

\begin{equation}
 d_{lcp}({\bf x},{\bf x}')
  =  min_{{\bf l}_{1},\ldots,{\bf l}_{m}}  \sum_{i=1}^{m-1} cost({\bf l}_{i},{\bf l}_{i+1})||{\bf l}_{i} - {\bf l}_{i+1}||
\label{eq.lcp}
\end{equation}

{\flushleft
 Least-cost} path distance can be calculated in
 many geographic information systems and other software packages,
including the {\bf R} package \mbox{\tt
  gdistance} \citep{vanetten:2011}.


The key ecological aspect of least-cost path modeling is the
development
of models for pixel-specific cost.
In this paper we model cost as a function of covariates
defined on every pixel of the raster according to, for example using a
single covariate $z(x)$:

\begin{equation}
 log(cost({\bf x}))=  \theta_{2} z({\bf x})
\label{eq.cost}
\end{equation}

{\flushleft This} is interpreted as cost of moving {\it through} a
pixel but, in practice, it is accumulated incrementally as the cost of
moving from a pixel ${\bf x}$ to one of its neighbors, say ${\bf
  x}'$. As such, it is conventional to express this as an average cost

\[
 log(cost({\bf x},{\bf x}'))=  \theta_{2} \frac{z({\bf x})+z({\bf x}')}{2}
\]

{\flushleft
Thus,} if $\theta_{2} = 0$ then substituting $cost({\bf x},{\bf x}')
=exp(0) = 1$ into
Eq. \ref{eq.lcp} will produce the ordinary Euclidean distance
between points.

In practical applications, variables that influence the cost of moving
across the landscape include things like highways
\citep[e.g.,][]{epps_etal:2005}, elevation \citep{cushman_etal:2006},
ruggedness \citep{epps_etal:2007}, snow cover
\citep{schwartz_etal:2009}, distance to escape terrain
\citep{shirk_etal:2010}, or range limitations
\citep{mcrae_beier:2007}.  Together multiple environmental variables
create a resistance surface, which forms the linchpin of all
connectivity planning \citep{spear_etal:2010}.  Often $\theta_{2}$ is
fixed by the investigator. Although $\theta_{2}$ is rarely known,
conservation biologists design linkages that require this resistance
value as input \citep[see][and articles cited
therein]{beier_etal:2008}.  Typically planners pick a value based on
expert opinion \citep{beier_etal:2008}, although recently researchers
have begun to define costs based on resource selection functions,
animal movement \citep{tracy:2006, fortin_etal:2005}, or genetic
distance data (e.g., \citet{gerlach_musolf:2000};
\citet{epps_etal:2007}; \citet{schwartz_etal:2009}. The published
methods require many assumptions and these estimates have not been
integrated with capture-recapture methods, nor have parameters been
formally estimated using capture-recapture data.

To formalize the use of cost-weighted distance in SCR models, we
substitute Eq. \ref{eq.lcp} in the expression for encounter
probability (Eq. \ref{eq.encounter}) and maximize the resulting
likelihood which we address below.

\subsection{Example of Computing Cost-weighted distance}

As an example of the cost-weighted distance calculation consider the
following landscape comprised of 16 pixels with unit spacing
identified as follows, along with the pixel-specific cost:
\begin{center}
\begin{verbatim}
  pixel ID                 Cost
  1  5  9  13          100   1   1  1
  2  6 10  14          100 100   1  1
  3  7 11  15          100 100 100  1
  4  8 12  16          100 100   1  1
\end{verbatim}
\end{center}
This simple cost
raster is shown in Fig. \ref{ecoldist.fig.raster}.
We assigned low cost of 1 to ``good habitat'' pixels (or pixels
we think of as ``highly connected'' by virtue of being in good
habitat) and, conversely, we assign high cost (100) to ``bad
habitat''. So the shortest cost-weighted distance between pixels 5 and
9 in this example is just 1 unit, the shortest cost-distance between
pixels 5 and 10 is $\sqrt{2}(1+1)/2 = 1.414214$ units, the shortest
distance between pixels 4 and 8 is 100 units, while the shortest
cost-distance between 4 and 12 is 150.5. A tough one is: what is the
shortest distance between 7 and 16? An individual at pixel 7 can move
diagonal and pay $sqrt(2)*(100+1)/2 + 1 =72.41778$.  


Once the cost raster is created, the
least-cost path distances are computed with just a couple {\bf R}
commands, and those can be inserted directly into the likelihood
construction for an ordinary spatial capture-recapture model (Appendix
1). The {\bf R} package \mbox{\tt gdistance} uses the implementation
of Dijkstra's algorithm \citep{dijkstra:1959} found in the \mbox{\tt
  igraph} package \citep{csardi:2010}.  Using \mbox{\tt gdistance},
(Appendix 1), we define the incremental cost of moving from one pixel
to another as the distance-weighted {\it average} of the 2 pixel
costs.

The {\bf R} commands for computing the least-cost distance between all pairs of pixels 
are as follows:
\begin{verbatim}
r<-raster(nrows=4,ncols=4)
projection(r)<- "+proj=utm +zone=12 +datum=WGS84"
extent(r)<-c(.5,4.5,.5,4.5)
costs1<- c(100,100,100,100,1,100,100,100,1,1,100,1,1,1,1,1)
values(r)<-matrix(costs1,4,4,byrow=FALSE)
par(mfrow=c(1,1))
plot(r)
\end{verbatim}

\begin{figure}
\begin{center}
\includegraphics[height=3.25in,width=3.25in]{Ch10/figs/raster_2values}
\end{center}
\caption{a 16 pt raster}
\label{ecoldist.fig.raster}
\end{figure}

Then we use the functions \mbox{\tt transition}, \mbox{\tt
  geoCorrection} (which doesn't really do anything if the coordinate system
  is UTM)
 and \mbox{\tt costDistance} to compute the distance
matrix. The transition function computes the cost of making a transition between
any two pixels, and it operates on the inverse-scale (''conductance'') and so the
\mbox{\tt transitionFunction} argument is given as $1/mean(x)$. 

To compute the cost distance we prescribe a set of points to compute the matrix for or
we can give two sets of points (from and to)......
ANDY STOPPED HERE
Then we define a set of points,
the center points of each raster, to compute the cost-distance between
them all. The commands altogether are as follows:
\begin{verbatim}
tr1<-transition(r,transitionFunction=function(x) 1/mean(x),directions=8)
tr1CorrC<-geoCorrection(tr1,type="c",multpl=FALSE,scl=FALSE)
pts<-cbind( sort(rep(1:4,4)),rep(4:1,4))
costs1<-costDistance(tr1CorrC,pts)
outD<-as.matrix(costs1)
\end{verbatim}
Now we can look at the result and see if it makes sense to us. Here we
print the first 4 columns of this distance matrix and illustration a
couple of examples of calculating the minimum cost-weighted distance
between points:
\begin{verbatim}
> outD[1:5,1:5]
         1         2        3        4         5
1   0.0000 100.00000 200.0000 205.2426  50.50000
2 100.0000   0.00000 100.0000 200.0000  71.41778
3 200.0000 100.00000   0.0000 100.0000 171.41778
4 205.2426 200.00000 100.0000   0.0000 154.74264
5  50.5000  71.41778 171.4178 154.7426   0.00000
\end{verbatim}
The interesting one is that between point 1 and 4. Note that Euclidean 
distance, weighted by cost, is XYZ whereas the actual least-cost path has
cost-weighted distance 50.5. Looking at the graph, the shortest path has
an individual moving from X to Y to Z to blah blah.















































\section{Maximum likelihood estimation}
\label{sec.mle}

Here we develop a standard method of parameter estimation based on
the marginal likelihood. That is, the likelihood in which the latent
variables ${\bf s}$ are removed by integration \citep{borchers_efford:2008}.

The individual- and trap-specific observations have a binomial
distribution conditional on the latent variable ${\bf s}_{i}$:

\begin{equation}
	y_{ij}| {\bf s}_{i} \sim \mbox{Bin}(K, p_{\theta}(d_{lcp}({\bf x}_{j},{\bf s}_{i};\theta_{2}); \theta_{0}, \theta_{1})
\label{mle.eq.cond-on-s}
\end{equation}

{\flushleft where} we have indicated the dependence of $p_{ij}$ on the parameters
${\bm \theta}$, and also $d_{lcp}$ which
itself depends on $\theta_{2}$, and the latent variable ${\bf s}$.
%The parameters
%${\bm \theta}$ include whatever parameters are involved in the
%cost-weighted distance function, i.e., at least $\theta_{2}$ from
%Eq. \ref{eq.cost}.
For the random effect we have ${\bf s}_{i} \sim  \mbox{Unif}({\cal
  S})$.
The joint distribution of the data for individual $i$ is the product
of $J$ binomial terms (i.e., contributions from each of $J$ traps):

\[
  [{\bf y}_{i} | {\bf s}_{i} , \theta] =
  \prod_{j=1}^{J} \mbox{Bin}(K, p_{\theta}({\bf x}_{j},{\bf s}_{i}) )
\]

{\flushleft This} assumes that encounter of individual $i$ in each
trap is independent of encounter in every other trap. Conditional on
${\bf s}_{i}$ this is reasonable in most applications in our view.
 The so-called marginal likelihood is computed by removing
${\bf s}_{i}$, by integration,  from the conditional-on-${\bf s}$
likelihood and regarding the {\it marginal} distribution of the data
as the likelihood. That
is, we compute:

\[
  [y|{\bm \theta}] =
\int_{{\cal S}}  [ {\bf y}_{i} |{\bf s}_{i},{\bm \theta}] g({\bf s}_{i}) d{\bf s}_{i}
\]

{\flushleft where}, under the uniformity assumption, we have
$g({\bf s}) = 1/||{\cal S}||$.
The joint likelihood for all $N$ individuals, assuming independence of
encounters among individuals, is the product of $N$ such terms:

\[
{\cal L}({\bm \theta} | {\bf y}_{1},{\bf y}_{2},\ldots, {\bf y}_{N}) = \prod_{i=1}^{N}
[{\bf y}_{i}|{\bm \theta}]
\]

The key operation for computing the likelihood is solving the
2-dimensional integration problem to remove ${\bf s}$. There are some
general purpose {\bf R} packages that implement a number of
multi-dimensional integration routines including 
%\mbox{\tt adapt} \citep{genz_etal:2007} and  %% Not on CRAN anymore
\mbox{\tt R2Cuba} \citep{hahn_etal:2011}.
We won't rely on these extraneous {\bf R} packages but instead will
use perhaps less efficient methods in which we replace the integral
with a summation over an equal area mesh of points on the state-space
${\cal S}$ and explicitly evaluate the integrand at each point. We
invoke the rectangular rule for integration here in which the
integrand is evaluated on a regular grid of points of equal area and
then averaged.  Let $u=1,2,\ldots,nG$ index a grid of $nG$ points,
${\bf s}_{u}$, where the area of grid cell $u$ is constant.  In this
case, the integrand, i.e., the marginal pmf of ${\bf y}_{i}$, is
approximated by

\begin{equation}
         [{\bf y}_{i}|\theta] = \frac{1}{nG} \sum_{u=1}^{nG}  [ {\bf
            y}_{i} |{\bf s}_u, \theta]
\label{mle.eq.intlik}
\end{equation}

To deal with the fact that $N$ is unknown, there are two key issues
that need to be addressed.  First is that we don't observe the
``all-zero'' encounter histories (i.e., $y_{ij} = 0$ for all $j$)
corresponding to uncaptured individuals, so we have to make sure we
compute the probability for that all zero encounter history which we
do operationally by tacking a row of zeros onto the encounter history
matrix. We include the number of such all-zero encounter histories as
an unknown parameter of the model, which we label $n_{0}$.  In
addition, we have to be sure to include a combinatorial term to
account for the fact that of the $n$ observed individuals there are
${N \choose n}$ ways to realize a sample of size $n$. The
combinatorial term involves the unknown $n_{0}$ and thus it must be
included in the likelihood.

To compute the integral requires that the bounds of integration are
specified, which is equivalent to prescribing the state-space of the
underlying point process, i.e., ${\cal S}$. Given ${\cal S}$, density
is
computed as $D({\cal S}) = N/||{\cal S}||$. In our simulation study
below we report $N$ as the two are equivalent summaries of the data
set once the state-space is fixed.

We wrote an {\bf R} function to evaluate the likelihood which we optimize
using the {\bf R} function \mbox{\tt nlm}.
The {\bf R} code is given as a script in the Appendix where we also
provide functions for simulating data and carrying-out a simulation study.














ANDY STOPPED HERE

\section{Examples of Computing Cost-Distance}

In this section we provide an example that we think is typical of 
how cost-weighted distance models can be used in real
capture-recapture problems.
\begin{comment}
In particular, we will typically have a polygon coverage either in the
form of a GIS shapefile or a matrix of points or some other specific
format, and we want to put that polygon on a map and use the polygon
boundary in some way to generate pixel-specific costs. So we want to
see if points or raster pixels are in the polygon, or not, or how far
they are from the polygon boundary (cost might be related to distance)
and similar operations. In the following examples, we confront how to
do some of these operations in {\bf R}. 
\end{comment}

We define a $20 \times 20$ pixel raster with
extent $[0.5, 4.5] \times [0.5, 4.5]$.
We regard for the purposes of this example as a coarse landscape 
covariate, with (scaled) pixels having ,  say, a $2 \times 2$ km resolution. Thus, the raster
therefore
defines a landscape of $40 \times 40$ km and we suppose that 16 camera
traps are established at the integer coordinates (1,1), (1,2),
... (4,4). 
Maybe we are studying a population of ocelots or lynx or some other
spotted cat.
We suppose 
that each raster is characterized by a single covariate and we consider two specific
cases. First is an increasing trend from 
the NW to the SE, where $z(x)$ is defined as $z(x) = r(x) + c(x)$ where $r(x)$ and $c(x)$ are 
just the row and column, respectively, of the raster. 
This might define something related to distance
from an urban area or a gradient in habitat quality due to land use,
or environmental conditions. 
In the second case we make up a covariate by generating a field of spatially correlated
noise to emulate a typical patchy habitat covariate. The two covariates are shown in 
Figure XYZ. 

We define 
\[
 log(cost(x))=  \beta z(x) 
 \]
where $\beta = 1$ which we regard as known. With $\beta=0$ then the model reduces to
one in which the cost of moving across each pixel is constant, and therefore Euclidean
distance is operative.
 Of course this is a model and it is completely
unreasonable to regard $\beta$ as known but this is standard practice in essentially all
historical studies of landscape or genetic connectivity although sometimes crude methods
of estimation in specific contexts are based on comparing distance matrices of genetic
structure with those produced by simulation models so that either $\beta$ might be estimated
crudely or covariates which improve fit can be selected. REFs????
However this stuff has never been formalized in a likelihood context and especially
not for SCR models. 
Anyhow, we consider the known-$\beta$ situation first before dealing with estimating
this parameter. 



\subsection{Non-stationarity of home range structure}

When distance is defined by the cost-weighted distance metric given
by Eq. \ref{eq.lcp} then individual space-usage varies
spatially in response to the landscape covariate(s) used in the
distance metric. For example, using one of the covariates we use in
our simulation study below (Fig. \ref{ecoldist.fig.raster100}, right
panel) with a Gaussian pdf detection function but having distance
metric defined by Eq. \ref{eq.lcp}, produces home ranges such
as those shown in Fig. \ref{fig.homeranges}. Later we simulate data
under the model that produces these home ranges and fit spatial
capture-recapture models to evaluate the efficacy of likelihood
estimation under this model.

\subsection{Simulation studies}

We devised a limited simulation study to evaluate three things: (1)
the general statistical performance of the density estimator under
this new model; (2) the effect of mis-specifying the model with a
normal Euclidean distance metric and (3) the statistical performance
of estimating the relative cost parameter.

We used population sizes of 100 and 200 individuals and subjected them
to encounter by 16 traps arranged in a $4\times 4$ grid according to
the Euclidean distance metric. We fit 3 different models; (i) the
misspecified euclidean distance model; (ii) the true data-generating
model with the relative cost raster {\it known} and (iii) the true
data-generating model but estimating the relative cost parameter by
maximum likelihood.  A sample realization of this is shown in Fig.
\ref{ecoldist.fig.raster100}.

We simulated a few levels of N and detection probability 
hi p: alpha0<- -2     sigma<- .5    K<-10
lo p: alpha0<- -2     sigma<- .5    K<- 5
Because any simulation study is inherently arbitrary, we have provided R scripts
for carrying out simulations in the Appendix so that the interested reader can experiment with
their own situations.

\begin{figure}
\begin{center}
\includegraphics[height=3.25in,width=3.25in]{Ch10/figs/raster_withN100}
\end{center}
\caption{a 16 pt raster with 100 guys living on it}
\label{ecoldist.fig.raster100}
\end{figure}
 
































\subsection{Illustration: Example Good vs. Bad habitat}

Here we analyze the cost-weighted distance for a landscape created to
mimic
a habitat corridor or park unit or some other block of
relatively homogeneous good-quality habitat for some species
(Fig. \ref{ecoldist.fig.corridor}).
It is surrounded by a suburban wasteland of McDonalds and Wal-Marts, much
less hospital habitat for most things. See
We describe the steps for creating this landscape shortly, so that the
reader can use a similar process to generate more relevant landscapes
for their own problems. 

In practice, we have a landscape with multiple polygons delineating
good or bad habitat or a forest preserve or corridors or
whatever. These exist maybe as shapefiles.
We have to create a raster, overly the polygon and assign values
depending on whether pixels are in good polygons or not.
You can do this in GIS but we do a version of this in R here. See
chapter 5.XYZ for an example of reading in the shapefile and doing it.

We provide 
 a function \mbox{\tt make.seg} which allows the user to make a specific
buffer given a trap region.  You can plot the region with a range or a
specific set of points and then click over it using these commands:
{\small 
\begin{verbatim}
make.seg<-function(npts){
l2<-locator(npts)
l2<-cbind(l2$x,l2$y)
l2<-round(l2,2)
tmp<-NULL
for(i in 1:nrow(l2)){
tmp<-paste(tmp,l2[i,1],l2[i,2])
if(i<nrow(l2))
tmp<-paste(tmp,",")
}
l2b<- paste("LINESTRING(",tmp,")")
l2<- readWKT(l2b)
return(l2)
}


plot(NULL,xlim=c(0,10),ylim=c(0,10))
l1<-make.seg(9)
plot(l1)
l2<-make.seg(5)
plot(l1)
lines(l2)
\end{verbatim}
}

We used this function to create a couple of line segments of class XXX
from package xyz XXXXXX  which can be loaded as 
follows\footnote{how to put this in the R package?}:
\begin{verbatim}
load("polygons.RData")
\end{verbatim}
This has 2 polygon files in it ......representing different segments
of this corridor or river system. We use the commands XYZ to join and
buffer the two segments and the resultt is that shown in fig. XYZ.

\begin{verbatim}
buffer<- 0.5
l1<-l1.old
l2<-l2.old
par(mfrow=c(1,1))
aa<-gUnion(l1,l2)
plot(gBuffer(aa,width=buffer),xlim=c(0,10),ylim=c(0,10))
pg<-gBuffer(aa,width=buffer)
pg.coords<- pg@polygons[[1]]@Polygons[[1]]@coords

xg<-seq(0,10,,30)
yg<-seq(10,0,,30)

delta<-mean(diff(xg))
pts<- cbind(sort(rep(xg,30)),rep(yg,30))
points(pts,pch=20)

in.pts<-point.in.polygon(pts[,1],pts[,2],pg.coords[,1],pg.coords[,2])
points(pts[in.pts==1,],pch=20,col="red")

\end{verbatim}

\begin{figure}
\begin{center}
\includegraphics[height=3.25in,width=3.25in]{Ch10/figs/corridor}
\end{center}
\caption{a habitat corridor or preserve}
\label{ecoldist.fig.corridor}
\end{figure}


We focus on devising a SCR model for this corridor system and we
imagine that animals will tend to severely avoid leaving the buffered
habitat zone.


Then we want to take the inverse of that image......... and do a simulation.

\subsection{Distance Weighting}

We consider a situation here in which we develop weights based on the
distance from some feature such as a highway or a river. 

\subsection{Hard Boundary}

like a lake or city or something......


\section{Ecological distance in SCR models}

Code for fitting the basic model  with minimum cost-distance {\it fixed}.

\section{Estimating Cost or Resistance Values}

\section{Bayesian Analysis}

Some of this stuff can be done easily enough in BUGS or we can code
our own. e.g., we only need the distance matrix computed ahead of time
and we can work on that with a discrete raster.  continuous space is
less easy because BUGS doesn't have a function for computing
ecological distance between any arbitrary points. 

As for estimating the parameters of the ecological distance function
-- this might at the present time be impossible in the BUGS
variants. However, we can use the functions described above to
implement our own MCMC algorithm following the developments of
Chapt. \ref{chapt.mcmc}.










\section{Summary and Outlook}

This shit is hot. We are the man. There is no limit to what we can do.


The effect of ignoring ecological distance e.g. if that is the true
data generating model would be useful to investigate for some specific
situations. obviously this will depend on the complexity of the
landscape being considered and the actual manner in which animals use
space, i.e., how they perceive distance. Since we can never know this,
any kind of sim study would be inherently arbitrary and so we didn't
pursue that here. [i guess i mean we never ever can observe actual
costs at the pixel level or even get directinfo about that because we
only observe the total distance traveled -- and that is only observed
imperfectly -- so getting direct info about cost seems difficult and
maybe not even possible. 
If we had radio-colared guys we could estimate 1st order transition
probabilities. i.e., Pr(goes to v(t+1) given at v(t)) and it seems
like those transition probabilites are direct information about cost
or propensity to go from v(t) to v(t+1).


moving around in a buffer or on a stream network seems like a useful
problem. what if we have such a network, then how screwed are we?








\section{Discussion}

Interest in SCR models has grown rapidly in the last several years
since their formalization using likelihood (Borchers and Efford 2008)
and Bayesian (Royle and Young 2008) inference methods. This is because
of both the severe practical limitations of classical non-spatial
capture-recapture models, and also the ubiquity of auxiliary spatial
information in all capture-recapture studies, and thus the universal
applicability of SCR models.

All applications of SCR models have been based on models for the
encounter probability that use the standard Euclidean distance between
individuals and traps. This has obvious limitations that it is
unaffected by landscape or habitat structure and it implies
stationary,  isotropic and symmetrical home ranges. These are standard
criticisms of the basic SCR model as universally applied in practice. 
In this paper,  we have developed for the first time a formal framework for integrating
``ecological distance'' into SCR models, where ecological distance is
defined as a cost-weighted distance between points, and where ``cost''
is characterized by one or more spatially explicit covariates that are
believe to influence movement or space-usage of individuals.  


How animals use space and therefore how distance to a trap is
perceived by individuals is not something that can ever be known. We
can only ever conjure up models to describe this
phenomenon. Historically we have come up with models and had to regard
those as fixed up to a single parameter or two that is invariant to
the underlying landscape. Here we have shown that there is hope to
estimate parameters, from capture-recapture data,
 that describe how animals use space and thereby
allowing for irregular home range geometry that is influenced by
landscape structure, and resulting affect on encounter probability in
the context of capture-recapture studies. 

As expected, our simulation study demonstrated that the MLE of model
parameters is approximately unbiased in moderate sample
sizes. Moreover, the 
effect of ignoring ecological distance and using normal Euclidean
distance in the model for encounter probability, has the 
logical effect of causing negative bias
in estimates of $N$. We expect this because the effect is similar to
failing to model heterogeneity. i.e., if we misspecify ``model Mh'' with
``Model M0'' then we will expect to under-estimate $N$. So the effect of
mis-specifiying the ecological distance metric with a standard
homogeneous euclidean distance has the same effect.

In our view, this bias is not really the most important reason to consider models of
ecological distance. Rather, inference about the structure of
ecological distance is fundamental to many problems in applied and
theoretical ecology that have to do with modeling landscape
connectivity, corridor and reserve design, population viability
analysis, gene flow, and other phenomena. 
Because encounter rate in traps, as it relates to distance from
individual activity centers, should be related to landscape
connectivity, our method allows investigators to to evaluate landscape
factors that influence movement of individuals over the landscape and
therefore SCR models based on ecological distance metrics might aid in
corridor design and understanding other aspects of space usage and
movement in animal populations. 

We adopted a standard approach to inference under our model based on
marginal likelihood (Borchers and Efford 2008). In principle, 
Bayesian analysis does not pose any unique challenges for this new
class of models, except that computing the cost-weighted distance is
computationally intensive. 
 So, having to do this at each iteration of an
MCMC algorithm may be impractical using existing algorithms.
A related issue is that the size of the raster slows things down. For
huge rasters, even likelihood analysis can be computationally challenging.

Some extensions of this method may be of interest. Instead of 
having explicit covariates it might be possible to estimate the 
``resistance surface'' as a latent field, much as \citep{wikle:2003}
 did in the developing of models of species spread based on a
 diffusion process. He define the spatially-explicit rate of 
diffusion, $\delta(x)$, as a Gaussian spatial process and it was
estimated from the data.  The coolness of this would defy experience. 




















