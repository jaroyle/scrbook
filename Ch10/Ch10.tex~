\chapter{
%Modeling Animal space-usage with 
%Detection Models based on Ecological Distance
Ecological Distance Models in Spatial Capture-Recapture
}
\markboth{Chapter XXX}{}
\label{chapt.implicit}


\vspace{.3in}


%% this material is a general introduction for a manuscript
Spatial capture-recapture models are a relatively new class of models 
for estimating animal density from capture-recapture data with auxiliary 
information about individual capture locations
(Borchers and Efford 2008; Royle and Young 2008; Royle et al. 2009). 
The basic idea is to expression encounter probability of individuals 
as a function of the distance between individual center of activity, 
say ${\bf s}_{i}$, and trap location, say ${\bf x}_{j}$.
In these models ${\bf s}_{i}$ is regarded as a latent variable and 
conventional methods of statistical inference either based on 
marginal likelihood (Borchers and Efford 2008) or Bayesian analysis by 
MCMC (Royle and Young 2008).

While the models are a relatively recent innovation, their use is 
becomming widespread (Efford XYZ ecology; Gopalaswamy et al. 2012; Foster and Harmsen
2012; More here) 
because they resolve critical problems with using 
ordinary non-spatial capture-recapture methods such as ill-defined area 
sampled, and heterogeneity in encounter probability due to the 
juxtaposition of individuals with traps. Furthermore, essentially all real 
capture-recapture studies produce auxiliary spatial information and therefore
SCR models are directly relevant to standard data collected from such 
studies. Indeed, the use of ordinary capture-recapture models essentially 
admits a model misspecification (i.e. homogeneous encounter probability) 
by ignoring the explicit spatial information.

Every application of SCR models to this point is based on encounter 
probability models in which distance between individual activity center 
and trap location is parameterized by simple Euclidean distance.
%Previously we have only considered stationary and symmetric models for detection probability. 
While these will often be sufficient for practical purposes, especially in 
small data sets, there will sometimes be interest in developing more 
complex models of the detection process as it relates to space usage of 
individuals.  
%These models are simple because they are based on
%Euclidean distance -- which are convenient because we know this
%distance precisely conditional on individual activity center ${\bf  s}$.
However, animals may not
judge distance in terms of euclidean distance but, rather, according
to quality of local habitat, landscape connectivity and other considerations that statisticians
might not really understand. Moreover, because encounter probability and the
distance metric upon which that is based represent outcomes of individual movements about their
home range, it is natural that 
landscape ecologists might have explicit
hypotheses about things that affect the distance metric. 

In this section we develop models for detection probability based on
alternative distance metrics that account for ecological
considerations -- which, in keeping with the conventions in the ecological literature,
 we will call ``ecological distance''. In
particular, we adopt a cost-weighted distance metric which is an idea
in widespread use in 
landscape ecology for modeling connectivity, movement and gene flow
\citep{adriaensen_etal:2003,manel_etal:2003,mcrae_etal:2008}. In the context of SCR
models we can use this as the basis for computing the distance
bwetween traps and individuals activity centers. In this way we can
explicitly accommodate some landscape structure and ideas related to how animals
use space in SCR models. We develop a likelihood-based inference framework for SCR model parameters
using this new distance metric when the ecological distance function is known.  
We show that the MLEs are approximately unbiased in moderate sample
sizes, as expected, but also that the misspecific model based on Euclidean distance can produce
substantial bias in estimates of $N$ and hence density.
Finally, we extend the model to allow for the case where the distance metric is only known by a 
covariate and the relative ``cost'' is estimated by maximum likelihood in addition to other
encounter probability parameters and density. 



\section{Cost Distance}

We use a cost-weighted distance metric in the package \mbox{\tt
gdistance} \index{R package!gdistance} which computes the distance 
between points by accumulating pixel-specific costs assigned by the user 
(but we consider estimating these in Sec. XYZ). The idea is widely used in
landscape ecology for modeling connectivity, movement and gene flow
\citep{adriaensen_etal:2003,mcrae_etal:2008}. As is customary for reasons of
computational tractability we consider a discrete landscape defined by a 
raster of some prescirbed resolution. The distance between any two points 
${\bf x}$ and ${\bf x}'$ can be represented by a sequence of line segments 
connecting a sequence of neighboring  pixels say 
${\bf l}_{1},{\bf l}_{2},\ldots,{\bf l}_{m}$. Then the cost-weighted distance 
between ${\bf x}$ and ${\bf x}'$ is
\[
 d({\bf x},{\bf x}')
  =  \sum_{i=1}^{m-1} cost({\bf l}_{i},{\bf l}_{i+1})||{\bf l}_{i} - {\bf l}_{i+1}||
\]
where $cost({\bf l}_{i},{\bf l}_{i+1})$ is the user-defined cost to move 
from pixel ${\bf l}_{i}$ to neighboring pixel ${\bf l}_{i}$ in the sequence.
To be consistent with our use of the {\bf R} package \mbox{\tt costDistance},
the incremental cost of moving from one pixel to another is the
distance-weighting of the {\it average} of the 2 pixel costs.
%%%%%%%%%% As a consequence, the terminal value is not counted. 
In addition to \mbox{\tt gdistance} we use functions from a number of
other {\bf R} packages including \mbox{\tt rgeos}, \mbox{\tt
  shapefiles} and \mbox{\tt raster}.

%% Kimmy: can you look up the algorithm that Gdistance uses?  
The algorithm XYZ XYZ (REF XYZ) seeks to find the  sequence of pixels ${\bf l}_{1},
\ldots, {\bf l}_{m}$ which minimizes the cost-weighted distance. 


As an example of the cost-weighted distance calculation consider the
following landscape comprised of 16 pixels with unit spacing 
identified as follows, along with the pixel-specific cost:
\begin{verbatim}
  pixel ID                 Cost
  1  2  3  4          100   1   1  1
  5  6  7  8          100 100   1  1
  9 10 11 12          100 100 100  1
 13 14 15 16          100 100   1  1 
\end{verbatim}
Then we assign low cost of 1 to ``good habitat'' pixels (or pixels we
think of as ``highly connected'' by virtue of being in good habitat)
and, conversely, we assign high cost (100) to ``bad habitat''. So the
shortest cost-weighted distance between pixels 2 and 3 in this example
is just 1 unit, the shortest cost-distance between pixels 2 and 7 is
%% Kimmy use this raster in the code below and then revise the stated
%% cost-distances
%% between the pixels
sqrt(2) units, the shortest distance between pixels 13 and 14 is 100
units, while the shortest cost-distance between 13 and 15 is 100. A
tough one is: what is the shortest distance between 10 and 16? A guy at pixel
10 can move diagonal and pay sqrt(2)*1 + 1 total.
An alternative pixel labeling scheme arising as if you were applying
the vec operation to the matrix ``as you look at it''. This produces
labels that are a transpose of those given previously, so the id's are
as follows:
\begin{verbatim}
 1 5  9 13
 2 6 10 14
 3 7 11 15
 4 8 12 16
\end{verbatim}
In this case we have to transpose or rotate something along the way. 
Be careful. This matters in the way you stick the ``cost'' data into
the elements of the raster.
In the following example we fill the cost
matrix using the default \mbox{\tt byrow=FALSE} standard in {\bf
  R}, but here we use cost values of 1-16 so that we can produce an
image of the resulting costs to visualize their ordering in the raster:
\begin{verbatim}
r<-raster(nrows=4,ncols=4)
projection(r)<- "+proj=utm +zone=12 +datum=WGS84"
extent(r)<-c(.5,4.5,.5,4.5)
values(r)<-matrix(1:16,4,4,byrow=FALSE)
par(mfrow=c(1,1))
plot(r)
\end{verbatim}
The raster, in image form, is shown in Fig. \ref{ecoldist.fig.raster}.

\begin{figure}
\begin{center}
\includegraphics[height=3.25in,width=3.25in]{Ch10/figs/raster}
\end{center}
\caption{a 16 pt raster}
\label{ecoldist.fig.raster}
\end{figure}

Then we use the functions \mbox{\tt transition}, \mbox{\tt
  geoCorrection} (which doesn't really do anything but reformat the
data somehow) and \mbox{\tt costDistance} to compute the distance
matrix. They operate on this inverse-scale so we first take the 
element-by-element inverse of the raster. Then we define a set of points,
the center points of each raster, to compute the cost-distance between
them all. The commands altogether are as follows:
\begin{verbatim}
%%%%%r<-1/r  Actually we don't do this
tr1<-transition(r,transitionFunction=max,directions=8)
tr1CorrC<-geoCorrection(tr1,type="c",multpl=FALSE,scl=FALSE)
tr1CorrC<-1/tr1CorrC  # because the transitions are computed in conductance which is 1/cost
pts<-cbind( sort(rep(1:4,4)),rep(4:1,4))
costs1<-costDistance(tr1CorrC,pts)
outD<-as.matrix(costs1)
\end{verbatim}
Now we can look at the result and see if it makes sense to us. Here we
print the first 4 columns of this distance matrix and illustration a
couple of examples of calculating the minimum cost-weighted distance
between points:
\small{
\begin{verbatim}
> outD[,1:4]
      1    2    3    4
1   0.0  1.5  4.0  7.5
2   1.5  0.0  2.5  6.0
3   4.0  2.5  0.0  3.5
4   7.5  6.0  3.5  0.0
5   3.0  4.5  7.0 10.5
6   5.5  4.0  6.5 10.0
7   9.0  7.5  5.0  8.5
8  13.5 12.0  9.5  6.0
9  10.0 11.5 14.0 17.5
10 13.5 12.0 14.5 18.0
11 18.0 16.5 14.0 17.5
12 23.5 22.0 19.5 16.0
13 21.0 22.5 25.0 28.5
14 25.5 24.0 26.5 30.0
15 31.0 29.5 27.0 30.5
16 37.5 36.0 33.5 30.0
\end{verbatim}
}
{\bf NEED TO REVISE BELOW}
Moving down the first (left-most, as you look at it) column of the
raster the cost-distance between pixel 1 and 2 is 1.0 which we know to
be right because the cost of leaving pixel 1 is tabulated, and that is
the value 1.0.  To get from pixel 1 to pixel 3 we have to leave both
pixels 1 and 2, for a total cost of $1+2 = 3$ which is the result
given above.  To see that the shortest cost-weighted distance is not always the
shortest Euclidean distance, consider moving from pixel .....



\section{Construction of the likelihood}

The likelihood is really just this.....
let $yij$ be the number of encounters of individual $i$ in trap $j$ out of
$K$ samples (e.g., nights). Then $yij$ is binomial with probabilities
$pij$ where pij = function() so the likelihood is the product of $n$ such
terms, 1 for each of the $n$ observed individuals. 
In addition, we have to account for the N-n unobserved individuals which
contributes a term to the likelihood which looks like this....
\[
 big formula
\]
accounting for the fact that the $n$ observed guys is a binomial draw
with index $N$ then the full (unconditional or joint) likelihood is
\[
 big formula
\]
which we optimize using the nlm() optimizer in {\bf R}. The R code for 
fitting the model is given in the following set 
of commands, which is provided as a script in the Appendix (and R package
scr) along with functions for simulating data.

Estimating density: N/areaofstatespace.


\section{Examples of Computing Cost-Distance}

In this section we provide a series of examples that we think
represent how cost-weighted distance models will be used in real
problems. 
The basic framework for analysis is based on R packages XYZ
and functions XYZ to do basic operations involving polygons and point
files...... 

In particular, we will typically have a polygon coverage either in the
form of a GIS shapefile or a matrix of points or some other specific
format, and we want to put that polygon on a map and use the polygon
boundary in some way to generate pixel-specific costs. So we want to
see if points or raster pixels are in the polygon, or not, or how far
they are from the polygon boundary (cost might be related to distance)
and similar operations. In the following examples, we confront how to
do some of these operations in {\bf R}. 


We develop an example here using our made-up 16 pixel raster above which 
we regard for the purposes of this example as an extremely coarse landscape 
covariate, e.g., at say a $10 \times 10$ km resolution. This $4 \times  4$ raster therefore 
defines a landscape of $40 \times 40$ km and we suppose that at the center point 
of each raster is a camera trap and we are studying a population of $N$ 
animals such as tigers at some extraordinarily high density. In this case you
would literaally have to beat tigers off you with a stick. We suppose 
that each raster is characterized by a single covariate which is increasing 
from XYZ to XYZ so that effectively there is an increasing gradient from the 
NW to the SE. We define 
\[
 log(cost(x))=  \beta z 
 \]
where $\beta = 1$ which we regard as known. With $\beta=0$ then the model reduces to
one in which the cost of moving across each pixel is constant, and therefore Euclidean
distance is operative.
 Of course this is a model and it is completely
unreasonable to regard $\beta$ as known but this is standard practice in essentially all
historical studies of landscape or genetic connectivity although sometimes crude methods
of estimation in specific contexts are based on comparing distance matrices of genetic
structure with those produced by simulation models so that either $\beta$ might be estimated
crudely or covariates which improve fit can be selected. REFs????
However this stuff has never been formalized in a likelihood context and especiallly
not for SCR models. 
Anyhow, we consider the known-$\beta$ situation first before dealing with estimating
this parameter. 


\subsection{Simulation studies}

We devised a simulation study to evaluate three things: (1) the general statistical performance
of the density estimator under this new model; (2) the effect of misspecifying the model with
a normal Euclidean distance metric and (3) the statistical performance of estimating the relative
cost parameter.
We used population sizes of 100 and 200 individuals and subjected them to encounter by 16 traps
arranged in a $4\times 4$ grid according to the Euclidean distance metric. We fit 3 different 
models; (i) the misspecifiedeuclidean distance model; (ii) the true data-generating model with
the relative cost raster {\it known} and (iii) the true data-generating model but estimating
the relative cost parameter by maximum likelihood. 
A sample realization of this is shown in Fig. 
\ref{ecoldist.fig.raster100}.
We simulated a few levels of N and detection probability 
hi p: alpha0<- -2     sigma<- .5    K<-10
lo p: alpha0<- -2     sigma<- .5    K<- 5

\begin{figure}
\begin{center}
\includegraphics[height=3.25in,width=3.25in]{Ch10/figs/raster_withN100}
\end{center}
\caption{a 16 pt raster with 100 guys living on it}
\label{ecoldist.fig.raster100}
\end{figure}


Secondly then we did a 3rd analysis of each 
data set where we estimated the parameter $\beta$ to see if we could estimate
it effectively. 


\begin{verbatim}
Summaries of sampling distribution
             mean         SD        0.025       0.50     0.975
high p cases  
N=100      mean n = 84.6
euclid      92.206114   4.671663  83.805619  91.992918 101.062115 
ecol/known  98.968879   4.664292  89.364618  99.263953 107.489414 
ecol/est    99.078592   4.718402  89.381868  99.569445 107.767255 
N=200
euclid     185.195251   5.508866 176.687894 184.911376 195.955084 
ecol/known 198.682523   5.578265 189.409647 197.983286 209.565110 
ecol/est   199.032216   5.832957 189.098127 198.821324 210.572747 

\end{verbatim}


\section{Discussion}

Interest in SCR models has grown rapidly in the last several years since 
their formalization using likelihood (Borchers and Efford 2008) and 
Bayesian (Royle and Young 2008) methods. This is because of the universal 
ubiquity of auxiliary spatial information in all capture-recapture stucdies 
and thus the unifersal applicability of SCR models. 

HEre we have developed for the first time a formal framework for 
estimating the relative cost of animals moving around the landscape 
from capture-recapture data. The cost is manifest in encounter rate 
data from which we formally fit encounter probability models using a 
modification of the SCR likelihood.

The effect of ignoring ecological distance e.g. if that is the true
data generating model has the logical effect of causing negative bias in
estimates of N. We expect this because the effect is similar to failing
to model heterogeneity. i.e., if we misspecifiy model Mh with Model M0 then
we will expect to under-estimate N. So the effect of mis-specifiying the
ecological distance metric with a standard homogeneous euclidean distance
has the same effect.  

How animals use space and therefore how distance to a trap is perceived
by individuals is not something that can ever be known. We can only ever
conjure up models to describe this phenomenon. Historically we have
come up with models and had to regard those as fixed up to a single parameter
or two that is invariant to the underlying landscape. Here we have shown
that there is hope to estimate parameters that describe how animals use
space and thereby allowing for a non-stationary effect on encounter probability.















\begin{comment}

% below stuff is for the book




\subsection{Illustration: Example Good vs. Bad habitat}

Here we analyze the cost-weighted distance for a landscape created to
mimic
a habitat corridor or park unit or some other block of
relatively homogeneous good-quality habitat for some species
(Fig. \ref{ecoldist.fig.corridor}).
It is surrounded by a suburban wasteland of McDonalds and Wal-Marts, much
less hospital habitat for most things. See
We describe the steps for creating this landscape shortly, so that the
reader can use a similar process to generate more relevant landscapes
for their own problems. 

In practice, we have a landscape with multiple polygons delineating
good or bad habitat or a forest preserve or corridors or
whatever. These exist maybe as shapefiles.
We have to create a raster, overly the polygon and assign values
depending on whether pixels are in good polygons or not.
You can do this in GIS but we do a version of this in R here. See
chapter 5.XYZ for an example of reading in the shapefile and doing it.

We provide 
 a function \mbox{\tt make.seg} which allows the user to make a specific
buffer given a trap region.  You can plot the region with a range or a
specific set of points and then click over it using these commands:
{\small 
\begin{verbatim}
make.seg<-function(npts){
l2<-locator(npts)
l2<-cbind(l2$x,l2$y)
l2<-round(l2,2)
tmp<-NULL
for(i in 1:nrow(l2)){
tmp<-paste(tmp,l2[i,1],l2[i,2])
if(i<nrow(l2))
tmp<-paste(tmp,",")
}
l2b<- paste("LINESTRING(",tmp,")")
l2<- readWKT(l2b)
return(l2)
}


plot(NULL,xlim=c(0,10),ylim=c(0,10))
l1<-make.seg(9)
plot(l1)
l2<-make.seg(5)
plot(l1)
lines(l2)
\end{verbatim}
}

We used this function to create a couple of line segments of class XXX
from package xyz XXXXXX  which can be loaded as 
follows\footnote{how to put this in the R package?}:
\begin{verbatim}
load("polygons.RData")
\end{verbatim}
This has 2 polygon files in it ......representing different segments
of this corridor or river system. We use the commands XYZ to join and
buffer the two segments and the resultt is that shown in fig. XYZ.

\begin{verbatim}
buffer<- 0.5
l1<-l1.old
l2<-l2.old
par(mfrow=c(1,1))
aa<-gUnion(l1,l2)
plot(gBuffer(aa,width=buffer),xlim=c(0,10),ylim=c(0,10))
pg<-gBuffer(aa,width=buffer)
pg.coords<- pg@polygons[[1]]@Polygons[[1]]@coords

xg<-seq(0,10,,30)
yg<-seq(10,0,,30)

delta<-mean(diff(xg))
pts<- cbind(sort(rep(xg,30)),rep(yg,30))
points(pts,pch=20)

in.pts<-point.in.polygon(pts[,1],pts[,2],pg.coords[,1],pg.coords[,2])
points(pts[in.pts==1,],pch=20,col="red")

\end{verbatim}

\begin{figure}
\begin{center}
\includegraphics[height=3.25in,width=3.25in]{Ch10/figs/corridor}
\end{center}
\caption{a habitat corridor or preserve}
\label{ecoldist.fig.corridor}
\end{figure}


We focus on devising a SCR model for this corridor system and we
imagine that animals will tend to severely avoid leaving the buffered
habitat zone.

\subsection{Distance Weighting}

We consider a situation here in which we develop weights based on the
distance from some feature such as a highway or a river. 

\subsection{Hard Boundary}

\section{Ecological distance in SCR models}

Code for fitting the basic model  with minimum cost-distance {\it fixed}.



\section{Real example}


\section{Estimating Cost or Resistance Values}

\section{Bayesian Analysis}

Some of this stuff can be done easily enough in BUGS or we can code
our own. e.g., we only need the distance matrix computed ahead of time
and we can work on that with a discrete raster.  continuous space is
less easy because BUGS doesn't have a function for computing
ecological distance between any arbitrary points. 

As for estimating the parameters of the ecological distance function
-- this might at the present time be impossible in the BUGS
variants. However, we can use the functions described above to
implement our own MCMC algorithm following the developments of
Chapt. \ref{chapt.mcmc}.


\section{Summary and Outlook}

This shit is hot. We are the man. There is no limit to what we can do.


The effect of ignoring ecological distance e.g. if that is the true
data generating model would be useful to investigate for some specific
situations. obviously this will depend on the complexity of the
landscape being considered and the actual manner in which animals use
space, i.e., how they perceive distance. Since we can never know this,
any kind of sim study would be inherently arbitrary and so we didn't
pursue that here. [i guess i mean we never ever can observe actual
costs at the pixel level or even get directinfo about that because we
only observe the total distance traveled -- and that is only observed
imperfectly -- so getting direct info about cost seems difficult and
maybe not even possible. 
If we had radio-colared guys we could estimate 1st order transition
probabilities. i.e., Pr(goes to v(t+1) given at v(t)) and it seems
like those transition probabilites are direct information about cost
or propensity to go from v(t) to v(t+1).


moving around in a buffer or on a stream network seems like a useful
problem. what if we have such a network, then how screwed are we?

\end{comment}

