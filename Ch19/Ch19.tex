\chapter{
Sampling design for spatial capture-recapture studies 
}
\markboth{Sampling design}{}
\label{chapt.design}

\vspace{.3in}


Statistical design is recognized as an important component of animal
population studies (Morrison et al. 2008; Williams et al. 2003).
Commonly, design is thought of in terms of number of samples to take,
when to sample, methods of capture, desired sample size (of
individuals), power of tests, and related considerations. 
In the context of spatial sampling problems, where populations of mobile
animals are sampled by an array of traps or devices, a critical design
element is the spacing and configuration of the trapping array.  
While conceptual and heuristic design considerations have been addressed by
a number of authors (e.g., Nichols and Karanth 2002, Ch. 11), little
or no formal analysis focused on spatial design of arrays has been
carried out. Some authors have addressed trap spacing and
configuration by sensitivity ``re-analysis'' (deleting traps and
reanalyzing; Wegge et al. 2004; Tobler et al. 2008), but it would be
natural also to consider a simulation-based approach in which the
precision of estimates is evaluated by Monte Carlo simulation given a
number of prescribed design.  

In this chapter we recommend a general framework for evaluating
specific design choices for SCR studies based on Monte Carlo simulation. 
It is always useful to perform a simulation study before conducting a field survey not only to evaluate the
design in terms of its ability to generate useful estimates of things,
but also so that you have an expectation of what the data will look
like as they are being collected. This gives you the ability to recognize some
pathologies and possibly intervene to resolve issues before they
render a whole study worthless. Suppose you design a study to place 40
camera traps based on your expectations of parameter values you obtained from
a careful review of the literature, and simulation studies suggest
that you should get 3-5 captures of individuals per night of
sampling. In the field you find that you're realizing 0 or 1 captures
per night and therefore you have the ability to sit down and
immediately question your initial assumptions and possibly take some
remedial action. Simulation evaluation of design {\it a priori} is
therefore a critical element of any field study (alternatively, \citet{efford_etal:2009} provide a mathematical procedure to determine the expected number of individuals captured and recaptures for a given detector array and set of model parameters).

%%% Andy working on this %%%%
In addition to this we use some theory related to 
the broader problem of ``spatial design'' (REFS XXXXXX) to provide
algorithhms for generating general SCR designs. It is
interesting that typical designs seem to favor trap arrangements that
are uniformly distributed and equally spaced. We will see that
optimal designs may not be as such.(section XYZ below). 



\section{Study design issues for (spatial) capture-recapture}

The importance of adequate trap spacing and overall configuration of the trapping array has long been an issue in the capture-recapture literature.
A heuristic based on recognizing the
importance of typical home range sizes (Dice 1938; 1941) and thus
being able to obtain information about home range size is that traps
should be spaced such that the array of available traps exposes as
many individuals as possible but, at the same time, individuals should
be captureable in multiple traps. Actually, traditional CR models require that all individuals in the study area have a probability $>$ 0 of being captured, which means that 
the trap array must not contain holes large enough to contain an animal's entire home range (Otis et al 1978). As a consequence, trap spacing should be on the
same order as the radius of a typical home range (e.g., Dillon and
Kelly 2007).  As such, for example, cats with small home ranges
should require closer trap spacing than large wide-ranging cats. 
Where approaches such as MMDM are used in combination with traditional CR models to obtain density estimates, trap spacing also has a major effect on movement estimates, since it determines the resolution of the information on individual movement (Parmenter et al. 2003; Wilson and Anderson 1985). If trap spacing is too wide, there is little to no information on animal movement because most animals will only be captured at one trap (Dillon and Kelly 2007). In addition, only a trapping grid that is large relative to individual movement can capture the full extent of such movements, and researchers have suggested that the grid size should be at least four times that of individual home ranges to avoid positive bias in estimates of density (Bondrup-Nielsen 1983).  This recommendation originated in small mammal trapping, and it should be relatively easy to follow when dealing with species covering home ranges $<$ 1ha. However, translated to large mammal research, this can entail having to cover several thousands of square kilometers.  

Whereas traditional CR studies are concerned with the number of individuals and recaptures and with satisfying the model assumption of all individuals having some probability of being captured, in spatial capture-recapture we are looking at an additional level of data: We need spatially
spread out captures and recaptures. That means, it is not enough to
recapture an individual, but we need to recapture at least some
individuals at several traps. Therefore, in general, design of SCR studies boils down to obtaining three bits
of information: total captures of unique individuals, 
gross recaptures informative about baseline encounter
rate, and spatial recaptures, informative about $\sigma$. Most SCR design
choices wind up trading these three things against each other to achieve
some optimal (or good) mix. So for example if we sample a very small
number of sites a huge number of times then we can get a lot of
recaptures but only very few spatial ones, and few unique individuals etc.
This need for spatial recaptures may appear as an additional constraint on study design, but actually, SCR studies are much less restricted than traditional CR studies, since $\sigma$ is estimated as a specified function of the ancillary spatial information collected in the survey and the capture frequencies at those locations. This function is able to make a prediction across distances even when these are latent, including distances larger than the extent of the trap array. When there is enough data across at least some range of distances, the model should do well at making predictions at unobserved distances. The key here is that there needs to be 'enough data across some range of distances', which induces some constraint on how large our overall trap array must be to provide this range of distances (e.g. Marques et al. 2011).   


XXXXX what to do about these alternative designs? XXXXX
Often field studies face logistic difficulties, especially when dealing with wide-ranging, rare and cryptic species like large carnivores. The need to sample large areas with limited resources often forces researchers to compromise between a study design that is optimized for data analysis and a study design that is logistically viable. In such studies, traps are often placed along roads or rivers where a large number of traps can be accessed with relative ease. Unless the study site is crossed by a network of roads or waterways, this will lead to a sampling design that is much more 'linear' compared to an area-based design where traps are spread out uniformly over the study site. Yet another design that has been proposed for regular capture-recapture studies is that of a trapping web (DESCRIBE). In the second part of this chapter we look into the performance of these trap arrangements.


\section{Trap spacing and array size relative to animal movement}

Using a simulation study, \citet{sollmann_etal:2012} investigated how trap spacing and array size relative to animal movement influence SCR parameter estimates and we will summarize this study here. They simulating detection histories on an 8 x 8 trap array with regular trap spacing of 2 units, using a Binomial encounter model with complementary log-log link, across a range of values for the movement parameter $\sigma^*$. We refer to the movement parameter as $\sigma^*$ here, because \citet{sollmann_etal:2012} use a slightly different parametrization of SCR models, in which $\sigma^*$ corresponds to $\sigma/\sqrt{2}$. 

Values for $\sigma^*$ were chosen so that there was a scenario where the trap array was smaller than a single individual’s home range ($\sigma^*$ = 5), a scenario where spaces between traps were large enough to contain entire home ranges ($\sigma^*$ = 0.5), and two intermediate scenarios and where sigma was smaller ($\sigma^*$ =1 unit) and larger ($\sigma^*$ = 2.5 units) than the trap spacing, respectively. 
$N$ was 100 and the baseline trap encounter rate $\lambda_0$ was 0.5 for all four scenarios, and trap encounters were generated over 4 occasions. Table \ref{design.tab.simres} shows the results as the average over 100 simulations.
 
 \begin{table}[ht]
  \centering
  \caption{Mean, relative root mean squared error (rrmse) of the mean, mode, 2.5 \% and 97.5 \% quantiles, relative bias of mean (RB) and 95BCI coverage (BCI) for spatial capture-recapture parameters across 100 simulations for four simulation scenarios, define by the input value of movement parameter $\sigma^*$. $N$ = number of individuals in the state space; $\lambda_0$ = baseline trap encounter rate}
    \begin{tabular}{l*{7}{c}}
    \hline
    Scenario & Mean  & rrmse & Mode  & 2.5\% & 97.5\% & RB    & BCI \\     \hline
    {\bf $\sigma^*$ = 1} & {\it } & {\it } & {\it } & {\it } & {\it } & {\it } & {\it } \\
    $N$ & 108.497 & 0.172 & 104.099 & 78.977 & 143.406 & 0.085 & 96 \\
    $\lambda_0$ & 0.518 & 0.248 & 0.477 & 0.303 & 0.752 & 0.035 & 94 \\
    $\sigma^*$ & 1.008 & 0.093 & 0.990 & 0.857 & 1.195 & 0.008 & 94 \\
    {\bf $\sigma^*$ = 2.5} & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } \\
    $N$ & 100.267 & 0.105 & 98.456 & 82.086 & 121.878 & 0.003 & 97 \\
    $\lambda_0$ & 0.507 & 0.118 & 0.500 & 0.409 & 0.623 & 0.014 & 92 \\
    $\sigma^*$ & 2.501 & 0.046 & 2.491 & 2.267 & 2.690 & $<$ 0.001 & 92 \\
    {\bf $\sigma^*$ = 5} & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } \\
    $N$ & 102.859 & 0.137 & 100.756 & 77.399 & 130.020 & 0.029 & 88 \\
    $\lambda_0$ & 0.505 & 0.075 & 0.501 & 0.435 & 0.580 & 0.011 & 93 \\
    $\sigma^*$ & 5.023 & 0.039 & 5.001 & 4.687 & 5.431 & 0.005 & 97 \\
    \end{tabular}
  \label{design.tab.simres}
\end{table}
 

\begin{table}[ht]
  \centering
  \caption{Summary statistics of 100 simulated data sets for four simulation scenarios, defined by the input value of movement parameter $\sigma$. Individual detection histories were simulated on an 8 x 8 trap array with regular trap spacing of 2 units.}
    \begin{tabular}{lm{2.1cm}m{2.3cm}m{2.1cm}m{2.8cm}}
    \hline
    Scenario & Inds. captured & Total captures & Inds. recaptured & Inds. captured at $>$ 1 trap \\ \hline
    {$\sigma^*$ = 0.5} & 18.29 (3.84) & 25.38 (5.86) & 5.52 (2.03) & 0.72 (0.95) \\
    {$\sigma^*$ = 1.0} & 37.70 (13.44) & 69.35 (26.05) & 19.48 (7.68) & 11.87 (5.43) \\
    {$\sigma^*$ = 2.5} & 44.19 (4.67) & 231.78 (33.98) & 36.60 (4.76) & 35.21 (4.73) \\
    {$\sigma^*$ = 5.0} & 40.51 (5.15) & 427.77 (79.09) & 33.09 (4.63) & 32.60 (4.76) \\
    \end{tabular}
  \label{design.tab.simdat}
\end{table}

All model parameters were identifiable and estimated with relatively low bias ($<$ 10 \%) and high to moderate precision (rrmse $<$ 25 \%) for all scenarios of $\sigma^*$, except $\sigma^*$ = 0.5 units (therefore excluded from Table \ref{design.tab.simres}. Data for the latter case mostly differed from the other scenarios in that fewer animals were captured and very few of the captured animals were recorded at more than 1 trap (Table \ref{design.tab.simdat}). For $\sigma^*$ = 0.5, abundance ($N$) was not identifiable in 88 \% of the simulations, and when identifiable, was underestimated by approximately 50 \%. This shows that a trap spacing that is considerably too large may be problematic in SCR studies.
 
Estimates of $N$ and $\sigma^*$ were least biased under the $\sigma^*$ = 2.5 scenario, while estimates of $\lambda_0$ were least biased under the $\sigma$ = 5 scenario. Precision for estimates of N was highest under the $\sigma^*$ = 2.5 scenario, while $\lambda_0$ and $\sigma^*$ were more precise at $\sigma^*$ = 5. All estimates had the highest relative bias and the lowest precision under the $\sigma^*$ = 1 scenario. 
Results from our simulation study corroborate our expectation that SCR models perform well even when using a trapping array smaller than an average home range: at $\sigma^*$ = 5, the home range of an individual was approximately 235 $units^2$, while our trapping grid only covered 196 $units^2$. Still, the model performed very well.

An important consideration in our simulation study was that all but the $\sigma^*$ = 0.5 units scenarios provided large amounts of data, including 20 + individuals being captured on the trapping grid. When dealing with real-life animals that are often territorial and may have lower trap encounter rates, a very small grid compared to an individual’s home range may result in the capture of few to no individuals. In that case, the sparse data will limit the ability of the model to estimate parameters (Marques et al. 2011), which is true of most models.

In summary, the simulation demonstrated that SCR models performed well as long as $\sigma^*$ was at least 0.5 times the average distance between traps (which corresponds to $\sigma$ being 0.35 times the average distance between traps). At this trap spacing to movement ratio, most individuals are captured at one trap only (see Tab. \ref{design.tab.simdat}). Still, parameter estimates exhibited low bias and remained relatively precise (see simulation results for $\sigma^*$ = 1 in Tab. \ref{design.tab.simres}).  In the simulation, only when $\sigma^*$ = 0.5 so that the home range size is very small relative to the trap spacing and hardly any individuals were captured at more than one trap, SCR models were unable to estimate $N$. Models performed best when $\sigma^*$ was slightly larger than trap spacing (or in other words, when $\sigma$ was slightly smaller).


{\flushleft \bf Example: Black bears from Pictured Rocks National Lakeshore: }

To see how trap array size influences parameter estimates from spatial
capture-recapture models in the real world, \citet {sollmann_etal:2012} also looked at a
 black bear data set from
Pictured Rocks National Lakeshore, Michigan, collected using 123 hair snares
distributed over an area of 440 $km^2$ along the shore of Lake
Superior in May-July 2005 \citep{Belant et al 2005}.
The SCR model for the bear data allowed for
differences in the baseline trap encounter rate and $\sigma^*$ between
males and females, and $\lambda_0$  varied across occasions. This was
motivated by a) the lower average number of detections for male bears
and b) the decreasing number of detections over time in the raw data,
and c) the fact that male black bears are known to move over larger
areas than females (e.g., (Gardner et al. 2010; Koehler and Pierce
2003). 

To address the impact of a smaller trap array on the parameter
estimates, the full data set and data subsets were analyzed with SCR models. The first subset retained only
those 50 \% of the traps closest to the grid center. In the second, 80 \% of all traps were removed, retaining only the southern 20 \% of the
trap array \ref{design.tab.bears}.

\begin{table}[ht]
  \centering
  \caption{Posterior summaries of SCR model parameters for black bears.}
    \begin{tabular}{lcccc}
    \addlinespace
	\hline
          & Mean (SE) & Mode  & 2.5\% & 97.5\% \\ \hline
    {\bf Full data set} &       &       &       &       \\
    {\it D } & 10.556 (1.076) & 10.448 & 8.594 & 12.792 \\
    $\sigma^*$ (males) & 7.451 (0.496) & 7.323 & 6.579 & 8.495 \\
    $\sigma^*$ (females) & 2.935 (0.143) & 2.939 & 2.671 & 3.226 \\
    {\bf 50\% of traps} &       &       &       &      \\
    {\it D } & 12.648 (1.838) & 12.205 & 9.307 & 16.713 \\
    $\sigma^*$ (males)  & 5.354 (0.511) & 5.248 & 4.472 & 6.473  \\
    $\sigma^*$ (females) & 3.318  (0.277) & 3.262 & 2.841 & 3.910 \\
    {\bf 20\% of traps} &       &       &       &        \\
    {\it D } & 6.752 (1.611) & 5.953 & 4.000 & 10.218  \\
    $\sigma^*$(males)  & 9.881 (3.572) & 7.566 & 5.121 & 18.447 \\
    $\sigma^*$ (females)  & 2.686 (0.391) & 2.657 & 2.121 & 3.404  \\
    \end{tabular}
  \label{design.tab.bears}
\end{table}

Reducing the area of the trap array by 50 \% created a grid polygon of 144 $km^2$, which was smaller than an estimated male black bear home range and only 50 \% larger than a female black bear home range - approximately 260 $km^2$ and 100 $km^2$, respectively, when converting estimates of $\sigma^*$ to home range size. Table \ref{design.tab.bears} shows that this did not greatly influence model results, compared to the full data set. The observed smaller differences in parameter estimates may be due to individual differences in detection and movement that manifest themselves when only a smaller portion of the overall population is sampled. By reducing the number of traps we effectively reduced the size of the overall data set estimates were based on (both in terms of individuals captured and recaptures). This was reflected in overall higher SE and wider confidence intervals. In spite of these differences, density estimates  – the main objective of applying SCR models – remained largely constant. Removing 80 \% of the traps and thereby reducing the area of the trap array to 64 $km^2$ - well below the average black bear home range - had a great effect on sample size (only 25 of the original 83 individuals sampled) and thus parameter estimates. Particularly, male black bear movement was overestimated and imprecise. The combination of the low baseline trap encounter rate of males and the considerable reduction in sample size led to a low level of information on male movement: 5 of the 12 males were captured at one trap only. Although they moved over smaller areas, owing to their higher trap encounter rate females were, on average, captured at more traps (3.4 traps per individual compared to 2.6 for males) so that their movement estimate remained relatively accurate. Overestimated male movements and female trap encounter rates resulted in an underestimate of density of almost 40 \%. This effect is contrary to what we would expect to see in non-spatial CR models, where too small an area leads to underestimated movement and overestimated density (Bondrup-Nielsen 1983; Dillon and Kelly 2007; Maffei and Noss 2008).

\subsection {Final musings: SCR models, trap spacing and array size}

When designing a capture-recapture study for a single species, trap spacing and the size of the array can (and should) be tailored to the spatial behavior of that species to ensure adequate data collection. However, some trapping devices like camera traps may collect data on more than one species and researchers may want to analyze these data, too. Independent of the trapping device used, study design will in most cases face a limit in terms of the number of traps available or logistically manageable. 

Particularly for large mammal research SCR models have much more realistic requirements in terms of area coverage than non-spatial CR models, under which density estimates can be largely inflated with small trapping grids relative to individual movement (Maffei and Noss 2008). How large the spatial survey effort needs to be does not only depend on the extent of movement of the target species, but also on the temporal effort, density and detection probability (Marques et al. 2011) – in summary, the amount of data that can be collected with any given trap array.

While there are limits to the flexibility in spatial trap array design for SCR modeling, the method is fairly robust to changes in trap array size and spacing relative to animal movement. Trapping grids with an extent of approximately a home range diameter can – in theory - adequately estimated density and home range size.

However, these results should not encourage researchers to design non-invasive trap arrays based on minimum area and spacing requirements. Study design should still strive to expose as many individuals as possible to sampling and obtain adequate data on individual movement. Large amounts of data can also improve precision of parameter estimates – the density estimate for the full black bear data set has narrower confidence intervals than estimates from the reduced data sets. This is particularly important when a study is concerned with monitoring population changes.  Also, only with sufficiently large data sets potentially important covariates (such as gender or time effects in the black bear example) can be included into SCR models to obtain density estimates that reflect the actual state of the studied population.

%%Andy?
\section {Mink case study}


\section{Model-based Spatial Design}

A point we have stressed in previous chapters is that SCR models are
basically glorified versions of generalized linear models (GLMs) with
a random effect that represents a latent spatial attribute of
individuals, interpretable loosely as the center of activity or home
range center.  This formulation makes analysis of the models readily
accessible in freely available software and also allows us to adapt
and use concepts from this broad class of models to solve problems in
spatial capture recapture. In particular, we can exploit
well-established model-based design concepts (Kiefer 1959; Box and
Draper 1959, 1987; Fedorov 1972; Sacks et al. 1989; Hardin and Sloane
1993; Fedorov and Hackl 1997; M$\ddot{u}$ller 2007) to develop a
framework for designing spatial trapping arrays for capture-recapture
studies.

In the following sections, we adapt these classical methods for constructing optimal designs
to obtain the configuration of traps
(or sampling devices) in some region (the design space, ${\cal X}$), 
that minimizes some appropriate objective function based on a
compromise between the variance of estimating $N$ for a prescribed
state-sdpace. We show that this criterion -- based on the Variance of
$\hat{N}$ -- represents a formal compromise between minimizing the
variance 
of the  MLEs of the
detection model parameters  and obtaining a probability of capture.
In particular, if our only objective was to minimize the variance of
paramter estimates than all of our traps should be in one or a small
number of clusters whereas if our objective was only to maxinimize the
expected probability of encounter then the array should be highly
uniform. By seeking to minimize the variance of $\hat{N}$ 
our objective function is, formally, a compromise between these two
objectives and the resulting designs are not always highly regular nor
clustered. 

% Existing theory (Sanathanan 1972) suggests that such designs
%should also be optimal for estimating density or abundance.  

\subsection{The Design Problem for Spatial Capture-Recapture Studies}


Let ${\cal X}$, the {\it design space}, denote some region within
which sampling could occur and let ${\bf X} = {\bf x}_{1},\ldots, {\bf
  x}_{J}$ denote the {\it design}, the set of sample locations (e.g.,
of camera traps) which henceforth will be referenced as ``traps.'' The
technical problem addressed in this paper is how to choose the
locations ${\bf X}$ in a manner that is statistically efficient for
estimating abundance or density.  The design space, ${\cal X}$, which
determines potential design points, will have to be prescribed.  This
could be some polygon describing a park or forest unit from which
we may choose trap locations.  Further, while ${\cal X}$ maybe be
continuous, 
in practice it will be sufficient to represent ${\cal X}$ by a
discrete collection of points.  This is especially convenient when
the geometry of ${\cal X}$ is complicated and irregular (which would
be in most practical applications).

The models considered here are based on the notion that individuals
have a static spatial location about which their movements are
concentrated and which can be related to trap-specific encounter
probabilities by distance alone.  The models regard the population of
$N$ such individual ``activity centers'' as the outcome of a point
process.  Denote the home range center of an individual by the coordinate
${\bf s}$ which is regarded
as the outcome of a random variable
uniformly distributed over the state-space ${\cal S}$, some
2-dimensional region.  The importance of ${\cal S}$ is obvious as it
defines a population of individuals (i.e., activity centers) and, in
practice, it is not usually the same as ${\cal X}$ due to the fact
that animals move freely over the landscape and the location of traps
is typically restricted by policies, ownership and other
considerations. That ${\cal X}$ and ${\cal S}$ are not the same is the
basic problem of geographic non-closure of the population for which
spatial capture-recapture models have been devised (Efford 2004; 
Borchers and Efford 2008; Royle and Young 2008; Royle and Gardner 2009).

Lets establish some basic ideas and concepts here.

Suppose you know ${\bf s}$ for an individual then his vector of counts
of encounter in each trap ${\bf y}$ are either binomial or Poisson
counts , i.e., just a GLM  with
\[
g( E({\bf y})  ) =  \alpha_0  + \alpha_1 ||{\bf s}-{\bf x}||
\]
Lets think about this in the context of a normal linear model then:    
\[
{\bf y}| = {\bf M}({\bf X},{\bf s})'{\bm \alpha} + error
\]
We could analyze the design problem for the binomial or Poisson case
but to establish basic ideas here lets just look at the normal model. 
The variance-covariance matrix of $\hat{\bm \alpha}$ is, supressing
the dependence on ${\bf X}$, is:
\[
 \mbox{Var}( {\bm \alpha}) = ({\bold M}({\bf s})'{\bold M}({\bf s}))^{-1}
\]
Therefore 
if we know ${\bf s}$ we could now easily find the design ${\bf X}$
that optimizes some function of the variance-covariance matrix, whatever function
we want. 
If we don't know ${\bf s}$ then we might as well minimize the expected variance:
\[
  E_{{\bf s}}\left\{ Var(\alpha) \right\} = \sum_{s \in {\cal S}}  (M'(s)M(s))^{-1}  
\]
This is really easy to do for any number of design points ${\bf
  x}_{1},\ldots, {\bf x}_{J}$
using imperfect exchange algorithms (Section XYZ XXXX) which always improve the
criterion but will not necessarily yield {\it the} optimal design. But
it is usually good enough for practice. 

Interestingly, if you minimize obvious functions of the variance then
this produces strongly clustered designs. For example if I pick a
design of size 11 then it puts 2 or 3 points in each corner of the
square and 1 or 2 points in the center. I think this makes a lot of
sense if your objective function is simply to minimize the variance of
your estimates of ${\bm \alpha}$.

This suggests to us that maybe minimizing this variance isn't really
the right thing to do.  In fact, it is not sufficient to make a design
that is optimal for estimating regression parameters -- we also want
to produce a low variance for estimating N.  since $n \sim
Bin(N,pbar)$ we want n to be as close to N as possible, generally
speaking.  This suggests that we should find a design that maximizes
"pbar" -- i.e., generates the highest expected sample size.

If we just design networks to maximize $\bar{p}$ then, as you expect,
these designs are highly regular. 

Ok, so what we really would like to do is optimize some function of
these three things. We want to minimize the variance of (alpha0,
alpha1 and n0).  The problem with this is that there aren't easy
formuals for this but we devise an approximation in the following
section and then we build designs for that criterion.

\begin{comment}
\subsection{Poisson case}

We adopt a class of models (Royle et al. 2009; Royle and Gardner 2009)
which assume that, conditional on an individual's activity center,
${\bf s}_{i}$, the encounter frequency for individual $i$ in trap $j$
is a Poisson random variable
\[
 y_{ij}|{\bf s}_{i} \sim \mbox{Poisson}(\lambda_{ij})
\]
with
\[
\lambda_{ij} =  \lambda_{0} exp(-\frac{1}{\sigma^{2}} ||{\bf s}_{i} -
{\bf x}_{j}||^{2} )
\]
Here $\lambda_{0}$, the baseline encounter frequency, is the expected
number of encounters in a trap that is situated at the center of an
individuals home range.  The interpretation and justification of this
model is as follows: cameras are left out for a period of time, a
night or perhaps several nights and if they are operational then the
observations are frequencies with no natural upper limit.  In practice
observations are often quantized to $\{0,1\}$ so that a Bernoulli
model might be more appropriate. There are good reasons for doing this
(due to non-independence of encounters that occur close together in
time; See Royle et al. 2009). If the encounter frequency is
sufficiently small then the Poisson model will be a good approximation
and so designs under this model are of some general interest.

For this Poisson encounter frequency model we see that
\begin{eqnarray}
  log(\lambda_{ij}) & = &  
log(\lambda_{0}) - \frac{1}{\sigma^{2}} ||{\bf s}_{i}-{\bf
  x}_{j}||^{2} \\ \nonumber
 & = & {\bf m}({\bf s}_{i})' {\ubeta}
\label{eq.linearpredictor}
\end{eqnarray}
for ${\bf m}({\bf s}_{i})' = (1,||{\bf s}_{i} - {\bf x}_{j}||^{2})$
and regression parameters ${\ubeta} =( log(\lambda_{0}),
1/\sigma^{2})$.  For a population of $N$ individuals, 
let $\ueta$ be the $N J \times 1$ vector having elements
$log(\lambda_{ij})$ stacked end-to-end (by individual), then ${\bf M}$
is the $N J \times 2$ design matrix where the 2nd column contains the
squared 
pairwise distances between each individual $i$ and trap $j$.
 This basic form as a Poisson GLM motivated the work
described in the rest of this paper.  

It is emphasized that the model has been presented here conditional on
${\bf s}_{i}$, but not particular attributes of the realizations of
$y_{ij}$ (i.e., whether the individual was captured or not). Thus,
given that an individual home range center, the observations are
realizations of a Poisson random variable -- and this may include a
set of all-zero observations.  In an inferential setting we deal with
unknown $N$ by data augmentation in a Bayesian framework (Royle and
Young 2008; Royle et al. 2009; Royle and Gardner 2009) or by
conditional likelihood (Sanathanan 1972) in which $N$ is removed from
the likelihood by conditioning (Borchers and Efford 2008).
\end{comment}





\subsection{An SCR Optimal Design Criterion}

Suppose our conditional estimator of $N$ is $\hat{N} = \hat{\theta} n$
where $\theta = 1/\bar{p}$, $\bar{p} = 1- \prod_{j} (1-p_{ij})$ and
$n$ is the observed sample size.
We want to build a spatial design for a trap array that minimizes the
{\it unconditional} variance. That is, that which is not conditional
on $n$ because in practice we will have no control over $n$.
By the laws of iterated conditional variance (RD Chapter 1 for an
example) we have
\[
 Var(\hat{N}) =  Var(\hat{\theta})E(n^2) + \theta^{2} Var(n)
\]
a delta approximation to  $Var(\hat{\theta})$ in terms of $\bar{p}$ produces
\[ 
 Var(\hat{\theta}) = \frac{1}{\bar{p}^{2}} Var(\bar{p})
\]
and note that $E(n^2) = \sigma^{2}_{n} + \mu_{n}^{2}$.
When we plug all of this in we have
\[
 Var(\hat{N}) = Var(\bar{p}) \frac{N (1-\bar{p} + N \bar{p})}{\bar{p}}
 + N \frac{(1-\bar{p})}{\bar{p}}
\]
Finally we use a delta approximation to express $Var(\bar{p})$ in
terms of $Var(\alpha)$: 
\[
Var(\bar{p}) =\left( \frac{ \delta \bar{p}}{\delta \alpha}  \right)^{2}
Var(\hat{\alpha})
\]
(matrix form of that)
We therefore need to identify the following quantities.......
$\bar{p}$... ariance of that thing.... 


We want to pick a design which minmiizes this or at least has a low
expected variance, and this objective functionhas a whole bunch of very nice intuitive
interpretations. 
In particular, this suggests that we want to obtain a high fraction of
the population in our sample -- (i.e., objective 2 from above) -- and
also we want to do a good job estimating $\bar{p}$ (objective 1 from
above). 
The variance if $\bar{p}$ is, approximately, 
(XXX I dont think this is right it should be d bar[p]/dalpha  squared
....
should also be a function of n somehow.....
\[
Var(\bar{p}) =  \bar{p}^{2}  Var(\alpha)
\] 
So plugging this in produces
\begin{eqnarray*}
 Var(\hat{N})  &=&   Var(\bar{\alpha})  + N\frac{(1-\bar{p})}{\bar{p}} \\
  & = &  Var(\bar{\alpha})  + E[n] (1-\bar{p}) 
\end{eqnarray*}
this is even more intuitive looking. As we pack samples close together
it looks as if we drive the 2nd term to 0, except that $\bar{p}$ {\it
  is a function of the state-space} so that is only true if we define
our state-space to be very small.   
Anyhow this shows that the criterion of interest is a compromise
between
good estimation of the coefficients $\alpha$ and maxinizing the
probability of capture of individuals

So how do we pick designs that do this effectively?

We need to come up with a ballpark guess of the model
parameters. i.e., what is $\alpha$ and $N$? If we do that, and specify
the state-space ${\cal S}$ and the number of traps to place, then we
can optimize the variance criterion. 



\subsection{Optimization of the criterion}

In formulating the optimization problem note that we have $J$ sample
locations corresponding to rows of ${\bf X}$.  The problem is a $2J$
dimensional optimization problem which, for $J$ small, could be solved
using standard numerical optimization algorithms as exist in almost
every statistical computation environment.  However, $J$ will almost
always be large enough so as to preclude effective use of such
algorithms. This is a common problem in experimental design, design
for response surface estimation, computer experiments, spatial
sampling designs and other disciplines for which sequential exchange
or swapping algorithms can be used (e.g., Wynn 1970; 
Fedorov 1972; Mitchell 1974; Meyer and
Nachtsheim 1995). The basic idea is to pose the problem as a sequence
of 1-dimensional optimization problems in which the objective function
is optimized over 1 or several coordinates at a time.

In the present case, we consider swapping out ${\bf x}_{j}$ for some
point in ${\cal X}$ that is nearby ${\bf x}_{j}$ (e.g., a 1st order
neighbor). The objective function is evaluated for all possible swaps
(at most 4 in the case of 1st order neighbors) and whichever point
yields the biggest improvement is swapped for the current value.  The
algorithm is iterated over all $J$ design points and this continues
until convergence is achieved. Such algorithms may yield local optima
and optimization for a number of random initial designs can yield
incremental improvements. We implemented this swapping algorithm in
{\bf R}, using the basic strategy employed elsewhere (e.g., Nychka et
al. 1997; Royle and Nychka 1998).  A version of a swapping
algorithm used to optimize a space-filling criterion is implemented
in the {\bf R} package {\bf
  fields} (Fields Development Team 2006).  I developed an
  implementation that requires
a discrete representation of ${\cal S}$ (an aribtrary matrix of
coordinates) and an indicator of which elements of ${\cal S}$ are
members of the design space ${\cal X}$. For each point in ${\bf X}$, only 
the nearest neighbors (the number is specified) are considered for
swapping into the design during each iteration.  

While swapping algorithms are convenient to implement, and efficient
at reducing the criterion in very high dimensional problems, they do
not always yield the global optimum.  In practice, as in the examples
below, it is advisable to apply the algorithm to a large number of
random starting designs.  My experience is that essentially
meaningless improvements are realized after searching through a few
dozen random starts.


\subsection{Illustration}


Consider designing a study for camera traps in a square region defined
by the square $[10,20] \times [10, 20]$ and with ${\cal X} = {\cal
  S}$.  For this illustration I assumed $\beta_{0} = log(\lambda_{0})
= -2.7$ and $\beta_{1} = 1/(\sigma^{2}) = 1/4$, $1/9$ and $1/16$, so
$\sigma = 2,3,4$. (this was dumb - note that $\sigma$ is really 2
times the standard deviation of a normal distribution. Oh well!).
Designs of size 9 and 10 were computed for each value of $\sigma$
using many random starting designs.  The putative optimal designs
(henceforth ``best'') are shown\footnote{My intention is to provide
  many of these results in an Appendix in order to reduce the length
  of the paper.} in Figure \ref{fig.fig1}.  For J=9, $\sigma =2$, the
best design was produced in 180 out of 1000 random starts.  For
$\sigma = 3$ (row 2, left panel) the best design was produced in about
88\% of all optimizations from random starting values.
%reason is that there are a lot of points in the interior that interact
%relatively little with the design and these ``holes'' tend to cause
%the algorithm to get caught in a local optimum (my interpretation) of
%the objective function.  Or, consider this, with sigma = small the
%design can probably be translated a little bit in space .... this is
%what I think happens.  
For $J=10$, and $\sigma =2$ (row 1, right panel), the best design was
found about 24\% of the time (from random starts). 
The $\sigma = 3$ best design (row 2, right
panel; 14\% of random starts) clusters 2 points in the center.
Finally, consider the $\sigma =4$ case (last row of
Fig. \ref{fig.fig1}).  We have two irregular looking designs and 
the design points cluster in various ways. 
% For $J=9$ this was produced
%only 1 time whereas only 8 instances of 1000 produced the
%best design for $J=10$.  We might thus have little confidence in that
%result\footnote{subsequent analyses have failed to find a better
%  design.}. 

I computed the best designs using the same settings but inreasing the
size of ${\cal S}$ relative to ${\cal X}$.  In particular, I nested
${\cal X}$ into $[9,21] \times [9,21]$ (Figure \ref{fig.fig2})
 and then $[8,22]^{2}$ (Figure \ref{fig.fig3}).
The obvious effect of this is
that the best designs move points toward the edge of the design space
${\cal X}$ so as to provide more exposure to points in ${\cal S}$.
The effect is more pronounced, obviously, as you provide more area
outside of ${\cal X}$ that is allowed to influence the design.

As a final example, 
consider placing 20 camera traps in this region. Where do they
go? Look at the 3 buffers, 3 values of sigma, thats 9 total designs
(use a single panel).
An interesting feature of the designs is that they are not
regular. Traps occur in clusters of several traps close together
with the clusters more widely spaced.





\section {Summary}


we should always do a simulation study. this allows us to learn what
to expect as we start collecting real data.  plus we can simulate for
any complex situation that we desire.

however formal model-based design of SCR models has great potential in





