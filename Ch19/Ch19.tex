\chapter{
Sampling design for spatial capture-recapture studies 
}
\markboth{Sampling design}{}
\label{chapt.design}

\vspace{.3in}

\section{Introduction}

Adequate sample design is the key to a successful capture-recapture
study. There is probably no practicing field biologist who has never
run into problems analyzing data that can be traced back to flawed
study design. Such problems can arise in two (not mutually exclusive)
ways: Study design may have been inadequate considering the
requirements and assumptions of the model we want to use to analyze
data (for example, lack of spatial independence of sample sites in
occupancy studies); or our sample size is too small, because of
insufficient spatial and/or temporal sampling effort. It is almost
impossible to give general recommendations for the latter issue - how
much effort you have to dispense to collect a reasonable amount of
data will depend on your survey method, study species and site. This
problem is probably best addressed by preliminary field studies that
give you an idea of the amount of data you can collect with any given
amount of effort. In this chapter we will therefore focus on the
former problem -- that of sampling design satisfying the requirements
and assumptions of the analytical tools we want to apply. Even for
this problem it is difficult to make recommendations that are
universally applicable, since the features of the data you collect
depend on the particular circumstances of your study (method, site,
species etc.) just as much as they depend on your sampling design. A
rule of thumb may work well in one situation and badly in another. And
simulation studies used to derive recommendations for study design are
only simplified versions of the complex processes that will generate
real field data. Therefore, recommendations should be taken with a
grain of salt and viewed more as a starting point from where to begin
planning a study.

General context: Simulation evaluation of designs and also some theory
realyted to the broader problem of ``spatial design'' (REFS XXXXXX)


\section{Study design issues for spatial capture-recapture}

The name 'spatial capture-recapture' pretty much gives away what we
are looking at in terms of data for SCR models: We need spatially
spread out captures and recaptures. That means, it is not enough to
recapture an individual, but we need to recapture at least some
individuals at several traps. This clearly induces a constraint on
trap spacing, similar but not identical to a constraint you may know
from regular capture-recapture studies. Non-spatial capture-recapture
models are based on the assumption that all individuals in the sampled
area have a probability $>0$ of being detected, which means that trap
spacing has to be narrow enough so that the array has no ``holes''
that could contain an individual's entire home range \citep{karanth_nichols:1998}.
In SCR we need the spatial recaptures to get at
$\sigma$ and the latent activity centers, but the consequences are
similar: traps should be spaced at distances smaller than a home range
diameter.

%% Rahel: Sorry to butcher your nice flow of things here but I'm must
%% adding
%% some thoughts here and there.
In general, design of SCR studies boils down to obtaining three bits
of information: total captures of unique individuals, 
gross recaptures informative about baseline encounter
rate, and spatial recaptures, informative about sigma. Most SCR design
choices wind up trading these three things against each other to achieve
some optimal (or good) mix. So for example if we sample a very small
number of sites a huge number of times then we can get a lot of
recaptures but not any spatial ones, and few unique individuals etc...

In non-spatial capture-recapture where approaches such as MMDM are used to obtain density estimates, trap spacing also has a major effect on movement estimates, since it determines the resolution of the information on individual movement (Parmenter et al. 2003; Wilson and Anderson 1985). If trap spacing is too wide, there is little to no information on animal movement because most animals will only be captured at one trap (Dillon and Kelly 2007). In addition, only a trapping grid that is large relative to individual movement can capture the full extent of such movements, and researchers have suggested that the grid size should be at least four times that of individual home ranges to avoid positive bias in estimates of density (Bondrup-Nielsen 1983).  This recommendation originated in small mammal trapping, and it should be relatively easy to follow when dealing with species covering home ranges $<$ 1ha. However, translated to large mammal research, this can entail having to cover several thousands of square kilometers.  SCR models are much less restricted since $\sigma$ is estimated as a specified function of the ancillary spatial information collected in the survey and the capture frequencies at those locations. This function is able to make a prediction across distances even when these are latent, including distances larger than the extent of the trap array. When there is enough data across at least some range of distances, the model should do well at making predictions at unobserved distances. The key here is that there needs to be 'enough data across some range of distances', which induces some constraint on how large our overall trap array must be to provide this range of distances (e.g. Marques et al. 2011).   


Clearly, both trap spacing and the size of the trap array have to be considered relative to the movements of the species we intend to study and we will illustrate how both measures affect the performance of SCR models based on a simulation study and empirical example published by Sollmann et al. (in revision). These are the most basic considerations, or the building blocks, of designing a field study and we can imagine applying these considerations to derive very distinct sampling designs. 

Often field studies face logistic difficulties, especially when dealing with wide-ranging, rare and cryptic species like large carnivores. The need to sample large areas with limited resources often forces researchers to compromise between a study design that is optimized for data analysis and a study design that is logistically viable. In such studies, traps are often placed along roads or rivers where a large number of traps can be accessed with relative ease. Unless the study site is crossed by a network of roads or waterways, this will lead to a sampling design that is much more 'linear' compared to an area-based design where traps are spread out uniformly over the study site. Yet another design that has been proposed for regular capture-recapture studies is that of a trapping web (DESCRIBE). In the second part of this chapter we look into the performance of these trap arrangements.


\section{Trap spacing and array size relative to animal movement}

To investigate how trap spacing and array size relative to animal movement influences SCR parameter estimates, we simulated detection histories on an 8 x 8 trap array with regular trap spacing of 2 units, using a Binomial encounter model with complementary log-log link. Details of the simulation study can be found in Sollmann et al (in revision).  

We chose four values for $\sigma$ so that we had a scenario where the trap array was smaller than a single individual’s home range ($\sigma$ = 5), a scenario where spaces between traps were large enough to contain entire home ranges ($\sigma$ = 0.5), and two intermediate scenarios and where sigma was smaller ($\sigma$ =1 unit) and larger ($\sigma$ = 2.5 units) than the trap spacing, respectively. We defined the state space $\cal{S}$ as the trapping array plus a buffer of 3 times $\sigma$ around it. Since density and home range size are often negatively correlated (e.g., (Benson et al. 2006; Dahle and Swenson 2003)), we chose an $N$ of 100 for all four scenarios. In combination with the varying state space this lead to a decrease in density with increasing $\sigma$. For all scenarios we used a baseline trap encounter rate $\lambda_0$ of 0.5 and simulated trap encounters over 4 occasions to produce 100 data sets. Each model was run in WinBUGS for 8000 iterations, with three chains, a burn-in of 5000 and a thinning rate of 2. For each data set we calculated the mean, standard error (SE) and mode of the posterior, as well as the relative bias (RB) and the relative root mean squared error (rrmse) of the mean, and determined whether the 95BCI included the true parameter value (BCI coverage). Table \ref{design.tab.simres} shows the results as the average over all simulations.
 
 \begin{table}[ht]
  \centering
  \caption{Simulation results}
    \begin{tabular}{l*{7}{c}}
    \hline
    Scenario & Mean  & rrmse & Mode  & 2.5\% & 97.5\% & RB    & BCI \\     \hline
    {\bf $\sigma$ = 1} & {\it } & {\it } & {\it } & {\it } & {\it } & {\it } & {\it } \\
    $N$ & 108.497 & 0.172 & 104.099 & 78.977 & 143.406 & 0.085 & 96 \\
    $\lambda_0$ & 0.518 & 0.248 & 0.477 & 0.303 & 0.752 & 0.035 & 94 \\
    $\sigma$ & 1.008 & 0.093 & 0.990 & 0.857 & 1.195 & 0.008 & 94 \\
    {\bf $\sigma$ = 2.5} & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } \\
    $N$ & 100.267 & 0.105 & 98.456 & 82.086 & 121.878 & 0.003 & 97 \\
    $\lambda_0$ & 0.507 & 0.118 & 0.500 & 0.409 & 0.623 & 0.014 & 92 \\
    $\sigma$ & 2.501 & 0.046 & 2.491 & 2.267 & 2.690 & $<$ 0.001 & 92 \\
    {\bf $\sigma$ = 5} & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } & {\bf } \\
    $N$ & 102.859 & 0.137 & 100.756 & 77.399 & 130.020 & 0.029 & 88 \\
    $\lambda_0$ & 0.505 & 0.075 & 0.501 & 0.435 & 0.580 & 0.011 & 93 \\
    $\sigma$ & 5.023 & 0.039 & 5.001 & 4.687 & 5.431 & 0.005 & 97 \\
    \end{tabular}
  \label{design.tab.simres}
\end{table}
 

\begin{table}[ht]
  \centering
  \caption{Summary statistics of 100 simulated data sets for four simulation scenarios, defined by the input value of movement parameter $\sigma$. Individual detection histories were simulated on an 8 x 8 trap array with regular trap spacing of 2 units.}
    \begin{tabular}{l|m{2.1cm}|m{2.3cm}|m{2.1cm}|m{2.8cm}}
    \hline
    Scenario & Individuals captured & Total captures & Individuals recaptured & Individuals captured at more than 1 trap \\ \hline
    {$\sigma$ = 0.5} & 18.29 (3.84) & 25.38 (5.86) & 5.52 (2.03) & 0.72 (0.95) \\
    {$\sigma$ = 1.0} & 37.70 (13.44) & 69.35 (26.05) & 19.48 (7.68) & 11.87 (5.43) \\
    {$\sigma$ = 2.5} & 44.19 (4.67) & 231.78 (33.98) & 36.60 (4.76) & 35.21 (4.73) \\
    {$\sigma$ = 5.0} & 40.51 (5.15) & 427.77 (79.09) & 33.09 (4.63) & 32.60 (4.76) \\
    \end{tabular}
  \label{design.tab.simdat}
\end{table}

All model parameters were identifiable and estimated with relatively low bias ($<$ 10 \%) and high to moderate precision (rrmse $<$ 25 \%) for all scenarios of $\sigma$, except $\sigma$ = 0.5 units (therefore excluded from Table \ref{design.tab.simres}. Data for the latter case mostly differed from the other scenarios in that fewer animals were captured and very few of the captured animals were recorded at more than 1 trap (Table \ref{design.tab.simdat}). For $\sigma$ = 0.5, abundance ($N$) was not identifiable in 88 \% of the simulations, and when identifiable, was underestimated by approximately 50 \%. Thus, we omitted parameter estimates for $\sigma$ = 0.5 scenarios from the results but note that this is an important scenario in that a trap spacing that is considerably too large may be problematic. 
Estimates of $N$ and $\sigma$ were least biased under the $\sigma$ = 2.5 scenario, while estimates of $\lambda_0$ were least biased under the $\sigma$ = 5 scenario. Precision for estimates of N was highest under the $\sigma$ = 2.5 scenario, while $\lambda_0$ and $\sigma$ were more precise at $\sigma$ = 5. All estimates had the highest relative bias and the lowest precision under the $\sigma$ = 1 scenario. 
Results from our simulation study corroborate our expectation that SCR models perform well even when using a trapping array smaller than an average home range: at $\sigma$ = 5, the home range of an individual was approximately 235 $units^2$, while our trapping grid only covered 196 $units^2$. Still, the model performed very well.

An important consideration in our simulation study was that all but the $\sigma$ = 0.5 units scenarios provided large amounts of data, including 20 + individuals being captured on the trapping grid. When dealing with real-life animals that are often territorial and may have lower trap encounter rates, a very small grid compared to an individual’s home range may result in the capture of few to no individuals. In that case, the sparse data will limit the ability of the model to estimate parameters (Marques et al. 2011), which is true of most models.

SCR models performed well as long as sigma was at least ½ the average distance between traps. At this trap spacing to movement ratio, most individuals are captured at one trap only (see Tab. \ref{design.tab.simdat}). This scenario represents a problem in non-spatial CR models when estimates of the effective sampled area are based on individual movements between traps (for example, half the mean maximum distance moved between traps by individuals captured at more than one trap – (Wilson and Anderson 1985)), as estimates of average movement tend to increase with the number of recaptures (Jett and Nichols 1987; Stickel 1954). Yet, with SCR models, parameter estimates exhibited low bias and remained relatively precise (see simulation results for $\sigma$ = 1 in Tab. \ref{design.tab.simres}).  In the simulation, only when $\sigma$ = 0.5 so that the home range size is very small relative to the trap spacing and hardly any individuals were captured at more than one trap, SCR models were unable to estimate $N$. A potential way to address this issue could be a nested trapping grid with narrower trap spacing for some subset of the traps (XXX see section on linear design and trapping webs?? XXX).



\subsubsection *{Real life example: Black bears from Pictured Rocks National Lakeshore}
To see how trap array size influences parameter estimates from spatial capture-recapture models in the real world, we also looked at a different data set for black bears. Black bears were studied in the Pictured Rocks National Lakeshore, Michigan, using 123 hair snares distributed over an area of 440 $km^2$ along the shore of Lake Superior in May–July 2005 (for details see Sollmann et al in review, Belant et al 2005).
To carry out the data augmentation, 450 -- n all-zero encounter histories were added to the encounter array.  The model allowed for differences in the baseline trap encounter rate and sigma between males and females, and lam0 varied across occasions. This was motivated by a) the lower average number of detections for male bears and b) the decreasing number of detections over time in the raw data, and c) the fact that male black bears are known to move over larger areas than females (e.g., (Gardner et al. 2010; Koehler and Pierce 2003). We defined the state space as the outermost coordinates of the trapping array plus a 15-km buffer.
To address the impact of a smaller trap array on the parameter estimates, we created to data subsets. In the first, we retained only those 50 \% of the traps closest to the grid center. In the second, we removed 80 \% of all traps, retaining only the southern 20 \% of the trap array.

\begin{table}[ht]
  \centering
  \caption{Bear results}
    \begin{tabular}{lccccccc}
    \addlinespace
	\hline
          & Mean (SE) & Mode  & 2.5\% & 97.5\% & 97.5\% & RB    & BCI \\ \hline
    {\bf Full data set} &       &       &       &       & {\it } & {\it } & {\it } \\
    {\it D } & 10.556 (1.076) & 10.448 & 8.594 & 12.792 & 143.406 & 0.085 & 96 \\
    $\sigma$ (males) & 7.451 (0.496) & 7.323 & 6.579 & 8.495 & 0.752 & 0.035 & 94 \\
    $\sigma$ (females) & 2.935 (0.143) & 2.939 & 2.671 & 3.226 & 1.195 & 0.008 & 94 \\
    {\bf Grid area – 50\%*} &       &       &       &       & {\bf } & {\bf } & {\bf } \\
    {\it D } & 12.648 (1.838) & 12.205 & 9.307 & 16.713 & 121.878 & 0.003 & 97 \\
    $\sigma$ (males)  & 5.354 (0.511) & 5.248 & 4.472 & 6.473 & 0.623 & 0.014 & 92 \\
    $\sigma$ (females) & 3.318  (0.277) & 3.262 & 2.841 & 3.910 & 2.690 & $<$ 0.001 & 92 \\
    {\bf Grid area – 20\%**} &       &       &       &       & {\bf } & {\bf } & {\bf } \\
    {\it D } & 6.752 (1.611) & 5.953 & 4.000 & 10.218 & 130.020 & 0.029 & 88 \\
    $\sigma$(males)  & 9.881 (3.572) & 7.566 & 5.121 & 18.447 & 0.580 & 0.011 & 93 \\
    $\sigma$ (females)  & 2.686 (0.391) & 2.657 & 2.121 & 3.404 & 5.431 & 0.005 & 97 \\
    \end{tabular}
  \label{design.tab.bears}
\end{table}

Reducing the area of the trap array by 50 \% created a grid polygon of 144 $km^2$, which was smaller than an estimated male black bear home range and only 50 \% larger than a female black bear home range - approximately 260 $km^2$ and 100 $km^2$, respectively, when converting estimates of $\sigma$ to home range size. Table \ref{design.tab.bears} shows that this did not greatly influence model results, compared to the full data set. The observed smaller differences in parameter estimates may be due to individual differences in detection and movement that manifest themselves when only a smaller portion of the overall population is sampled. By reducing the number of traps we effectively reduced the size of the overall data set estimates were based on (both in terms of individuals captured and recaptures). This was reflected in overall higher SE and wider confidence intervals. In spite of these differences, density estimates for black bears – the main objective of applying SCR models – remained largely constant.  Removing 80 \% of the traps and thereby reducing the area of the trap array to 64 $km^2$ - well below the average black bear home range - had a great effect on sample size (only 25 of the original 83 individuals sampled) and thus parameter estimates. Particularly, male black bear movement was overestimated and imprecise. The combination of the low baseline trap encounter rate of males and the considerable reduction in sample size led to a low level of information on male movement: five of the 12 males were captured at one trap only. Although they moved over smaller areas, owing to their higher trap encounter rate females were, on average, captured at more traps (3.4 traps per individual compared to 2.6 for males) so that their movement estimate remained relatively accurate. Overestimated male movements and female trap encounter rates resulted in an underestimate of density of almost 40 \%. This effect is contrary to what we would expect to see in non-spatial CR models, where too small an area leads to underestimated movement and overestimated density (Bondrup-Nielsen 1983; Dillon and Kelly 2007; Maffei and Noss 2008).

\subsection {Final musings: SCR models, trap spacing and array size}
When designing a capture-recapture study for a single species, trap spacing and the size of the array can (and should) be tailored to the spatial behavior of that species to ensure adequate data collection. However, some trapping devices like camera traps may collect data on more than one species and researchers may want to analyze these data, too. Independent of the trapping device used, study design will in most cases face a limit in terms of the number of traps available or logistically manageable. 
Particularly for large mammal research SCR models have much more realistic requirements in terms of area coverage than non-spatial CR models, under which density estimates can be largely inflated with small trapping grids relative to individual movement (Maffei and Noss 2008). How large the spatial survey effort needs to be does not only depend on the extent of movement of the target species, but also on the temporal effort, density and detection probability (Marques et al. 2011) – in summary, the amount of data that can be collected with any given trap array.
While there are limits to the flexibility in spatial trap array design for SCR modeling, the method is fairly robust to changes in trap array size and spacing relative to animal movement. Trapping grids with an extent of approximately a home range diameter can – in theory - adequately estimated density and home range size.
However, these results should not encourage researchers to design non-invasive trap arrays based on minimum area and spacing requirements. Study design should still strive to expose as many individuals as possible to sampling and obtain adequate data on individual movement. Large amounts of data can also improve precision of parameter estimates – the density estimate for the full black bear data set has narrower confidence intervals than estimates from the reduced data sets. This is particularly important when a study is concerned with monitoring population changes.  Also, only with sufficiently large data sets potentially important covariates (such as gender or time effects in the black bear example) can be included into SCR models to obtain density estimates that reflect the actual state of the studied population.


\section {Linear sampling designs}

\section{Cluster samples}

???

\section {Trapping webs}


\section{Models-based Spatial Design}

 
Stuff from my old ms on spatial design.


My email about this --- to focus my thoughts later -----

I dredged up this old spatial design paper I started writing about 3 years ago that deals with computing optimal design for SCR type problems.  My hope is to put a section in Rahel's design chapter on this.  I just wanted to throw some details out there to hear myself think and FWIW.

 The basic idea is this: If you know where a guy lives  then his counts y[s,1:J] are either Binomial or Poisson counts related to some function of alpha0+alpha1*distance(s,x)

  Think about this in the context of a normal linear model then:     y[s] = M'alpha + error

**  We could analyze the design problem for the binomial or Poisson case but I think we can get by just looking at the Normal case which is what I'm going to do here to clarify a couple of thoughts:

The variance-covariance matrix of [alpha0,alpha1]  = solve(M'M)   remember that M is a function of s so lets write this:

 Var(alpha) = solve(M'(s)'M(s))

If we know s we could now easily find the set of "x" coordinates that minimize functions of the variance. Whatever function you want.   But we don't know "s" so we might as well minimize the expected variance:

  E[var(alpha)) = sum_{all s}  solve(M'(s)M(s))  

Two comments:

 (1) This is really easy to do for any number of design points x[1], ..., x[J]  (sometimes it takes a few minutes)

 (2) Interestingly, if you minimize obvious functions of the variance then this produces strongly clustered designs. For example if I pick a design of size 11 then it puts 2 or 3 points in each corner of the square and 1 or 2 points in the center. I think this makes a lot of sense if your objective function is simply to minimize the variance of your eestimates of alpha.

 (3) In fact, it is not sufficient to make a design that is optimal for estimating regression parameters -- we also want to produce a low variance for estimating N.   since n ~ Bin(N,pbar)  we want n to be as close to N as possible, generally speaking.  This suggests that we should find a design that maximizes "pbar" -- i.e., generates the highest expected sample size.

 (4) AS YOU MIGHT EXPECT: These designs are highly regular

 (5) Ok, so what we really would like to do is optimize some function of these three things. We want to minimize the variance of (alpha0, alpha1 and n0).  The problem with this is that there aren't easy formuals for that one  so I haven't figured it out yet ;)

Anyhow, so far the theory works out with heuristic expectations so thats pretty cool.



 

\section {Summary}