\chapter{
Likelihood Analysis of Spatial Capture-Recapture Models
}
\markboth{Chapter 5}{}
\label{chapt.mle}

%%%% TO-DO LIST
% 1. comparison of Bayes with MLE for wolverine data (need to rerun WinBUGS)
% 2. Beth clean up a couple things in SECR analysis.
% 3. Need to finish MLE for restricted state-space
%%   requires code from Rahel
% 4. draft up intlik3 wrapper "scr()"

\vspace{.3in}



In this book we mainly focus on Bayesian analysis of spatial
capture-recapture models. And, in the previous chapters we learned how
to fit some basic spatial capture-recapture models using a Bayesian
formulation of the models analyzed in BUGS engines including {\bf
  WinBUGS} and {\bf JAGS}.  Despite our focus on Bayesian analysis, it
is instructive to develop the basic conceptual and methodological
ideas behind classical analysis based on likelihood methods and
frequentist inference.  In fact, simple SCR models can be analyzed
fairly easily using such methods. This has been the approach taken by
\citet{borchers_efford:2008, dawson_efford:2009} and related papers.

This chapter provides some conceptual and technical footing for
likelihood-based analysis of spatial capture-recapture models. We
recognized earlier (chapt. 4) that SCR models are versions of
binomial (or other) GLMs, but with random effects – i.e., GLMMs. These
models are 
routinely analyzed by likelihood methods. In particular, likelihood
analysis is based on the integrated likelihood in which the random
effects are removed by integration from the likelihood. In SCR models,
the random effect, ${\bf s}$, i.e., the 2-dimensional coordinate, is a
bivariate random effect. 

In this chapter, we show that it is
straightforward to compute the maximum likelihood estimates (MLE) for
SCR models by integrated likelihood. We develop the MLE framework
using {\bf R}, and we also provide a basic introduction to an {\bf R} package
\mbox{\tt secr} \citep{efford:2011} which is based on the stand-alone
package 
{\bf DENSITY} \citep{efford_etal:2004}.
 To set the context we analyze the SCR model
here when $N$ is known because, in that case, it is precisely a GLMM and
does not pose any difficulty at all. We generalize the model to allow
for unknown $N$ using both conventional ideas based on the ``joint
likelihood'' \citep[e.g.,][]{borchers_etal:2002}
and also using a formulation
based on data augmentation.  We obtain the MLEs for 
the SCR model from the wolverine camera trapping study \citep{magoun_etal:2011}
 analyzed in previous chapters to compare/contrast the
results.

\section{Likelihood analysis }

We noted in chapter 4 that, with $N$ known, the basic SCR model is a
type of binomial regression with a random effect. For such models we
can easily obtain maximum likelihood estimators of model parameters
based on integrated likelihood. The integrated likelihood is based on
the marginal distribution of the data $y$ in which the random effects
are removed by integration. Conceptually, our model is a specification
of the conditional-on-${\bf s}$ model $[y|{\bf s},\theta]$ and we have
a ``prior distribution'' for ${\bf s}$, say $[{\bf s}]$, and the
marginal distribution of the data $y$ is
\[
[y|\theta] =  \int_{\bf s} [y|{\bf s},\theta][{\bf s}] d{\bf s}.
\]
When viewed as a function of $\theta$ for pursposes of estimation, the
marginal distribution $[y|\theta]$ is often referred to as the {\it
  integrated likelihood}.

It is worth analyzing 
the simplest SCR model with known-$N$ in order to understand the
underlying mechanics and basic concepts. These are directly relevant to
the manner in which many capture-recapture models are classically
analyzed, such as model Mh, and individual covariate models (see
chapt. \ref{chapt.closed} and  \citet[][chapt. 6]{royle_dorazio:2008}). To develop integrated
likelihood for SCR models, we first identify the conditional
likelhiood. 

The observation model for each encounter observation $y_{ij}$,
specified conditional on ${\bf s}_{i}$, is 
\begin{equation}
	y_{ij}| {\bf s}_{i} \sim \mbox{Bin}(K, p_{\theta}({\bf x}_{j},{\bf s}_{i}))
\label{mle.eq.cond-on-s}
\end{equation}
where we have indicated the dependence of $p_{ij}$ on ${\bf s}$ and
parameters $\theta$
explicitly.
For the random effect we have ${\bf s}_{i} \sim  \mbox{Unif}({\cal
  S})$.
The joint distribution of the data for individual $i$ is the product
of $J$ such terms (i.e., contributions from each of $J$ traps).
\[
  [{\bf y}_{i} | {\bf s}_{i} , \theta] = 
  \prod_{j=1}^{J} \mbox{Bin}(K, p_{\theta}({\bf x}_{j},{\bf s}_{i}) )
\]
We note that this assumes that encounter of individual $i$ in each
trap is independent of encounter in every other trap, conditional on
${\bf s}_{i}$, this is the fundamental property of SCR0 or
``multi-catch'' traps. 

 The so-called marginal likelihood is computed by removing
${\bf s}_{i}$, by integration (hence also {\it integrated} likelihood), from the conditional-on-${\bf s}$
likelihood and regarding the {\it marginal} distribution of the data
as 
the likelihood. That
is, we compute:
\[
  [y|\theta] = 
\int_{{\cal S}}  [ \theta | {\bf y}_{i} |{\bf s}_{i}] g({\bf s}_{i}) d{\bf s}_{i}
\]
In most SCR models, $g({\bf s}) = 1/||{\cal S}||$ (but see chapt. \ref{chapt.ipp}).

The joint likelihood for all $N$ individuals, assuming independence of
encounters among individuals, is the product of $N$ such terms:
\[
{\cal L}(\theta | {\bf y}_{1},{\bf y}_{2},\ldots, {\bf y}_{N}) =     \prod_{i=1}^{N}
[{\bf y}_{i}|\theta]
\]
We emphasize that two independence assumptions are explicit in this
development: independence of trap-specific encounters within
individuals and also independence among individuals. In particular,
this would only be valid when individuals are not physically
restrained or removed upon capture, and when traps do not ``fill up''.

The key operation for computing the likelihood is solving a
2-dimensional integration problem. There are some general purpose {\bf
  R} packages that implement a number of 
 multi-dimensional integration routines
including \mbox{\tt adapt} \citep{genz_etal:2007} and \mbox{\tt R2cuba}
\citep{hahn_etal:2011}.  In practice, we won't rely
on these extraneous {\bf R} packages (except see
chapt. \ref{chapt.ipp} for an application of \mbox{\tt Rcuba})
but instead will use perhaps less
efficient methods in which we replace the integral with a summation
over an equal area mesh of points on the state-space ${\cal S}$ and explicitly
evaluate the integrand at each point. We invoke the rectangular rule
for integration here\footnote{e.g., 
\url{http://en.wikipedia.org/wiki/Rectangle_method}
} in which we
evaluate the
integrand on a regular grid of points of equal area and compute the
average of
the integrand over that grid of points. 
Let $u=1,2,\ldots,nG$ index a grid of
$nG$ points, ${\bf s}_{u}$,  where the area of grid cell $u$ is
constant, say $A$.
In this case, the integrand, i.e., the marginal pmf of 
${\bf y}_{i}$, is approximated by  
\begin{equation}
         [{\bf y}_{i}|\theta] = \frac{1}{nG} \sum_{u=1}^{nG}  [ {\bf
            y}_{i} |{\bf s}_u, \theta]
\label{mle.eq.intlik}
\end{equation}

This is a specific case of the general expression that could be used
for approximating the integral for any arbitrary (bivariate or otherwise)
distribution $g({\bf s})$. The general case is
\[
[y]  = \frac{A}{nG} \sum_{u} [y|{\bf s}_{u}] [{\bf s}_{u}]
\]
 In the present context it happens that  $[{\bf s}] = (1/A)$
and thus the grid-cell area cancels in the above
expression to yield eq. \ref{mle.eq.intlik}, but we commonly apply
this in the context of normal prior distributions, as we did for
likelihood analysis of Model $M_{h}$ in  sec. \ref{closed.sec.modelmh}).
The rectangular rule for integration can be seen as an application of
the Law of Total Probability for a discrete random variable ${\bf
  s}$, having $nG$ 
unique values with equal probabilities $1/nG$.



\subsection{ Implementation (simulated data)}

Here we will illustrate how to carryout this integration and
optimization based on the integrated likelihood using simulated data
 (i.e., following that from Chapter 4). Using \mbox{\tt simSCR0.fn}
 we simulate data for 100 individuals and a 25 trap array
layed out in a $5 \times 5$ grid of unit spacing.  The specific encounter
model is the half-normal model. The 100 activity centers were
simulated on a state-space defined by a $8 \times 8$ square 
within which the
trap array was centered (thus the trap array is buffered by 2
units). Therefore, the density of individuals in this system is fixed
at $100/64$.

In the following set of R commands we generate the data and 
then harvest the required data objects:
\begin{verbatim}
data<-simSCR0.fn(discard0=FALSE,sd=2013)
y<-data$Y
traplocs<-data$traplocs
nind<-nrow(y)
X<-data$traplocs
J<-nrow(X)
K<-data$K
Xl<-data$xlim[1]
Yl<-data$ylim[1]
Xu<-data$xlim[2]
Yu<-data$ylim[2]
\end{verbatim}
Now we need to define the integration grid, say ${\bf G}$, which we do with
the following set of {\bf R} commands (here, \mbox{\tt delta} is the grid spacing):
\begin{verbatim}
delta<- .2
xg<-seq(Xl+delta/2,Xu-delta/2,by=delta) 
yg<-seq(Yl+delta/2,Yu-delta/2,by=delta) 
npix<-length(xg)          # assumes xg and yg same dimension here
area<- (Xu-Xl)*(Yu-Yl)/((npix)*(npix)) # don’t need area for anything
G<-cbind(rep(xg,npix),sort(rep(yg,npix)))
nG<-nrow(G)
\end{verbatim}
In this case, the integration grid is set up as a grid with spacing
$\delta = 0.2$ which produces a $40 \times 40$ grid of points for evaluating the
integrand if the state-space buffer is set at 2.

We next create an {\bf R} function that defines the likelihood as a function
of the data objects $y$ and $X$ which were created above but, in general,
you would read these files into {\bf R}, e.g., from a .csv file.
In addition to these data
objects, we need to have defined the  quantities $G$ and $nG$ associated
with the integration grid.
However, instead of worrying about making all of these objects and
keeping track of them we just put that code above into the likelihood
function and pass $\delta$ as an additional (optional) argument and a
few other things that we need such as the boundary of the state-space
over which the integration (summation) is being done.
Here is one reasonably useful variation of a function for estimation
based on the integrated likelihood:

{\small 
\begin{verbatim}
intlik1<-function(parm,y=y,delta=.2,X=traplocs,ssbuffer=2){

Xl<-min(X[,1]) - ssbuffer 
Xu<-max(X[,1]) + ssbuffer
Yu<-max(X[,2]) + ssbuffer
Yl<-min(X[,2]) - ssbuffer

xg<-seq(Xl+delta/2,Xu-delta/2,,length=npix) 
yg<-seq(Yl+delta/2,Yu-delta/2,,length=npix) 
npix<-length(xg)

G<-cbind(rep(xg,npix),sort(rep(yg,npix)))
nG<-nrow(G)
D<- e2dist(X,G)  

alpha<-parm[1]
theta<-parm[2]
probcap<- plogis(alpha)*exp(-theta*D*D)
Pm<-matrix(NA,nrow=nrow(probcap),ncol=ncol(probcap))
                    # all zero encounter histories
n0<-sum(apply(y,1,sum)==0) 
                    # encounter histories with at least 1 detection
ymat<-y[apply(y,1,sum)>0,] 
ymat<-rbind(ymat,rep(0,ncol(ymat)))
lik.marg<-rep(NA,nrow(ymat))
for(i in 1:nrow(ymat)){
Pm[1:length(Pm)]<- (dbinom(rep(ymat[i,],nG),K,probcap[1:length(Pm)],log=TRUE))
lik.cond<- exp(colSums(Pm))
lik.marg[i]<- sum( lik.cond*(1/nG))  
}
nv<-c(rep(1,length(lik.marg)-1),n0)
-1*( sum(nv*log(lik.marg)) )
}
\end{verbatim}
}


The function accepts as
input the encounter history matrix, $y$, the trap locations, $X$, and the
state-space buffer. This allows us to vary the state-space buffer and
easily evaluate the sensitivity of the MLE to the size of the
state-space. 
Note that we have a peculiar handling of the encounter history
matrix $y$. In particular, we remove the all-zero encounter histories
from the matrix and tack-on a single all-zero encounter history as the
last row which then gets weighted by the number of such encounter
histories ($n0$). This is a bit long-winded and strictly unnecessary
when $N$ is known, but we did it this way because the extension to the
unknown-$N$ case is now transparent (as we demonstrate in the following
section). 
 The matrix $Pm$ holds the log-likelihood contributions of
each encounter frequency for each possible state-space location of the
individual. 
The log contributions are summed up and the result
exponentiated on the next line, producing lik.cond, the
conditional-on-s likelihood (Eq. \ref{mle.eq.cond-on-s}
above). The marginal
likelihood (\mbox{\tt lik.marg}) sums up the conditional elements weighted by
$\Pr({\bf s})$ (Eq. \ref{mle.eq.intlik} above).
This is a fairly primative function which doesn't allow much
flexibility in the data structure. For example, it assumes that $K$,
the number 
of replicates, is constant for each trap. Further, it assumes that the
state-space is a square. We generalize this to some extent later in
this chapter. 

Here is the {\bf R} command for maximizing the likelihood and saving the
results into an object called \mbox{\tt frog}.  The output is a list of the
following structure and these specific estimates are produced using
the simulated data set:

{\small 
\begin{verbatim}
# should take 15-30 seconds

> starting.values <- c(-2, 2) 
> frog<-nlm(intlik1,starting.values,y=y,delta=.1,X=traplocs,ssbuffer=2,hessian=TRUE)
> frog

$minimum
[1] 297.1896

$estimate
[1] -2.504824  2.373343

$gradient
[1] -2.069654e-05  1.968754e-05

$hessian
          [,1]      [,2]
[1,]  48.67898 -19.25750
[2,] -19.25750  13.34114

$code
[1] 1

$iterations
[1] 11
\end{verbatim}
} 
Details about this output can be found on the help page for
\mbox{\tt nlm}. We note briefly that \mbox{\tt frog\$minimum} is the
negative log-likelihood value at the MLEs, which are stored in the
\mbox{\tt frog\$estimate} component of the list. The hessian is the
observed Fisher information matrix, which can be inverted to obtain
the variance-covariance matrix using the commands:
\begin{verbatim}
> solve(frog$hessian)
\end{verbatim}

It is worth drawing attention to the fact that the estimates are
different than the Bayesian estimates reported previously in chapt. \ref{chapt.scr0}.
How can that be?!  There are several reasons for
this.  First Bayesian inference is based on the posterior distribution
and it is not generally the case that the MLE should correspond to any
particular value of the posterior distribution. If the prior
distributions in a Bayesian analysis are uniform, then the
(multivariate) mode of the
posterior is the MLE, but note that Bayesians almost always report
posterior {\it means} and so there will typically be a discrepancy
there. Secondly, we have implemented an approximation to the integral
here and there might be a slight bit of error induced by that. We will
evaluate that shortly. Third, the Bayesian analysis by MCMC is subject
to some amount of Monte Carlo error which the analyst should always be
aware of in practical situations.  All of these different explanations
are likely responsible for some of the discrepancy. Accounting for
these, we see general consistency between the
two estimates.

To compute the integrated likelihood we used a discrete representation
of the state-space so that the integral could be approximated as a
summation over possible values of ${\bf s}$ with each value being
weighted by its probability of occurring, which is $1/nG$ under the
assumption that ${\bf s}$ is uniform on the state-space ${\cal
  S}$. Recall
in chapt. \ref{chapt.scr0} we 
used a discrete state-space in developing a Bayesian analysis of the
model in order to be able to modify the state-space in a flexible
manner. In that case, we could use the discretized state-space as the
integration grid and just feed it into our integrated likelihood
routine. 

In summary, we note that, for the basic SCR model, integrated
likelihood is a really easy calculation when $N$ is known. Even for $N$
unknown it is not too difficult, and we will do that shortly.
However, if you can solve the known-$N$ problem then you should be able
to do a real analysis, for example by considering different values of
$N$ and computing the results for each value and then making a plot of
the log-likelihood or AIC and choosing the value of $N$ that produces
the best log likelihood or AIC. As a homework problem we suggest that
the reader take the code given above and try to estimate $N$ without
modifying the code – by just repeatedly calling that code for
different values of $N$ and trying to deduce the best value.
Nevertheless, we will formalize the unknown-$N$ problem shortly.

The
software package {\bf DENSITY} \citep{efford_etal:2004} implements
certain types of SCR models using integrated likelihood methods.
{\bf DENSITY} has been made into an {\bf R} package called \mbox{\tt secr} \citep{efford:2011}
and we provide an analysis of some data using \mbox{\tt secr} shortly along
with a discussion of its capabilities.


\section{MLE when N is Unknown} 

Here we build on the previous introduction to integrated likelihood
but we consider now the case in which $N$ is unknown. We will see that
adapting the analysis based on the $N$-known model is really
straightforward for the more general problem. The main distinction is
that we don’t observe the all-zero encounter history so we have to
make sure we compute the probability for that encounter history which
we do by tacking a row of zeros onto the encounter history matrix. In
addition, we include the number of such all-zero encounter histories
as an unknown parameter of the model. Call that unknown quantity $n_{0}$.
In addition, we have to be sure to include a combinatorial term to
account for the fact that of the $n$ observed individuals there are
${N \choose n}$
 ways to realize a sample of size $n$. The combinatorial term
involves the unknown $n_{0}$ and thus it must be included in the likelihood.

Operationally then, things proceed much as before: 
We compute the marginal probability of each observed ${\bf y}_{i}$,
i.e., by removing the latent ${\bf s}_{i}$ by integration. In
addition, we 
 compute the marginal probability of the ``all-zero'' encounter
history ${\bf y}_{n+1}$, and make sure to weight it $n_{0}$ times. We
accomplish this by ``padding'' the data set with a single encounter
history having $y_{n+1,j}=0$ for all traps $j=1,2,\ldots,J$. Then we
be sure to include the combinatorial term in the likelihood or
log-likelihood computation. We demonstrate this shortly.

To analyze a specific case, we’ll read in our fake data set (simulated
using the parameters given above). To set some things up in our
workspace we do this:
\begin{verbatim}
data<-simSCR0.fn(discard0=TRUE,sd=2013)
y<-data$Y
nind<-nrow(y)
X<-data$traplocs
J<-nrow(X)
K<-data$K
Xl<-data$xlim[1]
Yl<-data$ylim[1]
Xu<-data$xlim[2]
Yu<-data$ylim[2]
\end{verbatim}

Recall that these data were generated with $N=100$, on an $8 \times 8$ unit
state-space representing the trap locations (${\bf X}$) buffered by 2 units.
As before, the likelihood is defined in the {\bf R} workspace as an
{\bf R}
function which takes an argument being the unknown parameters of the
model and additional arguments as prescribed. In particular, 
 we provide the encounter history matrix ${\bf y}$, the trap locations
\mbox{\tt traplocs}, the spacing of the integration grid ($\delta$) and the
state-space buffer. Here is the new likelihood function:
\begin{verbatim}
intlik2<-function(parm,y=y,delta=.3,X=traplocs,ssbuffer=2){

Xl<-min(X[,1]) -ssbuffer
Xu<-max(X[,1])+ ssbuffer
Yu<-max(X[,2])+ ssbuffer
Yl<-min(X[,2])- ssbuffer

#delta<- (Xu-Xl)/npix
xg<-seq(Xl+delta/2,Xu-delta/2,delta) 
yg<-seq(Yl+delta/2,Yu-delta/2,delta) 
npix.x<-length(xg)
npix.y<-length(yg)
area<- (Xu-Xl)*(Yu-Yl)/((npix.x)*(npix.y))
G<-cbind(rep(xg,npix.y),sort(rep(yg,npix.x)))
nG<-nrow(G)
D<- e2dist(X,G) 

alpha<-parm[1]
theta<-parm[2]
n0<-exp(parm[3])
probcap<- plogis(alpha)*exp(-theta*D*D)
Pm<-matrix(NA,nrow=nrow(probcap),ncol=ncol(probcap))
ymat<-rbind(y,rep(0,ncol(y)))

lik.marg<-rep(NA,nrow(ymat))
for(i in 1:nrow(ymat)){
Pm[1:length(Pm)]<- (dbinom(rep(ymat[i,],nG),K,probcap[1:length(Pm)],log=TRUE))
lik.cond<- exp(colSums(Pm))
lik.marg[i]<- sum( lik.cond*(1/nG) )  
}                                                 
nv<-c(rep(1,length(lik.marg)-1),n0)
part1<- lgamma(nrow(y)+n0+1) - lgamma(n0+1)
part2<- sum(nv*log(lik.marg))
 -1*(part1+ part2)
}
\end{verbatim}

To execute this function for the data that we created with \mbox{\tt simSCR0.fn},
 we execute the following command (saving the result in our
friend \mbox{\tt frog}).
This results in the usual output, including the parameter estimates,
the gradient, and the numerical Hessian which is useful for obtaining
asymptotic standard errors (see below):
\begin{verbatim}
> frog<-nlm(intlik2,c(-2.5,2,log(4)),hessian=TRUE,y=y,X=X,delta=.2,ssbuffer=2)
There were 50 or more warnings (use warnings() to see the first 50)
> 
>
> frog
$minimum
[1] 113.5004

$estimate
[1] -2.538334  2.466515  4.232810

[…. Additional output deleted ….]
\end{verbatim}
While this produces some {\bf R} warnings, these happen to be harmless
in this case, and we will see from the \mbox{\tt nlm} output that the
algorithm performed satisfactory in minimizing the objective function.
The estimate of population size for the state-space (using the default 
state-space buffer) is
\begin{verbatim}
> nrow(y)+exp(4.2328)
[1] 110.9099
\end{verbatim}
Which differs from the data-generating value ($N=100$) as we might
expect. We usually will present an estimate of uncertainty assocated
with this MLE which we can obtain by inverting the Hessian. Note that
$Var(\hat{N}) = n + \mbox{Var}(\hat{n}_{0})$.
Since we
have parameterized the model in terms of $log(n_{0})$ we use a delta
approximation to obtain the variance on the scale of $n_{0}$ as
follows:
\begin{verbatim}
> (exp(4.2328)^2)*solve(frog$hessian)[3,3]
[1] 260.2033
> sqrt(260)
[1] 16.12452
\end{verbatim}
Therefore, the asymptotic ``Wald-type'' confidence interval for $N$ is
$110.91 +/- 1.96 \times 16.125 = (79.305, 142.515)$. To report this in
terms of density, we scale appropriately by the area of the prescribed
state-space which is $64$ units of area (i.e., an $8 \times 8$ square).

\subsection{Exercises}

{\flushleft 
{\bf 1.}	
Run the analysis with different state-space buffers and comment on the result. 
}


{\flushleft 
{\bf 2.} Conduct a brief simulation study using this code by
  simulating 100 data sets and obtain the MLEs for each data set. Do
  things seem to be working as you expect?  }

{\flushleft 
{\bf 3.} 
Further extensions: It should be straightforward to
  generalize the integrated likelihood function to accommodate many
  different situations. For examples, if we want to include more
  covariates in the model we can just add stuff to the object \mbox{\tt probcap},
 and add the relevant parameters to the argument that gets
  passed to the main  function.  For the simulated data, make up a
  covariate by generating a Bernoulli covariate (``trap type'' – perhaps
  baited or not baited) randomly and try to modify the likelihood to
  accommodate that.  }

{\flushleft {\bf 4.}  We would probably be interested in devising the
  integrated likelihood for the full 3-d encounter history array so
  that we could include temporally varying covariates. This is not
  difficult but naturally will slow down the execution
  substantially. The interested reader should try to expand the
  capabilities of this basic {\bf R} function.  }


\subsection{Integrated Likelihood using the model under data augmentation } 

Note that this likelihood analysis is based on the standard likelihood
in which $N$ (or $n_{0}$) is an explicit parameter. This is usually called
the ``joint likelihood'' or ``unconditional likelihood''.  We could also
express the joint likelihood using data augmentation, replacing the
parameter $N$ with $\psi$ \citep[e.g., see sec. 7.1.6][for an example]{royle_dorazio:2008}.
We don't go into detail here, but we note that the
likelihood under data augmentation is a zero-inflated binomial
mixture – precisely an occupancy type model \citep{royle:2006}.
Thus, while it is possible to carryout likelihood analysis of
models under data augmentation, we primarily advocate data
augmentation for Bayesian analysis.


\subsection{ Extensions}

We have only considered basic SCR models with no additional
covariates. However, in practice, we are interested in other types of
covariate effects including ``behavioral response'', 
sex-specificity of parameters, and potentially other effects. Some of
these  can be added directly to the likelihood – if the covariate is fixed
and known for all individuals captured or not. An example is a
behavioral response, which amounts to having a covariate $x_{ik}=1$ if
individual $i$ was captured prior to occasion $k$ and $x_{ik}=0$
otherwise. For uncaptured individuals, $x_{ik}=0$ for all $k$.
 \citet{royle_etal:2011jwm} called this a global behavioral
response because the covariate is defined for all traps, no matter the
trap in which an individual was captured. We could also define a {\it
  local} behavioral response which occurs at the level of the trap,
i.e., $x_{ijk}=1$ if individual $i$ was captured in trap $j$ prior to
occassion $k$, etc.. 
Trap-specific covariates such as trap type or status, or
time-specific covariates such as date, are easily accommodated as
well. As an example, \citet{kery_etal:2010} develop a model for the
European wildcat in which traps are either baited or not (a
trap-specific covariate with only 2 values), and also encounter
probability varies over time in the form of a quadratic seasonal response.
We consider models with behavioral response or fixed covariates in
chapter XXXX, although 
the integrated likelihood routines we provided above can be
modified directly for such cases, which we leave to the interested reader. 

Sex-specificity is more difficult to deal with since sex is not known
for uncaptured individuals (and sometimes not even for all captured
individuals).  To analyze such models, we do Bayesian analysis of the
joint likelihood facilitated by the use of data augmentation
\citep{gardner_etal:2010,russell_etal:2012}. For covariates that are
not fixed and known for all individuals, it is somewhat more
challenging to do MLE for these based on the joint likelihood as we
have developed above. Instead it is more conventional to use what is
colloquially referred to as the ``Huggins-Alho'' type model which is
one of the approaches taken in the software package \mbox{\tt secr}
\citep[][see sec. \ref{mle.sec.secr}]{efford:2011}. This idea is
motivated by thinking about unequal probability sampling methods known
as Horvitz-Thompson sampling \citep[e.g.,
see][]{overton_stehman:1995}.  We don't use that method anywhere in
this book because it represents a paradigm shift in the inference
framework which is done historically only for convenience (i.e., ease
of constructing an estimator) and not for philosophical or theoretical
reasons.






\section{Classical model selection and assessment}

In most analyses, one is interested in choosing from among various
potential models. A good thing about classical analysis based on
likelihood is we can apply AIC methods without difficulty
\citep{burnham_anderson:2002}. There are two distinct contexts for
model-selection that we think are relevant to SCR models. First is
selecting among models that represent distinct biological hypotheses
(e.g., covariates affecting encounter probability or density), and AIC
is convenient for assessing the relative merits of these different
models although if there are only a few models it is not objectionable
to use hypothesis tests or confidence intervals to determine
importance of effects. The second context is selecting among various
detection functions. 
Indeed, when distance is used as a covariate (e.g., distance
sampling), AIC is usually
applied to some large and arbitrary selection of distance
functions with no biological motivation.
As a general rule, we don’t recommend this given there is hardly ever (if at
all) a rational subject-matter based reason motivating specific 
distance functions. As a result, we believe that doing too much model
selection will
invariably lead to over-fitting and thus over-statement of
precision. This is the main reason that we haven't loaded you down
with a basket of models for detection probability so far, although we
discuss many possibilities in chapter XYZ. 


Goodness-of-fit: For many standard capture-recapture models, it is
possible to identify goodness-of-fit statistics based on the
multinomial likelihood and evaluate model adequancy using formal
statistical tests. Similar strategies can be applied to SCR models
using expected cell-frequencies based on the marginal distribution of
the observations. Also, because computing MLEs is somewhat more
efficient in many cases compared to Bayesian analysis, it is also
sometimes easy to use bootstrap methods\footnote{I could use some
  references in the context of SCR models for this stuff}.
 Bayesian goodness-of-fit is almost always addressed with
Bayesian p-values or some other posterior predictive check (chapter 2 REF
XXX and see chapter \ref{chapt.gof}). 
\citet{royle_etal:2011mee} suggested checking model fit by decomposing
fit into two components: an evaluation of the encounter process model
based on expected encounter frequencies computed {\it conditional} on
${\bf s}$, and then independent evaluation  of the ``spatial randomness''
hypotehesis. We discuss this in chapter \ref{chapt.gof}.


\section{Likelihood analysis of the wolverine camera trapping data}
\label{mle.sec.wolverine}

{\bf ANDY STOPPED HERE}

Here we compute the MLEs for the wolverine data using an expanded
version of the function we developed in the previous section. To
accommodate that each trap might be operational a variable number of
nights, we provided an additional argument to the likelihood function
(allowing for a vector $K$), which requires also a modification to the
construction of the likelihood.  In addition,
we had to accommodate that the state-space is a general rectangle, and
we included a line in the code to compute the state-space area which
we apply below for computing density.  The more general function
(\mbox{\tt intlik3}) is given in the {\bf R} package. It has a general
purpose wrapper named \mbox{\tt scr}\footnote{Not written yet} which has other capabilities too. 

The data were read into our R session and manipulated using the
following commands. Note that we use the utility {\bf R} function
\mbox{\tt SCR23darray.fn} which we defined in chapt. 4.

\begin{verbatim}
> wcaps<-source("wcaps.R")$value
> wtraps<-source("wtraps.R")$value
> K.wolv<-apply(wtraps[,4:ncol(wtraps)],1,sum)
> 
> xx<-SCR23darray.fn(wcaps,ntraps=37,nperiods=165)
> y.wolv<- apply(xx,c(1,3),sum)
> traplocs.wolv<-wtraps[,2:3]
> traplocs.wolv<-traplocs.wolv/10000
>
> frog<-nlm(intlik3,c(-1.5,1.2,log(4)),hessian=TRUE,y=y.wolv,K=K.wolv,X=traplocs.wolv,delta=.1,ssbuffer=2)
There were 23 warnings (use warnings() to see them)
> frog

$minimum
[1] 220.4355

$estimate
[1] -2.817570  1.255112  3.599040

$gradient
[1] -6.274309e-06  2.146722e-05 -1.045566e-05

$hessian
           [,1]       [,2]      [,3]
[1,]  37.687931 -11.852236  4.688911
[2,] -11.852236  30.846144 -9.199113
[3,]   4.688911  -9.199113 13.050428

$code
[1] 1

$iterations
[1] 12

> exp(3.599)*sqrt(solve(frog$hessian)[3,3])
[1] 11.41059
> 

\end{verbatim}

We optained the MLEs for a state-space buffer of 2 (standardized
units) and for integration grid with spacing $\delta = .3, .2, .1,
.05$. The MLEs for these 4 cases including the relative runtime are
given in Table \ref{mle.tab.integration}.

\begin{table}[ht]
\centering
\caption{Run time and MLEs for different integration grid resolutions.}
\begin{tabular}{l|rccc}
\hline \hline
$\delta$ &   & \multicolumn{3}{c}{Estimates} \\ \hline
         &  runtime        & $\alpha_0$ & $\theta$ & $log(n_0)$ \\ \hline
 0.30   &  9.9  &  -2.819786 & 1.258468 & 3.569731  \\
 0.20   & 32.3  &  -2.817610 & 1.254757 & 3.583690 \\
 0.10  & 115.1  &  -2.817570 & 1.255112 & 3.599040 \\
 0.05 &  407.3 &   -2.817559&  1.255281&  3.607158 \\
\end{tabular}
\label{mle.tab.integration}
\end{table}


We see the results change only slightly as the fineness of the
integration grid increases. Conversely, the runtime on the platform of
the day for the 4 cases increases rapidly. 
As we have suggested previously these runtimes could be regarded in
relative terms,  across platforms, for gaging the decrease in
speed as the fineness of the integration grid increases. The effect of
this is that we anticipate some numerical error in approximating the
integral on a mesh of points, and that error increases as the
coarsenss of the mesh increases. 


We studied the effect of the state-space buffer on the MLEs,
using a fixed $\delta = .2$ for all analyses. We used state-space buffers
of 1 to 4 units stepped by .5. This produced the following results,
given here are the state-space buffer, area of the state-space, the
MLE of $N$ for the prescribed state-space and the corresponding MLE of
density:\footnote{uniformly about 20\% higher than ch. 4
  estimates. Error in ch. 4 code needs to be rerun.}
\begin{verbatim}
     ssbuff       Ass      Nhat      Dhat
[1,]    1.0  66.98212  37.73338 0.5633352
[2,]    1.5  84.36242  46.21008 0.5477567
[3,]    2.0 103.74272  57.00617 0.5494956
[4,]    2.5 125.12302  69.03616 0.5517463
[5,]    3.0 148.50332  82.17550 0.5533580
[6,]    3.5 173.88362  96.44018 0.5546249
[7,]    4.0 201.26392 111.83524 0.5556646
\end{verbatim}
The estimates of $D$ stabilize rapidly and the incremental difference
is within the numerical error associated with approximating the
integral.  The results suggest that wolverine density is around $0.56$
individuals per $100$ $km^2$ (recall that a state-space unit is $10
\times 10$ $km$).  This is about $5.6$ individuals per thousand $km^2$
which compares with XXX XYZ\footnote{insert final} reported in
sec. \ref{scr0.sec.wolverine}.


\subsection{Restricted state-space}

In section \ref{scr0.sec.discrete} 
back in chapt. 4 we used a discrete representation of
the state-space in order to have control over its extent and shape,
for example so that we could clip out ``non-habitat''. Clearly that
formulation of the model is relevant to the use of integrated
likelihood in the sense that such a representation of the state-space
underlies the computation of the integral. Thus, for example, we could
easily compute the MLE of parameters under some model with a
restricted state-space merely by creating the required state-space at
whatever grid resolution is desired, and then feed that state-space
into the likelihood evaluation above. The {\bf R} function \mbox{\tt scr}
which comes with the {\bf R} package for this book accommodates an
arbitrary state-space fashioned in this manner, as well as
state-spaces created by polygons or GIS shapefiles.

XYZ-lookup-XYZ reported in
\citet{royle_etal:2011jwm} based on a clipped state-space as described
in section XYZ (XYZ chapter 4 XYZ).

{\bf TO BE COMPLETED}


\subsection{
Exercises
}

{\flushleft
1.	Compute the 95\% confidence interval for wolverine density,
somehow. Comment on the practical implication of this level of precision.
}

{\flushleft
2.	Compute the AIC of this model and modify \mbox{\tt intlik3}
 to consider alternative link functions (at least one additional) and
 compare the  AIC of the different models and the estimates. Comment. 
}

\section{Program DENSITY and the R package \mbox{\tt secr} }
\label{mle.sec.secr}


{\bf DENSITY} is a software program developed by \citet{efford:2004}
for fitting spatial capture-recapture models based mostly on classical
maximum likelihood estimation and related inference methods.
\citet{efford:2011} has also released an {\bf R} package named
\mbox{\tt secr}, that contains much of the functionality of {\bf
  DENSITY} but also incorporates new models and features.  Here, we
will focus on \mbox{\tt secr} as it will continue to be developed,
contains more functionality and is based in {\bf R}.


 To install
and run models in \mbox{\tt secr}, you must download the package and load itin
{\bf R}.
\begin{verbatim}
> install.packages(“secr”)
> library(secr)
\end{verbatim}
\mbox{\tt secr} allows the user to simulate data and fit a suite of models with
various detection functions and covariate responses.  \mbox{\tt secr}
uses the
standard {\bf R} model specification framework using tildes. E.g., the model
command is \mbox{\tt secr.fit} and is generally written as
\begin{verbatim}
> secr.fit(capturedata, model = list(D~1, g0~1, sigma~1), buffer = 20000)
\end{verbatim}
where we have \verb#g0~1# indicating the intercept model. 
 Possible predictors for detection probability include both
pre-defined variables (e.g., \mbox{\tt t} and \mbox{\tt b}
corresponding to ``time'' and 
``behavior''), and user-defined covariates of several kinds. 
For example, to include a behavioral response, this would be written
as \verb#g0~b#.
The discussion of covariates is developed more in chapter XX(8)\footnote{Beth:
  does secr fit a local trap-specific response or just a global
  behavioral response?}

Before we can fit the models, the data must first be packaged properly
for 
\mbox{\tt secr}.  Two input files are required: trap layout (location and
identification information for each trap) and capture data (e.g.,
sampling session, animal identification, trap day, and trap location).
\mbox{\tt secr} requires that you specify the trap type, the two most common for
camera trapping/hair snares are ‘proximity’ detectors and ‘count’
detectors.  The `proximity' detector type allows, at most, one
detection of each individual at a particular detector on any occasion.
The ‘count’ detector designation allows repeat encounters of each
individual at a particular detector on any occasion.  There are other
detector types that one can select such as: `polygon' detector type
which allows for a trap to be a sampled polygon, e.g., scat surveys,
and 'signal' detector which allows for traps that have a strength
indicator, e.g., acoustic arrays.  The detector types ‘single’ and
‘multi’ can be confusing as ‘multi’ seems like it would appropriate
for something like a camera trap, but instead these two designations
refer to traps that retain individuals, thus precluding the ability
for animals to be captured in other traps during the sampling
occasion.  The ‘single’ type indicates trap that can only catch one
animal at a time, while ‘multi’ indicates traps that may catch more
than one animal at a time.  For a full review of the detector types,
one should look at the help manual, which can be accessed in {\bf R} after
installing the \mbox{\tt secr} package by using the command:
\begin{verbatim}
> RShowDoc("secr-manual", package = "secr")
\end{verbatim}
As with all of the scr models, \mbox{\tt secr} fits a detection function relating
the probability of detection to the distance of a detector from an
individual activity center. \mbox{\tt secr} allows the user to specify one of a
variety of detection functions including the commonly used
half-normal, hazard rate, and exponential.  There are 12 different
functions, but some are only available for simulating data, and one
should take caution when using different detection functions as the
interpretation of the parameters, such as sigma, may not be consistent
across formulations.  The different detection functions are defined in
the secr manual and can be found by calling the help function for the
detection function:
\begin{verbatim}
> ?detectfn
\end{verbatim}
It is useful to note that \mbox{\tt secr} requires the buffer distance to be
defined in meters and density will be returned as number of animals
per hectare.  Thus to make comparisons between \mbox{\tt secr} and other models,
we will often have to convert the density to the same units.  Also,
note that sigma is returned in units of meters.

\footnote{One question: SECR only ever reports “sigma”. What exactly is sigma?  It is a scale parameter of a detection function and all detection functions have a scale parameter. But in what sense is this sigma parameter related to “home range diameter”?  Efford doesn’t explain this, does he?  In some sections in chapter 4 or possibly 6 we get into this issue. 
}

\subsection{ Analysis using the \mbox{\tt secr} package}

To demonstrate the use of the \mbox{\tt secr} package, we will show how to do the
same analysis on the wolverine study as shown in section 4.6.  To use
the \mbox{\tt secr} package, the data need to be formatted in a similar but
slightly different manner than we use in {\bf
  WinBUGS}\footnote{Elaborate on this point -- and how is this
  different than introduced in chapter 4?}.  After installing
the \mbox{\tt secr} package, we first have to read in the trap locations and
other related information, such as if the trap is operational during a
sampling occasion.  The \mbox{\tt secr} package reads in the trap data through a
command called ``\mbox{\tt read.traps}'', which requires the detector type as
input.  The detector type is important because it will determine the
likelihood that \mbox{\tt secr} will use to fit the model.  Here, we have
selected ‘proximity’ since individuals are captured at most once in
each trap during each sampling occasion.
{\small
\begin{verbatim}
> traps= read.csv(“wtraps.csv”)
>    #name the first 3 columns to match the secr nomenclature
> colnames(traps)[1:3]<- c("trapID","x", "y")  

> trapfile <- read.traps(data = traps, detector = "proximity")
\end{verbatim}
}

After reading in the data, we now need to create the encounter matrix
or array.  The \mbox{\tt secr} package does this through the use of the
\mbox{\tt make.capthist} command, where we provide the capture histories in raw
data format (each line contains the session, identification number,
occasion, and trap id for only 1 individual).  This is the format that
was shown in the data input file ``\mbox{\tt wcaps}'', and we only need a line or
two to organize the data into the order that the make.capthist command
wants.  In creating the capture history, we provide also the trapfile
with the trap information, and the format (e.g., here \mbox{\tt fmt= ``trapID''})
so that \mbox{\tt secr} knows how to match the encounters to the trap, and
finally, we provide the number of occasions:\footnote{Beth: Do you
  need to update this?}
{\small 
\begin{verbatim}
> wolv.dat <- wcaps[,c(2, 3, 1)]   
           #NEED TO UPDATE THIS WHEN I GET THE FILES, 
           ### I JUST GUESSED AT THE CODE, BUT WOULD LIKE TO TRY IT. 
> wolv.dat <- cbind(rep(1, dim(wolv.dat)[1], wolv.dat)  
> colnames(wolv.dat) <- c("Session", "ID", "Occasion", "trapID")

> wolvcapt=make.capthist(wolv.dat, trapfile, fmt = "trapID", noccasions = 165)
\end{verbatim}
}
Calling the secr.fit command, will run the model.  We are using the
basic model (SCR0), so we do not need to make any specifications in
the command line except for the providing the buffer size (in $m$).  To
specify different models, you can change the default
\verb#D~1, g0~1, sigma~1#, which the interested reader can do with
very little difficulty.

{\small
\begin{verbatim}
> wolv.secr=secr.fit(wolvcapt, model = list(D~1, g0~1, sigma~1), buffer = 20000)

> wolv.secr

secr.fit( capthist = wolvcapt, buffer = 20000, binomN = 1 )
secr 2.0.0, 18:26:39 05 Jul 2011

Detector type     proximity 
Detector number   37 
Average spacing   4415.693 m 
x-range           593498 652294 m 
y-range           6296796 6361803 m 
N animals       :  21  
N detections    :  115 
N occasions     :  165 
Mask area       :  1037069 ha 

Model           :  D~1 g0~1 sigma~1 
Fixed (real)    :  none 
Detection fn    :  halfnormal 
Distribution    :  poisson 
N parameters    :  3 
Log likelihood  :  -746.754 
AIC             :  1499.508 
AICc            :  1500.920 

Beta parameters (coefficients) 
           beta    SE.beta        lcl       ucl
D     -9.749576 0.23027860 -10.200913 -9.298238
g0    -4.275736 0.15846104  -4.586313 -3.965158
sigma  8.699202 0.07868944   8.544973  8.853430

Variance-covariance matrix of beta parameters 
                 D           g0        sigma
D      0.053028233  0.000546922 -0.005226926
g0     0.000546922  0.025109900 -0.005885213
sigma -0.005226926 -0.005885213  0.006192027

Fitted (real) parameters evaluated at base levels of covariates 
       link     estimate  SE.estimate          lcl          ucl
D       log 5.831941e-05 1.360973e-05 3.713638e-05 9.158548e-05
g0    logit 1.371121e-02 2.142902e-03 1.008756e-02 1.861207e-02
sigma   log 5.998124e+03 4.727205e+02 5.140849e+03 6.998355e+03
\end{verbatim}
}

Under the fitted (real) parameters, we find $D$, the density, given in
units of individuals/hectare (1 hectare = 10000 $m^2$).  To convert this
into individuals/1000 $km^2$, we multiply by 100000, thus our density
estimate is 5.83 individuals/1000 $km^2$.  $\sigma$ is given in units of
meters, to convert to kilometers, we divide by 1000, which puts sigma
at 5.99 $km$.  Both of these estimates are very similar to those
provided in section 4.6 for the buffer size equal to 20 km XXXX How
similar? XXXXX.  

As an
exercise, run this analysis for 30 and 40 km buffers and compare those
found in section 4.6 under {\bf WinBUGS}.  NOTE: The function \mbox{\tt
  secr.fit} 
will return a
warning when the buffer size appears to be too small.  This is useful
particularly with the different units being used between programs and
packages.


\section{Summary and Outlook}

In this chapter, we showed that classical analysis of SCR models based
on likelihood methods is a relatively simple proposition.  Analysis is
based on the so-called integrated likelihood in which the individual
activity centers (random effects) are removed from the
conditional-on-{\bf s} likelihood by integration. We showed how to construct
the integrated likelihood and fit some simple models in the {\bf R}
programming language.  In addition, likelihood analysis for some broad
classes of SCR models can be accomplished in the software package
{\bf DENSITY} 
or the {\bf R}
library \mbox{\tt secr} which we provided an illustration of here. In later
chapters we provide more detailed analyses of SCR data using the
\mbox{\tt secr}
package.

To compute the integrated likelihood we have to precisely describe the
state-space of the underlying point process. In practice, this leads
to a ``buffer'' around the trap array. We note that this is not really a
``buffer strip'' in the sense of \citet{wilson_anderson:1985a} 
which is a feature
of the analysis but it is somewhat more general here. In particular,
it establishes the support of the integrand which we generally require
to compute any integral. It might be that the integrand itself is
finite even if the support is infinity but that may or may not be the
case depending on the choice of detection function. As a practical
matter then, it will typically be the case that, while estimates of $N$
increase with the size of the buffer, estimates of density
stabilize. This is not a feature of the classical methods based on
using model $M_0$ or model $M_h$ and buffering the trap array.

Why or why not use likelihood inference exclusively? For certain
specific models, it is probably more computationally efficient to
produce MLEs. However, {\bf BUGS} is extremely flexible in terms of
describing models, although it sometimes can be quite slow. We can
devise models in the {\bf BUGS} language easily that we cannot fit in
\mbox{\tt secr}. E.g.,
random individual effects of various types (see next chapter), we can
handle missing covariates in complete generality and seamlessly, and
impose arbitrary distributions on random variables. Moreover, models
can easily be adapted to include auxiliary data types. For example, we
might have camera trapping and genetic data and we can describe the
models directly in {\bf BUGS} and fit a joint model. For the MLE we have
to write a custom new piece of code for each model or hope someone has
done it for us.  Later we consider open population models which are
straightforward to develop in {\bf BUGS} but, so far, there is no
available platform for doing MLE although we imagine one could develop
this.  Another thing that is more conceptual here is non-CSR point
processes (see chapter XXXX) and generating predictions of how many
individuals have home range centers in any particular polygon.  Basic
benefits of Bayesian analysis have been discussed elsewhere (XXXXXXXX Chapter
2? BPA book? Link and Barker?) and we believe these are compelling. On
the other hand, likelihood analysis makes it easy to do
model-selection by AIC and in some cases compute standard errors or
carry-out goodness-of-fit evaluations. 


In summary, basic SCR models are easy to implement by either
likelihood or Bayesian methods but we feel that the typical user will
realize much more flexibility in model development using existing
platforms for Bayesian analysis. While these tend to be slow
(sometimes excruitatingly slow), this will probably not be an
impediment in most problems, especially at some near point in the
future.  Since we spent a lot of time here talking about specific
technical details on how to implement likelihood analysis of SCR
models, we provided a corresponding treatment in the next chapter on
how to devise MCMC algorithms for SCR models. This is a bit more
tedious and requires more coding, but is not technically challenging
(exept perhaps to develop highly efficient algorithms which we don’t
excel at).



