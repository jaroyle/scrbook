\chapter{
Likelihood Analysis of Spatial Capture-Recapture Models
}
\markboth{Likelihood Estimation}{}
\label{chapt.mle}

%%%% TO-DO LIST
% 1. Rectify shapefile issue in the repo  (doesn't work)
% 1. comparison of Bayes with MLE for wolverine data (need to rerun WinBUGS)
% 2. Beth clean up a couple things in SECR analysis.
%  Beth:  I've cleaned up most of the ??s, but I'll look it over again to make sure I 
%  got everything. 
% 3. Need to finish MLE for restricted state-space
%%   requires code from Rahel
\vspace{.3in}

We have so far mainly focused on Bayesian analysis of spatial
capture-recapture models. And, in the previous chapters we learned how
to fit some basic spatial capture-recapture models using a Bayesian
formulation of the models analyzed in BUGS engines including {\bf
  WinBUGS} and {\bf JAGS}.  Despite our focus on Bayesian analysis, it
is instructive to develop the basic concepts and ideas behind
classical analysis based on likelihood methods and frequentist
inference for SCR models. This has been the approach taken by
\citet{borchers_efford:2008, dawson_efford:2009} and related papers.
Therefore, in this chapter, we provide some conceptual and technical
foundation for likelihood-based analysis of spatial capture-recapture
models. We recognized earlier (Chapt. \ref{chapt.scr0}) that SCR
models are versions of binomial (or other) GLMs, but with random
effects, i.e., GLMMs. These models are routinely analyzed by
likelihood methods. In particular, likelihood analysis is based on the
integrated likelihood in which the random effects are removed by
integration from the likelihood. In SCR models, the 2-dimensional
coordinate, ${\bf s}$, is a bivariate random effect. Beyond that,
there is little difference between likelihood analysis of SCR models
and ordinary GLMMs.

We will show here that it is straightforward to compute the maximum
likelihood estimates (MLE) for SCR models by integrated or marginal
likelihood. We develop the MLE framework using {\bf R}, and we also
provide a basic introduction to an {\bf R} package \mbox{\tt secr}
\citep{efford:2011} which mostly does likelihood analysis of SCR
models (see also the the stand-alone package {\bf DENSITY}
\citep{efford_etal:2004}).  To set the context for likelihood analysis
of SCR models, we first analyze the SCR model here when $N$ is known
because, in that case, it is precisely a GLMM and does not pose any
difficulty at all. We generalize the model to allow for unknown $N$
using both conventional ideas based on the ``joint likelihood''
\citep[e.g.,][]{borchers_etal:2002} and also using a formulation based
on data augmentation.  We obtain the MLEs for the SCR model from the
wolverine camera trapping study \citep{magoun_etal:2011} analyzed in
previous chapters to compare/contrast the results.

\section{MLE with known N}

We noted in Chapt. \ref{chapt.scr0} that, with $N$ known, the basic SCR model is a
type of binomial regression with a random effect. For such models we
can  obtain maximum likelihood estimators of model parameters
based on integrated likelihood. The integrated likelihood is based on
the marginal distribution of the data $y$ in which the random effects
are removed by integration. Conceptually, our SCR model is a specification
of the conditional-on-${\bf s}$ model $[y|{\bf s},{\bm \alpha}]$ and we have
a ``prior distribution'' for ${\bf s}$, say $[{\bf s}]$, and the
marginal distribution of the data $y$ is
\[
[y|{\bm \alpha}] =  \int_{\bf s} [y|{\bf s},{\bm \alpha}][{\bf s}] d{\bf s}.
\]
When viewed as a function of $\alpha$ for purposes of estimation, the
marginal distribution $[y|\alpha]$ is often referred to as the {\it
  integrated likelihood}.

It is worth analyzing 
the simplest SCR model with known-$N$ in order to understand the
underlying mechanics and basic concepts. These are directly relevant to
the manner in which many capture-recapture models are classically
analyzed, such as model $M_h$, and individual covariate models (see
Chapt. \ref{chapt.closed}).

 To develop the integrated
likelihood for SCR models, we first identify the conditional-on-${\bf s}$
likelhiood. 
The observation model for each encounter observation $y_{ij}$,
specified conditional on ${\bf s}_{i}$, is 
\begin{equation}
y_{ij}| {\bf s}_{i} \sim \mbox{Binomial}(K, p_{\alpha}({\bf x}_{j},{\bf s}_{i}))
\label{mle.eq.cond-on-s}
\end{equation}
where we have indicated the dependence of $p_{ij}$ on ${\bf s}$ and
parameters ${\bm \alpha}$
explicitly.
For the random effect we have ${\bf s}_{i} \sim  \mbox{Uniform}({\cal
  S})$.
The joint distribution of the data for individual $i$ is the product
of $J$ such terms (i.e., contributions from each of $J$ traps).
\[
  [{\bf y}_{i} | {\bf s}_{i} , {\bm \alpha}] = 
  \prod_{j=1}^{J} \mbox{Binomial}(K, p_{\alpha}({\bf x}_{j},{\bf s}_{i}) )
\]
We note this assumes that encounter of individual $i$ in each
trap is independent of encounter in every other trap, conditional on
${\bf s}_{i}$, this is the fundamental property of the basic model SCR0.


The marginal likelihood is computed by removing
${\bf s}_{i}$, by integration (hence also {\it integrated} likelihood), from the conditional-on-${\bf s}$
likelihood and regarding the {\it marginal} distribution of the data
as 
the likelihood. That
is, we compute:
\[
  [y|{\bm \alpha}] = 
\int_{{\cal S}}  [ {\bf y}_{i} |{\bf s}_{i}, {\bm \alpha}] g({\bf s}_{i}) d{\bf s}_{i}
\]
In most SCR models, $g({\bf s}) = 1/||{\cal S}||$ (but see Chapt. \ref{chapt.state-space} for
alternative specifications).

The joint likelihood for all $N$ individuals, assuming independence of
encounters among individuals, is the product of $N$ such terms:
\[
{\cal L}({\bm \alpha} | {\bf y}_{1},{\bf y}_{2},\ldots, {\bf y}_{N}) =     \prod_{i=1}^{N}
[{\bf y}_{i}|{\bm \alpha}]
\]
We emphasize that two independence assumptions are explicit in this
development: independence of trap-specific encounters within
individuals and also independence among individuals. In particular,
this would only be valid when individuals are not physically
restrained or removed upon capture, and when traps do not ``fill up''
(i.e., this is model SCR0, from Chapt. \ref{chapt.scr0}).

The key operation for computing the likelihood is solving a
2-dimensional integration problem. There are some general purpose {\bf
  R} packages that implement a number of 
 multi-dimensional integration routines
including \mbox{\tt adapt} \citep{genz_etal:2007} and \mbox{\tt R2cuba}
\citep{hahn_etal:2011}.  In practice, we won't rely
on these extraneous {\bf R} packages (except see
Chapt. \ref{chapt.state-space} for an application of \mbox{\tt Rcuba})
but instead will use perhaps less
efficient methods in which we replace the integral with a summation
over an equal area mesh of points on the state-space ${\cal S}$ and explicitly
evaluate the integrand at each point. We invoke the rectangular rule
for integration here\footnote{e.g., 
\url{http://en.wikipedia.org/wiki/Rectangle_method}
} in which we
evaluate the
integrand on a regular grid of points of equal area and compute the
average of
the integrand over that grid of points. 
Let $u=1,2,\ldots,nG$ index a grid of
$nG$ points, ${\bf s}_{u}$,  where the area of grid cells is
constant, say $A$.
In this case, the integrand, i.e., the marginal pmf of 
${\bf y}_{i}$, is approximated by  
\begin{equation}
         [{\bf y}_{i}|{\bm \alpha}] = \frac{1}{nG} \sum_{u=1}^{nG}  [ {\bf
            y}_{i} |{\bf s}_u, {\bm \alpha}]
\label{mle.eq.intlik}
\end{equation}

This is a specific case of the general expression that could be used
for approximating the integral for any arbitrary (bivariate or otherwise)
distribution $g({\bf s})$. The general case is
\[
[y|{\bm \alpha}]  = \frac{A}{nG} \sum_{u} [y|{\bf s}_{u},{\bm \alpha}] [{\bf s}_{u}]
\]
 In the present context it happens that  $[{\bf s}] = (1/A)$
and thus the grid-cell area cancels in the above
expression to yield eq. \ref{mle.eq.intlik}.
The rectangular rule for integration can be seen as an application of
the Law of Total Probability for a discrete random variable ${\bf
  s}$, having $nG$ 
unique values with equal probabilities $1/nG$.


\subsection{Implementation (simulated data)}

Here we will illustrate how to carryout this integration and
optimization based on the integrated likelihood using simulated data
 (i.e., following that from Chapt. \ref{chapt.scr0}). Using \mbox{\tt simSCR0.fn}
 we simulate data for 100 individuals and a 25 trap array
laid out in a $5 \times 5$ grid of unit spacing.  The specific encounter
model is the Gaussian model. The 100 activity centers were
simulated on a state-space defined by a $8 \times 8$ square 
within which the
trap array was centered (thus the trap array is buffered by 2
units). Therefore, the density of individuals in this system is fixed
at $100/64$.

In the following set of {\bf R} commands we generate the data and 
then harvest the required data objects:
{\small
\begin{verbatim}
data<-simSCR0.fn(discard0=FALSE,sd=2013)
y<-data$Y
traplocs<-data$traplocs
nind<-nrow(y)
J<-nrow(traplocs)
K<-data$K
Xl<-data$xlim[1]
Yl<-data$ylim[1]
Xu<-data$xlim[2]
Yu<-data$ylim[2]
\end{verbatim}
}
{\flushleft Now } we need to define the integration grid, say ${\bf G}$, which we do with
the following set of {\bf R} commands (here, \mbox{\tt delta} is the grid spacing):
{\small
\begin{verbatim}
delta<- .2
xg<-seq(Xl+delta/2,Xu-delta/2,by=delta) 
yg<-seq(Yl+delta/2,Yu-delta/2,by=delta) 
npix<-length(xg)          # assumes square state-space here
G<-cbind(rep(xg,npix),sort(rep(yg,npix)))
nG<-nrow(G)
\end{verbatim}
}
{\flushleft In this case}, the integration grid is set up as a grid with spacing
$\delta = 0.2$ which produces a $40 \times 40$ grid of points for evaluating the
integrand if the state-space buffer is set at 2.

We next create an {\bf R} function that defines the likelihood as a
function of the data objects $y$ and $X$ which were created above but,
in general, you would read these files into {\bf R}, e.g., from a .csv
file.  In addition to these data objects, we need to have defined the
quantities $G$ and $nG$ associated with the integration grid.
However, instead of worrying about making all of these objects and
keeping track of them we just put that code above into a
function, say \mbox{\tt intlik1}, and pass $\delta$ as an additional
(optional) argument and a few other things that we need such as the
boundary of the state-space over which the integration (summation) is
being done. This function is available in the \mbox{\tt
  scrbook} package (use {\tt ?intlik1} at the {\bf R} prompt).
 The code is reproduced here:

{\small 
\begin{verbatim}
intlik1<-function(parm,y=y,delta=.2,X=traplocs,ssbuffer=2){

Xl<-min(X[,1]) - ssbuffer 
Xu<-max(X[,1]) + ssbuffer
Yu<-max(X[,2]) + ssbuffer
Yl<-min(X[,2]) - ssbuffer

xg<-seq(Xl+delta/2,Xu-delta/2,,length=npix) 
yg<-seq(Yl+delta/2,Yu-delta/2,,length=npix) 
npix<-length(xg)

G<-cbind(rep(xg,npix),sort(rep(yg,npix)))
nG<-nrow(G)
D<- e2dist(X,G)  

alpha0<-parm[1]
alpha1<-parm[2]
probcap<- plogis(alpha0)*exp(-alpha1*D*D)
Pm<-matrix(NA,nrow=nrow(probcap),ncol=ncol(probcap))
                    # all zero encounter histories
n0<-sum(apply(y,1,sum)==0) 
                    # encounter histories with at least 1 detection
ymat<-y[apply(y,1,sum)>0,] 
ymat<-rbind(ymat,rep(0,ncol(ymat)))
lik.marg<-rep(NA,nrow(ymat))
for(i in 1:nrow(ymat)){
Pm[1:length(Pm)]<- (dbinom(rep(ymat[i,],nG),K,probcap[1:length(Pm)],log=TRUE))
lik.cond<- exp(colSums(Pm))
lik.marg[i]<- sum( lik.cond*(1/nG))  
}
nv<-c(rep(1,length(lik.marg)-1),n0)
-1*( sum(nv*log(lik.marg)) )
}
\end{verbatim}
}


The function \mbox{\tt intlik1} accepts as input the encounter history matrix, $y$, the
trap locations, $X$, and the state-space buffer. This allows us to
vary the state-space buffer and easily evaluate the sensitivity of the
MLE to the size of the state-space.  Note that we have a peculiar
handling of the encounter history matrix $y$. In particular, we remove
the all-zero encounter histories from the matrix and tack-on a single
all-zero encounter history as the last row which then gets weighted by
the number of such encounter histories (\mbox{\tt n0}). This is a bit
long-winded and strictly unnecessary when $N$ is known, but we did it
this way because the extension to the unknown-$N$ case is now
transparent (as we demonstrate in the following section).  The matrix
\mbox{\tt Pm} holds the log-likelihood contributions of each encounter
frequency for each possible state-space location of the individual.
The log contributions are summed up and the result exponentiated on
the next line, producing lik.cond, the conditional-on-${\bf s}$
likelihood (Eq. \ref{mle.eq.cond-on-s} above). The marginal likelihood
(\mbox{\tt lik.marg}) sums up the conditional elements weighted by
$\Pr({\bf s})$ (Eq. \ref{mle.eq.intlik} above).  This is a fairly
primitive function which doesn't allow much flexibility in the data
structure. For example, it assumes that $K$, the number of replicates,
is constant for each trap. Further, it assumes that the state-space is
a square. We generalize this to some extent later in this chapter.

Here is the {\bf R} command for maximizing the likelihood and saving the
results into an object called \mbox{\tt frog}.  The output is a list of the
following structure and these specific estimates are produced using
the simulated data set:

{\small 
\begin{verbatim}
# should take 15-30 seconds

starts<-c(-2,2)
frog<-nlm(intlik1,starts,y=y,delta=.1,X=traplocs,ssbuffer=2,hessian=TRUE)
frog

$minimum
[1] 297.1896

$estimate
[1] -2.504824  2.373343

$gradient
[1] -2.069654e-05  1.968754e-05

$hessian
          [,1]      [,2]
[1,]  48.67898 -19.25750
[2,] -19.25750  13.34114

$code
[1] 1

$iterations
[1] 11
\end{verbatim}
} 
Details about this output can be found on the help page for
\mbox{\tt nlm}. We note briefly that \mbox{\tt frog\$minimum} is the
negative log-likelihood value at the MLEs, which are stored in the
\mbox{\tt frog\$estimate} component of the list. The Hessian is the
observed Fisher information matrix, which can be inverted to obtain
the variance-covariance matrix using the command:
\begin{verbatim}
> solve(frog$hessian)
\end{verbatim}

It is worth drawing attention to the fact that the estimates are slightly
different than the Bayesian estimates reported previously in
sec. \ref{scr0.sec.winbugs1}.   There are several reasons
for this.  First Bayesian inference is based on the posterior
distribution and it is not generally the case that the MLE should
correspond to any particular value of the posterior distribution. If
the prior distributions in a Bayesian analysis are uniform, then the
(multivariate) mode of the posterior is the MLE, but note that
Bayesians almost always report posterior {\it means} and so there will
typically be a discrepancy there. Secondly, we have implemented an
approximation to the integral here and there might be a slight bit of
error induced by that. We will evaluate that shortly. Third, the
Bayesian analysis by MCMC is itself subject to some amount of Monte Carlo
error which the analyst should always be aware of in practical
situations.  All of these different explanations are likely
responsible for some of the discrepancy. Accounting for these, we see
general consistency between the two estimates.

\begin{comment} 
To compute the integrated likelihood we used a discrete representation
of the state-space so that the integral could be approximated as a
summation over possible values of ${\bf s}$ with each value being
weighted by its probability of occurring, which is $1/nG$ under the
assumption that ${\bf s}$ is uniform on the state-space ${\cal
  S}$. Recall
in Chapt. \ref{chapt.scr0} we 
used a discrete state-space in developing a Bayesian analysis of the
model in order to be able to modify the state-space in a flexible
manner. In that case, we could use the discretized state-space as the
integration grid and just feed it into our integrated likelihood
routine. 
\end{comment}

In summary, for the basic SCR model, integrated
likelihood is a really easy calculation when $N$ is known. Even for $N$
unknown it is not too difficult, and we will do that shortly.
However, if you can solve the known-$N$ problem then you should be able
to do a real analysis, for example by considering different values of
$N$ and computing the results for each value and then making a plot of
the log-likelihood or AIC and choosing the value of $N$ that produces
the best log-likelihood or AIC. As a homework problem we suggest that
the reader take the code given above and try to estimate $N$ without
modifying the code by just repeatedly applying it for 
different values of $N$ in attempt to deduce the best value.
We will formalize the unknown-$N$ problem next.

%The
%software package {\bf DENSITY} \citep{efford_etal:2004} implements
%certain types of SCR models using integrated likelihood methods, and
%\mbox{\tt secr} \citep{efford:2011} is an {\bf R} package with similar functionality.
%We provide an analysis of some data using \mbox{\tt secr} shortly along
%with a discussion of its capabilities, and we use \mbox{\tt secr} in
%later chapters for likelihood analysis of other SCR models.


\section{MLE when N is Unknown} 
\label{mle.sec.Nunknown}

Here we build on the previous introduction to integrated likelihood
but we consider now the case in which $N$ is unknown. We will see that
adapting the analysis based on the known-$N$ model is 
straightforward for the more general problem. The main distinction is
that we don't observe the all-zero encounter history so we have to
make sure we compute the probability for that encounter history which
we do by tacking a row of zeros onto the encounter history matrix. In
addition, we include the number of such all-zero encounter histories
as an unknown parameter of the model. Call that unknown quantity
$n_{0}$, and we have to be sure to include a combinatorial term to
account for the fact that of the $n$ observed individuals there are
${N \choose n}$ ways to realize a sample of size $n$. The
combinatorial term involves the unknown $n_{0}$ and thus it must be
included in the likelihood.

Therefore, to compute the likelihood, we require 
the following 3 components: (1) the marginal
probability of each ${\bf y}_{i}$ as before,
\[
  [{\bf y}_{i}|{\bm \alpha}] = 
\int_{{\cal S}} \mbox{Binomial}({\bf y}_{i} |{\bf s}_{i}, {\bm \alpha})g({\bf s}_{i}) d{\bf s}_{i}.
\]
(2) We compute
the probability of an all-0 encounter history:
\[
\pi_{0} = [{\bf y} = {\bf 0} | {\bm \alpha}] = 
\int_{{\cal S}} \mbox{Binomial}({\bf 0} |{\bf s}_{i}, {\bm \alpha})g({\bf s}_{i}) d{\bf s}_{i}
\]
(3) The combinatorial term: ${N \choose n}$. Then, 
 the marginal likelihood has this form:
\begin{equation}
 {\cal L}({\bm \alpha}, n_{0}| {\bf y})  = \frac{N!}{n! n_{0}!}
 \left\{ \prod_{i=1}^{n}  [{\bf y}_{i}|{\bm \alpha}] \right\}
 (\pi_{0})^{n_{0}}.
\label{mle.eq.binomialform}
\end{equation}
This is discussed in \citet[][p. 379]{borchers_efford:2008} as the
conditional-on-$N$ form of the likelihood -- we might also call it
``binomial form'' because of its appearance. 

Operationally, things proceed much as before: 
We compute the marginal probability of each observed ${\bf y}_{i}$,
i.e., by removing the latent ${\bf s}_{i}$ by integration. In
addition, we 
 compute the marginal probability of the ``all-zero'' encounter
history ${\bf y}_{n+1}$, and make sure to weight it $n_{0}$ times. We
accomplish this by ``padding'' the data set with a single encounter
history having $y_{n+1,j}=0$ for all traps $j=1,2,\ldots,J$. Then we
be sure to include the combinatorial term in the likelihood or
log-likelihood computation. We demonstrate this shortly.
To analyze a specific case, we'll read in our fake data set (simulated
using the parameters given above). To set some things up in our
workspace we do this:
\begin{verbatim}
data<-simSCR0.fn(discard0=TRUE,sd=2013)
y<-data$Y
nind<-nrow(y)
traplocs<-data$traplocs
J<-nrow(X)
K<-data$K
\end{verbatim}
Recall that these data were generated with $N=100$, on an $8 \times 8$ unit
state-space representing the trap locations  buffered by 2 units.

As before, the likelihood is defined in the {\bf R} workspace as an
{\bf R}
function, \mbox{\tt intlik2}, 
 which takes an argument being the unknown parameters of the
model and additional arguments as prescribed. In particular, 
 we provide the encounter history matrix ${\bf y}$, the trap locations
\mbox{\tt traplocs}, the spacing of the integration grid (argument
\mbox{\tt delta}) and the
state-space buffer. Here is the new likelihood function:
{\small
\begin{verbatim}
intlik2<-function(parm,y=y,delta=.3,X=traplocs,ssbuffer=2){

Xl<-min(X[,1]) -ssbuffer
Xu<-max(X[,1])+ ssbuffer
Yu<-max(X[,2])+ ssbuffer
Yl<-min(X[,2])- ssbuffer

xg<-seq(Xl+delta/2,Xu-delta/2,delta) 
yg<-seq(Yl+delta/2,Yu-delta/2,delta) 
npix.x<-length(xg)
npix.y<-length(yg)
area<- (Xu-Xl)*(Yu-Yl)/((npix.x)*(npix.y))
G<-cbind(rep(xg,npix.y),sort(rep(yg,npix.x)))
nG<-nrow(G)
D<- e2dist(X,G) 

alpha0<-parm[1]
alpha1<-parm[2]
n0<-exp(parm[3])
probcap<- plogis(alpha0)*exp(-alpha1*D*D)
Pm<-matrix(NA,nrow=nrow(probcap),ncol=ncol(probcap))
ymat<-rbind(y,rep(0,ncol(y)))

lik.marg<-rep(NA,nrow(ymat))
for(i in 1:nrow(ymat)){
Pm[1:length(Pm)]<- (dbinom(rep(ymat[i,],nG),K,probcap[1:length(Pm)],log=TRUE))
lik.cond<- exp(colSums(Pm))
lik.marg[i]<- sum( lik.cond*(1/nG) )  
}                                                 
nv<-c(rep(1,length(lik.marg)-1),n0)
part1<- lgamma(nrow(y)+n0+1) - lgamma(n0+1)
part2<- sum(nv*log(lik.marg))
 -1*(part1+ part2)
}
\end{verbatim}
}
To execute this function for the data that we created with \mbox{\tt simSCR0.fn},
 we execute the following command (saving the result in our
friend \mbox{\tt frog}).
This results in the usual output, including the parameter estimates,
the gradient, and the numerical Hessian which is useful for obtaining
asymptotic standard errors (see below):
\begin{verbatim}
starts<-c(-2.5,2,log(4))
frog<-nlm(intlik2,starts,hessian=TRUE,y=y,X=traplocs,delta=.2,ssbuffer=2)

There were 50 or more warnings (use warnings() to see the first 50)

frog
$minimum
[1] 113.5004

$estimate
[1] -2.538334  2.466515  4.232810

[... Additional output deleted ...]
\end{verbatim}
While this produces some {\bf R} warnings, these happen to be harmless
in this case, and we will see from the \mbox{\tt nlm} output that the
algorithm performed satisfactory in minimizing the objective function.
The estimate of population size for the state-space (using the default 
state-space buffer) is
\begin{verbatim}
nrow(y)+exp(4.2328)
[1] 110.9099
\end{verbatim}
Which differs from the data-generating value ($N=100$) as we might
expect for a single realization. We usually will present an estimate of uncertainty associated
with this MLE which we can obtain by inverting the Hessian. Note that
$\mbox{Var}(\hat{N}) = n + \mbox{Var}(\hat{n}_{0})$.
Since we
have parameterized the model in terms of log($n_{0}$) we use the delta
method\footnote{
We found a good set of notes on the delta approximation on Dr. David
Patterson's ST549 notes: 
\url{http://www.math.umt.edu/patterson/549/Delta.pdf}
}
\citep[][Appendix F4]{williams_etal:2002}
 to obtain the variance on the scale of $n_{0}$ as
follows:
\begin{verbatim}
(exp(4.2328)^2)*solve(frog$hessian)[3,3]
[1] 260.2033
> sqrt(260)
[1] 16.12452
\end{verbatim}
Therefore, the asymptotic ``Wald-type'' confidence interval for $N$ is
$110.91 \pm 1.96 \times 16.125 = (79.305, 142.515)$. To report this in
terms of density, we scale appropriately by the area of the prescribed
state-space which is $64$ units of area (i.e., an $8 \times 8$ square).


\begin{comment}

\subsection{Exercises}

{\flushleft 
{\bf 1.}	
Run the analysis with different state-space buffers and comment on the result. 
}


{\flushleft 
{\bf 2.} Conduct a brief simulation study using this code by
  simulating 100 data sets and obtain the MLEs for each data set. Do
  things seem to be working as you expect?  }

{\flushleft 
{\bf 3.} 
Further extensions: It should be straightforward to
  generalize the integrated likelihood function to accommodate many
  different situations. For examples, if we want to include more
  covariates in the model we can just add stuff to the object \mbox{\tt probcap},
 and add the relevant parameters to the argument that gets
  passed to the main  function.  For the simulated data, make up a
  covariate by generating a Bernoulli covariate (``trap type'', perhaps
  baited or not baited) randomly and try to modify the likelihood to
  accommodate that.  }

{\flushleft {\bf 4.}  We would probably be interested in devising the
  integrated likelihood for the full 3-d encounter history array so
  that we could include temporally varying covariates. This is not
  difficult but naturally will slow down the execution
  substantially. The interested reader should try to expand the
  capabilities of this basic {\bf R} function.  }
\end{comment}




\subsection{Integrated Likelihood using the model under data augmentation } 
\label{mle.sec.intlikDA}

The likelihood analysis developed in the previous sections
is based on the likelihood
in which $N$ (or $n_{0}$) is an explicit parameter. This is usually called
the ``full likelihood'' or sometimes ``unconditional likelihood''.  
We could also
express the  likelihood using data augmentation, replacing the
parameter $N$ with $\psi$ \citep[e.g., see Sec. 7.1.6][for an example]{royle_dorazio:2008}.
We don't go into detail here, but we note that the
likelihood under data augmentation is a zero-inflated binomial
mixture -- precisely an occupancy type model \citep{royle:2006}.
Thus, while it is possible to carryout likelihood analysis of
models under data augmentation, we primarily advocate data
augmentation for Bayesian analysis.


\subsection{ Extensions}

We have only considered basic SCR models with no additional
covariates. However, in practice, we are interested in other types of
covariate effects including ``behavioral response'', 
sex-specificity of parameters, and potentially other effects. Some of
these  can be added directly to the likelihood if the covariate is fixed
and known for all individuals captured or not. An example is a
behavioral response, which amounts to having a covariate $x_{ik}=1$ if
individual $i$ was captured prior to occasion $k$ and $x_{ik}=0$
otherwise. For uncaptured individuals, $x_{ik}=0$ for all $k$.
 \citet{royle_etal:2011jwm} called this a global behavioral
response because the covariate is defined for all traps, no matter the
trap in which an individual was captured. We could also define a {\it
  local} behavioral response which occurs at the level of the trap,
i.e., $x_{ijk}=1$ if individual $i$ was captured in trap $j$ prior to
occasion $k$, etc.. 
Trap-specific covariates such as trap type or status, or
time-specific covariates such as date, are easily accommodated as
well. As an example, \citet{kery_etal:2010} develop a model for the
European wildcat \emph{Felis silvestris}  in which traps are either baited or not (a
trap-specific covariate with only 2 values), and also encounter
probability varies over time in the form of a quadratic seasonal response.
We consider models with behavioral response or fixed covariates in
Chapt. \ref{chapt.covariates}.
The integrated likelihood routines we provided above can be
modified directly for such cases, which we leave to the interested
reader to investigate. 

Sex-specificity is more difficult to deal with since sex is not known
for uncaptured individuals (and sometimes not even for all captured
individuals).  To analyze such models, we do Bayesian analysis of the
joint likelihood using  data augmentation
\citep{gardner_etal:2010jwm,russell_etal:2012}, discussed further in
Chapt. \ref{chapt.covariates}. For such covariates (i.e., that are
not fixed and known for all individuals), it is somewhat more
challenging to do MLE for these based on the joint likelihood as we
have developed above. Instead it is more conventional to use what is
colloquially referred to as the ``Huggins-Alho'' type model which is
one of the approaches taken in the software package \mbox{\tt secr}
\citep[][]{efford:2011} which we describe
in sec. \ref{mle.sec.secr} below. This idea is
motivated by thinking about unequal probability sampling methods known
as Horvitz-Thompson sampling \citep[e.g.,
see][]{overton_stehman:1995}.  



\section{Classical model selection and assessment}

In most analyses, one is interested in choosing from among various
potential models, or ranking models, or something else to do with
assessing the relative merits of a set of models. A good thing about
classical analysis based on likelihood is we can apply AIC methods
\citep{burnham_anderson:2002} without difficulty. There are two
distinct contexts for model-selection that we think are relevant to
SCR models. First is, and AIC selecting among models that represent
distinct biological hypotheses (e.g., covariates affecting encounter
probability or density). AIC is convenient for assessing the relative
merits of these different models although if there are only a few
models it is not objectionable to use hypothesis tests or confidence
intervals to determine importance of effects. The second model
selection context has to do with choosing among various detection
functions although, as a general rule, we don't recommend this
application of model selection.  This is because there is hardly ever
(if at all) a rational subject-matter based reason motivating specific
distance functions. As a result, we believe that doing too much model
selection will invariably lead to over-fitting and thus over-statement
of precision. This is the main reason that we haven't loaded you down
with a basket of models for detection probability so far, although we
discuss many possibilities in Chapt. \ref{chapt.covariates}.


{\bf Goodness-of-fit} -- For many standard capture-recapture models,
it is possible to identify goodness-of-fit statistics based on the
multinomial likelihood and evaluate model adequacy using formal
statistical tests. Similar strategies can be applied to SCR models
using expected cell-frequencies based on the marginal distribution of
the observations. Also, because computing MLEs is somewhat more
efficient in many cases compared to Bayesian analysis, it is also
sometimes easy to use bootstrap methods although, at the present time,
we don't know of any applications of goodness-of-fit testing for SCR
models based on likelihood inference\footnote{WE NEED TO LOOK INTO THIS!!}.


%Bayesian goodness-of-fit, which we take up in more detail in
%Chapt. \ref{chapt.gof}, is almost always addressed with Bayesian
%p-values or some other posterior predictive check
%(sec. \ref{glms.sec.gof}, \citet[][sec. 2.6]{kery:2010}).
%\citet{royle_etal:2011mee} suggested checking model fit for SCR models
%by decomposing fit into two components: (1) That of the encounter
%process model, evaluated by the expected encounter frequencies
%computed {\it conditional} on ${\bf s}$; and, (2) That of the spatial
%point process model (``spatial randomness'').


\section{Likelihood analysis of the wolverine camera trapping data}
\label{mle.sec.wolverine}


Here we compute the MLEs for the wolverine data using an expanded
version of the function we developed in the previous section. To
accommodate that each trap might be operational a variable number of
nights, we provided an additional argument to the likelihood function
(allowing for a vector ${\bf K}= (K_{1},\ldots,K_{J})$), which requires also a modification to the
construction of the likelihood.  In addition,
we accommodate  the state-space is a general rectangle, and
we included a line in the code to compute the state-space area which
we apply below for computing density.  The more general function
(\mbox{\tt intlik3}) is given in the {\bf R} package \mbox{\tt scrbook}. 
%It has a general
%purpose wrapper named \mbox{\tt scr}\footnote{Not written yet} which
%has other capabilities too.
Incidentally, this function also returns the area of the state-space for a given set
of parameter values, as an attribute to the function value, which will
be used in converting $\hat{N}$ to $\hat{D}$.
To use this function to obtain the MLEs for the wolverine camera trap
study, we execute the following commands (note: these are in the help
file and will execute if you type \mbox{\tt example(intlik3)}:
{\small
\begin{verbatim}
library("scrbook")
data("wolverine")
 
traps<-wolverine$wtraps
traplocs<-traps[,2:3]/10000
K.wolv<-apply(traps[,4:ncol(traps)],1,sum)

y3d<-SCR23darray.fn(wolverine$wcaps,traps)
y2d<-apply(y3d,c(1,3),sum)

starts<-c(-1.5,1.2,log(4))
frog<-nlm(intlik3,starts,hessian=TRUE,y=y2d,K=K.wolv,X=traplocs,delta=.2,ssbuffer=2)
There were 23 warnings (use warnings() to see them)

frog
$minimum
[1] 220.4313

$estimate
[1] -2.817610  1.254757  3.583690

$gradient
[1]  1.210460e-06 -5.255072e-06 -5.710212e-07

$hessian
           [,1]       [,2]      [,3]
[1,]  37.686164 -11.849561  4.686501
[2,] -11.849561  30.842624 -9.193201
[3,]   4.686501  -9.193201 12.973354

$code
[1] 1

$iterations
[1] 12
\end{verbatim}
}
Of course we're interested in obtaining an estimate of population size
for the prescribed state-space, or density, and associated measures of
uncertainty which we do using the delta method
\citep[][Appendix F4]{williams_etal:2002}.
To do all of that we need to manipulate the output of
\mbox{\tt nlm} since we have our  estimate in terms of $\mbox{\tt
  log(n0)}$. We execute the following commands:
{\small 
\begin{verbatim}
frog<-nlm(intlik3,starts,hessian=TRUE,y=y2d,K=K.wolv,X=traplocs,delta=.2,ssbuffer=2)
Nhat<-nrow(y2d)+exp(frog$estimate[3])
area<-attr(intlik3(starts,y=y2d,K=K.wolv,X=traplocs,delta=.2,ssbuffer=2),"SSarea")
Dhat<- Nhat/area

Dhat
[1] 0.5494956

SE<- (1/area)*exp(frog$estimate[3])*sqrt(solve(frog$hessian)[3,3])

SE
[1] 0.1087101
\end{verbatim}
} 
So our estimate of density is $0.55$ individuals per ``standardized
unit'' which is 100 $km^2$, because we divided UTM coordinates by
10000.  So this is about 5.5 individuals per 1000 $km^2$,
with a SE of around 1.09
individuals.  This compares closely with $5.77$
reported in
sec. \ref{scr0.sec.wolverine} based on Bayesian
analysis of the model.


To evaluate the effect of the integration grid density, 
we obtained the MLEs for a state-space buffer of 2 (standardized
units) and for integration grid with spacing $\delta = .3, .2, .1,
.05$. The MLEs for these 4 cases including the relative runtime are
given in Table \ref{mle.tab.integration}.
We see the results change only slightly as the fineness of the
integration grid increases. Conversely, the runtime on the platform of
the day for the 4 cases increases rapidly. 
These runtimes could be regarded in
relative terms,  across platforms, for gaging the decrease in
speed as the fineness of the integration grid increases. The effect of
this is that we anticipate some numerical error in approximating the
integral on a mesh of points, and that error increases as the
coarseness of the mesh increases. 


\begin{table}[ht]
\centering
\caption{Run time and MLEs for different integration grid resolutions
  for the wolverine camera trapping data.}
\begin{tabular}{crccc}
\hline \hline
$\delta$ &   & \multicolumn{3}{c}{Estimates} \\ \hline
         &  runtime        & $\hat{\alpha}_0$ & $\hat{\alpha}_1$ &  $\widehat{\log(n_0)}$ \\ \hline
 0.30   &  9.9  &  -2.819786 & 1.258468 & 3.569731  \\
 0.20   & 32.3  &  -2.817610 & 1.254757 & 3.583690 \\
 0.10  & 115.1  &  -2.817570 & 1.255112 & 3.599040 \\
 0.05 &  407.3 &   -2.817559&  1.255281&  3.607158 \\ \hline
\end{tabular}
\label{mle.tab.integration}
\end{table}


We studied the effect of the state-space buffer on the MLEs,
using a fixed $\delta = .2$ for all analyses. The results are show in Table \ref{mle.tab.buff}. 
We used state-space buffers
of 1 to 4 units stepped by .5. As we can see in Table \ref{mle.tab.buff}, 
the estimates of $D$ stabilize rapidly and the incremental difference
is within the numerical error associated with approximating the
integral.  

\begin{table}[ht]
\centering
\caption{Results of the effect of the state-space buffer on the MLE. 
Given here are the state-space buffer (buff), area of the state-space (area), the
MLE of $N$ ($\hat{N}$) for the prescribed state-space and the corresponding MLE of
density ($\hat{D}$).}
\begin{tabular}{crcc}
\hline \hline
buff    & area & $\hat{N}$ & $\hat{D}$ \\ \hline
 1.0 & 66.98212 & 37.73338 & 0.5633352  \\
 1.5 & 84.36242 & 46.21008 & 0.5477567  \\
 2.0 &103.74272 & 57.00617 & 0.5494956  \\
 2.5 &125.12302 & 69.03616 & 0.5517463  \\
 3.0 &148.50332 & 82.17550 & 0.5533580  \\ 
 3.5 &173.88362 & 96.44018 & 0.5546249  \\
 4.0 &201.26392 &111.83524 & 0.5556646  \\  \hline
\end{tabular}
\label{mle.tab.buff}
\end{table}


\subsection{Restricted state-space}
\label{mle.sec.shapefile}

In sec. \ref{scr0.sec.discrete} 
 we used a discrete representation of
the state-space in order to have control over its extent and shape,
for example so that we could clip out ``non-habitat''. Clearly that
formulation of the model is relevant to the calculation of the
marginal 
likelihood in the sense that the discrete state-space 
is equivalent to the integration grid.
Thus, for example, we could
easily compute the MLE of parameters under some model with a
restricted state-space merely by creating the required state-space at
whatever grid resolution is desired, and then inputting that state-space
into the likelihood function above, instead of computing it in the
function itself. We can easily create an explicit
state-space grid for integration from arbitrary polygons or GIS
shapefiles \index{shapefile} which we 
demonstrate here. Our approach is to create the integration grid
(or state-space grid) outside of the likelihood evaluation, and then
determine which points of the grid lie in the polygon defined by the
shapefile using 
functions in the {\bf R} packages \mbox{\tt sp} \index{R
  package!sp} and
\mbox{\tt maptools} \index{R package!maptools} \index{maptools}.  Here
are the {\bf R} commands for doing this:  
{\small
\begin{verbatim}
library(maptools}
library(sp)
SSp<-readShapeSpatial('Sim_Polygon.shp')
Pcoord<-SpatialPoints(G)
PinPoly<-over(Pcoord,SSp)
Pin<-as.numeric(!is.na(PinPoly[,1]))
G<-G[Pin==1,]
\end{verbatim}
}
We created  the function \mbox{\tt intlik4} which accepts the integration
grid as an explicit argument, and this function is also available in
the package  \mbox{\tt scrbook}.

We apply this modification to the wolverine camera trapping
study. \citet{royle_etal:2011jwm} created 2, 4 and 8 km state-space
grids so as to remove ``non-habitat'' (mostly ocean, bayes, and large
lakes). We previously analyzed the model using {\bf JAGS} and {\bf WinBUGS} in
Chapt. \ref{chapt.scr0}.  To set up the wolverine data and fit the
model we execute the following commands
{\small 
\begin{verbatim}
library("scrbook")
data("wolverine")

traps<-wolverine$wtraps
traplocs<-traps[,2:3]/10000
K.wolv<-apply(traps[,4:ncol(traps)],1,sum)

y3d<-SCR23darray.fn(wolverine$wcaps,traps)
y2d<-apply(y3d,c(1,3),sum)
G<-wolverine$grid2/10000

starts<-c(-1.5,1.2,log(4))
frog<-nlm(intlik4,starts,hessian=TRUE,y=y2d,K=K.wolv,X=traplocs,G=G)

frog
$minimum
[1] 225.8355

$estimate
[1] -2.995541  1.265021  4.110476

$gradient
[1]  3.808485e-05 -9.930579e-06  3.906668e-06

$hessian
           [,1]       [,2]      [,3]
[1,]  47.059393 -21.415124  4.406148
[2,] -21.415124  38.255192 -7.386245
[3,]   4.406148  -7.386245 15.406613

$code
[1] 1

$iterations
[1] 14
\end{verbatim}
}

Next we convert the parameter estimates to estimates of total
population size for the prescribed state-space, and then obtain an
estimate of density (per 1000
$km^2$) using the area computed as the number of pixels in the
state-space grid \mbox{\tt G} multiplied by the area per grid cell. In
the present case (the calculation above) we used a state-space grid
with $2 \times 2$ $km$ pixels.  Finally, we compute
a standard errors using the delta approximation: 
\begin{verbatim}
Nhat<- 21+exp(frog$estimate[3])
SE<-  exp(frog$estimate[3])*sqrt(solve(frog$hessian)[3,3])
D<- (Nhat/(nrow(G)*area))*1000
SE.D<- (SE/(nrow(G)*area))*1000
\end{verbatim}
We did this for each the 2 $km$, 4 $km$ and 8 $km$ state-space grids
which produced the estimates summarized in Tab. \ref{mle.tab.wolv}.
These estimates compare with the 8.6 (2 km grid) and 8.2 (8 km grid)
reported in 
\citet{royle_etal:2011jwm} based on a clipped state-space as described
in sec. \ref{scr0.sec.discrete}.

\begin{table}
\centering
\caption{MLEs for the wolverine camera trapping data using 2, 4 and 8 km state-space grids.}
\begin{tabular}{cccccccc}
\hline \hline
grid &  $\alpha_0$  &  $\alpha_1$ &   $log(n_0)$  & $N$   &  SE & D(1000) &  SE \\ \hline
2  &  -2.995541& 1.265021 &4.110476 &81.97574& 16.30904 &8.310598 &1.653391\\
4  &  -2.991268&1.344055  &4.157026 &84.88126& 16.76202 &8.570401& 1.692450\\
8   & -3.051705& 1.080083 &4.058542 &78.88983& 15.31392 &7.851296& 1.524077\\   \hline
\end{tabular}
\label{mle.tab.wolv}
\end{table}


\begin{comment}
\subsection{
Exercises
}

{\flushleft
1.	Compute the 95\% confidence interval for wolverine density,
somehow. Comment on the practical implication of this level of precision.
}

{\flushleft
2.	Compute the AIC of this model and modify \mbox{\tt intlik3}
 to consider alternative link functions (at least one additional) and
 compare the  AIC of the different models and the estimates. Comment. 
}
\end{comment}


\section{DENSITY and the R package \mbox{\tt secr} }
\label{mle.sec.secr}

{\bf DENSITY} is a software program developed by \citet{efford:2004}
for fitting spatial capture-recapture models based mostly on classical
maximum likelihood estimation and related inference methods.
\citet{efford:2011} has also released an {\bf R} package called
\mbox{\tt secr}, that contains much of the functionality of {\bf
  DENSITY} but also incorporates new models and features.  Here, we
briefly introduce the \mbox{\tt secr} package which we prefer to use
instead of {\bf DENSITY} because it allows us to remain in the {\bf R}
environment for data processing and summarization. 

To install
and run models in \mbox{\tt secr}, you must download the package and
load it in
{\bf R}.
\begin{verbatim}
 install.packages("secr")
 library(secr)
\end{verbatim}
\mbox{\tt secr} allows the user to simulate data and fit a suite of models with
various detection functions and covariate responses. It also contains
a number of helpful constructor functions for creating objects of the
proper class that are recognized by other \mbox{\tt secr}
functions. We provide a brief overview of the capabilities here, but
the \mbox{\tt secr} help manual can be accessed with the command:
\begin{verbatim}
 RShowDoc("secr-manual", package = "secr")
\end{verbatim}


The main model-fitting function in   \mbox{\tt secr} is called
\mbox{\tt secr.fit}, which 
makes use of the
standard {\bf R} model specification framework with tildes. 
As an example, the equivalent of the
basic model SCR0  is fitted as follows: {\bf XXXX need centered tildes
  here XXXXX}
\begin{verbatim}
 secr.fit(capturedata, model = list(D~1, g0~1, sigma~1), buffer = 20000)
\end{verbatim}
where \mbox{\tt capturedata} is the  object created by \mbox{\tt secr}
containing the encounter history data and the trap information, and
the model expression \verb#g0~1# indicates the intercept-only (i.e.,
constant) model.  Possible predictors for detection probability
include both pre-defined variables (e.g., \mbox{\tt t} and \mbox{\tt
  b} corresponding to ``time'' and ``behavior''), and user-defined
covariates of several kinds.  For example, to include a global
behavioral response, this would be written as \verb#g0~b#.  The
discussion of this (global versus local trap-specific behavioral
response) and other covariates is developed more in
Chapt. \ref{chapt.covariates}.

Before we can fit the models, the data must first be packaged properly
for 
\mbox{\tt secr}.  
We require data files that contain two types of information:
trap layout (location and
identification information for each trap), which is equivalent to our
trap deployment file (TDF) described in sec. \ref{scr0.sec.wolverine}
and the capture data file containing 
sampling session, animal identification, trap day, and trap
location,  equivalent in information content to our encounter data file (EDF).
There are three important constructor functions that help package-up
your 
data for use in \mbox{\tt secr}:
\mbox{\tt read.traps},
\mbox{\tt make.capthist} and
\mbox{\tt read.mask}. 
We provide a brief description of each here, but apply them to our
wolverine camera trapping data in the next section:
\begin{itemize}
\item[(1)] 
\mbox{\tt read.traps}: This function points to an external file or
{\bf R} data object containing the trap coordinates, and other
information, and also requires specification of the type of encounter
devices (described in the next section). A typical application of this
function would be:
\begin{verbatim}
trapfile<-read.traps(data=traps,detector="proximity")
\end{verbatim}
\item[(2)] \mbox{\tt make.capthist}: This function takes the EDF and combines it
with trap information, and the number of sampling occasions. A typical
application looks like this:
\begin{verbatim}
capturedata<-make.capthist(enc.data,trapfile,fmt="trapID",noccasions=165)
\end{verbatim}
See \mbox{\tt ?make.capthist} for definition of distinct file
formats. Specifying  \mbox{\tt fmt = trapID}  is equivalent to our EDF format.
\item[(3)] \mbox{\tt read.mask}: If there is a habitat mask
  available 
(as described in sec. \ref{mle.sec.shapefile}), then this function
will organize it so that \mbox{\tt secr.fit} knows what to do with it.
The function accepts either an external file name (see \mbox{\tt
  ?read.mask} for details of the structure) or a $nG \times 2$ {\bf R}
object, say \mbox{\tt mask.coords},
containing the coordinates of the mask. A typical application looks
like:
\begin{verbatim}
grid<-read.mask(data=mask.coords)
\end{verbatim}
\end{itemize}
These constructor functions produce output that can then be used in
the fitting of models using \mbox{\tt secr.fit}.

\subsection{Encounter device types and detection models}

The
\mbox{\tt secr} package requires that you specify the type of encounter
device. 
Instead of of 
describing models by their statistical distribution (Bernoulli,
Poisson, etc..), 
 \mbox{\tt secr} 
uses certain operational classifications of detector types including
'proximity', 'multi', 'single', 'polygon' and 'signal'.
For 
camera trapping/hair snares we might consider `proximity' detectors or `count'
detectors.  The `proximity' detector type allows, at most, one
detection of each individual at a particular detector on any occasion
(i.e., it is equivalent to the Bernoulli or binomial encounter process
model, or model SCR0).
The `count' detector designation allows repeat encounters of each
individual at a particular detector on any occasion.  There are other
detector types that one can select such as: `polygon' detector type
which allows for a trap to be a sampled polygon
\citep{royle_young:2008} which we discuss further in Chapt. \ref{chapt.searchencounter},
and 'signal' detector which allows for traps that have a strength
indicator, e.g., acoustic arrays \citep{dawson_efford:2009}.
%The detector types `single' and
%'multi' can be confusing as 'multi' seems like it would appropriate
%for something like a camera trap, but instead 
The detector types 'single' and 'multi' 
refer to traps that retain individuals, thus precluding the ability
for animals to be captured in other traps during the sampling
occasion.  The 'single' type indicates trap that can only catch one
animal at a time (single-catch traps), while 'multi' indicates traps that may catch more
than one animal at a time (multi-catch). These are both variations of
the multinomial encounter models described in
Chapt. \ref{chapt.poisson-mn}.

As with all SCR models, \mbox{\tt secr} fits a detection function relating
the probability of detection to the distance of a detector from an
individual activity center. \mbox{\tt secr} allows the user to specify one of a
variety of detection functions including the commonly used
half-normal, hazard rate, and exponential.  There are 12 different
functions (see Tab. \ref{covariates.tab.detmodels} in Chapt. \ref{chapt.covariates}), but
some are only available for simulating data.
%, and one
%should be cautious when using different detection functions as the
%interpretation of the parameters, such as $\sigma$, may not be consistent
%across formulations.  
The different detection functions are defined in
the \mbox{\tt secr} manual and can be found by calling the help function for the
detection function:
\begin{verbatim}
 ?detectfn
\end{verbatim}
It is useful to note that \mbox{\tt secr} requires the buffer distance to be
defined in meters and density will be returned as number of animals
per hectare.  Thus to make comparisons between \mbox{\tt secr} and other models,
we will often have to convert the density to the same units.  Also,
note that $\sigma$ is returned in units of meters.

Most of the detection functions available in \mbox{\tt secr} contain
some kind of a scale parameter which is usually labeled
$\sigma$. However, we caution that the meaning of this parameter
depends on the specific model being used and it should not be directly
compared as a measure of home-range size across models. Instead, as we
noted in sec. \ref{scr0.sec.implied} any encounter probability model
does imply an {\bf XXXX IMPLY AN IMPLICIT - ISN'T THAT SAYING THE SAME THING TWICE? XXX}
implicit model of space-usage and fitted encounter models should be
converted to a common currency such as ``area used.'' 


\subsection{Analysis using the \mbox{\tt secr} package}

To demonstrate the use of the \mbox{\tt secr} package, we will show
how to do the same analysis on the wolverine study as shown in
sec. \ref{scr0.sec.wolverine}. To use the \mbox{\tt secr} package, the
data need to be formatted in a similar but slightly different manner
than we use in {\bf WinBUGS}.

For example, in sec. \ref{scr0.sec.wolverine} we introduced a standard
data format for the encounter data file (EDF) and trap deployment file
(TDF). The EDF shares the same format as that used by the \mbox{\tt
  secr} package with 1 row for every encounter observation and 4 columns representing 
trap session ('Session'), individual identity ('ID'), sample occasion
('Occasion'), and trap identity ('trapID').
For a standard closed population study that takes place during a
single season, the 'Session' column
in our case is all 1s, to indicate a single primary sampling
occasion. 
In addition to providing the EDF,  we must tell \mbox{\tt secr}
information about the traps, which is formated as a matrix with column labels
'trapID', 'x' and 'y', the last two being the coordinates of each
trap, with additional columns representing the operational state of
each trap during each occasion (1=operational, 0=not). 

We demonstrate these differences now by
walking through an analysis of the wolverine camera trapping data
using  \mbox{\tt secr}.
To read in
the trap locations and other related information, we make use of the
constructor function \mbox{\tt read.traps} which also requires that we
specify the detector type.
The
detector type is important because it will determine the likelihood
that \mbox{\tt secr} will use to fit the model.  Here, we have
selected ``proximity'' which corresponds to the Bernoulli encounter
model in which individuals are captured at most once in each trap
during each sampling occasion: 
{\small
\begin{verbatim}
library("secr")
library("scrbook")
data("wolverine")

traps<-as.matrix(wolverine$wtraps)
dimnames(traps)<-list(NULL,c("trapID","x","y",paste("day",1:165,sep="")))
traps1<-as.data.frame(traps[,1:3])
trapfile1<-read.traps(data=traps1,detector="proximity")
\end{verbatim}
}
Here we note that trap coordinates are extracted from the wolverine
data but we do {\it not} standardize them. This is because
\mbox{\tt secr} defaults to coordinate scaling of meters which is the
extant scaling of the wolverine trap coordinates. Note that we add a 'trapID' column to
the trap coordinates and provide appropriate column labels to the
'traps' matrix. 
An important aspect of the
wolverine study is that while the camera traps were operated over a
165 day period, each trap was operational during only a portion of
that period. We need to provide the trap operation information which
is contained in the columns to the right of the trap coordinates in
our standard TDF. Unfortunately, this is less easy to do in \mbox{\tt
  secr}, which requires an external file with a single long string of
1's and 0's indicating the days in which each trap was operational (1)
or not (0). We can create this external file and then read it back in
using these commands:
\begin{verbatim}
hold<-rep(NA,nrow(traps))
for(i in 1:nrow(traps)){
hold[i]<-paste(traps[i,4:ncol(traps)],collapse="")
}
traps1<- cbind(traps[,1:3],"usage"=hold)

write.table(traps1, "traps.txt", row.names=FALSE, col.names=FALSE)
trapfile2<-read.traps("traps.txt",detector="proximity") 
\end{verbatim}
These operations can be accomplished using the function \mbox{\tt
  scr2secr} which is provided in the {\bf R} package \mbox{\tt scrbook}.

After reading in the trap data, we now need to create the encounter matrix
or array using the
\mbox{\tt make.capthist} command, where we provide the capture
histories in EDF format, which is the existing format of
the data input file \mbox{\tt wcaps}.
In creating the capture history, we provide also the trapfile created
previously, the format (e.g., here EDF format is \mbox{\tt fmt=
  ``trapID''}), 
and finally, we provide the number of occasions. We also set up a
habitat mask using the $2 \times 2$ $km$ grid which we used previously
in the analysis of the wolverine data and then pass the relevant
objects to \mbox{\tt secr.fit} as follows:
{\small 
\begin{verbatim}
#
# grab the encounter data file and format it:
#
wolv.dat<-wolverine$wcaps
dimnames(wolv.dat)<-list(NULL,c("Session","ID","Occasion","trapID"))
wolv.dat<-as.data.frame(wolv.dat)
wolvcapt2<-make.capthist(wolv.dat,trapfile2,fmt="trapID",noccasions=165)

# grab the habitat mask (2 x 2 km) and format it:
#
gr2<-(as.matrix(wolverine$grid2))
dimnames(gr2)<-list(NULL,c("x","y"))

# To fit the model we use secr.fit:
#
wolv.secr2<-secr.fit(wolvcapt2,model=list(D~1, g0~1, sigma~1), buffer=20000,mask=gr2)
\end{verbatim}
}
We are using the 
basic ``proximity detector'' model (SCR0), so we do not need to make any specifications in
the command line because we have specified the detector type using the
constructor function \mbox{\tt read.traps},
except to provide the buffer size (in $m$).  To
specify different models, you can change the default
\verb#D~1, g0~1, sigma~1#, which the interested reader can do with
very little difficulty. We provide all of these commands and
additional analyses in the \mbox{\tt scrbook} package with the
function called \mbox{\tt secr\_wolverine}. Printing the output object
produces the following (slightly edited):

{\small
\begin{verbatim}
wolv.secr2

secr 2.3.1, 15:52:45 29 Aug 2012

Detector type     proximity 
Detector number   37 
Average spacing   4415.693 m 
x-range           593498 652294 m 
y-range           6296796 6361803 m 
N animals       :  21  
N detections    :  115 
N occasions     :  165 
Mask area       :  987828.1 ha 

Model           :  D~1 g0~1 sigma~1 
Fixed (real)    :  none 
Detection fn    :  halfnormal 
Distribution    :  poisson 
N parameters    :  3 
Log likelihood  :  -602.9207 
AIC             :  1211.841 
AICc            :  1213.253 

Beta parameters (coefficients) 
           beta    SE.beta       lcl       ucl
D     -9.390124 0.22636698 -9.833795 -8.946452
g0    -2.995611 0.16891982 -3.326688 -2.664535
sigma  8.745547 0.07664648  8.595323  8.895772

Variance-covariance matrix of beta parameters 
                  D            g0        sigma
D      0.0512420110 -0.0004113326 -0.003945371
g0    -0.0004113326  0.0285339045 -0.006269477
sigma -0.0039453711 -0.0062694767  0.005874683

Fitted (real) parameters evaluated at base levels of covariates 
       link     estimate  SE.estimate          lcl          ucl
D       log 8.354513e-05 1.915674e-05 5.360894e-05 1.301982e-04
g0    logit 4.762453e-02 7.661601e-03 3.466689e-02 6.509881e-02
sigma   log 6.282651e+03 4.822512e+02 5.406315e+03 7.301037e+03
\end{verbatim}
}

The object returned by \mbox{\tt secr.fit} provides extensive default
output when printed. Much of this is basic descriptive information
about the model, the traps, or the encounter data. We focus here on
the parameter estimates.
Under the fitted (real) parameters, we find $D$, the density, given in
units of individuals/hectare (1 hectare = 10000 $m^2$).  To convert this
into individuals/1000 $km^2$, we multiply by 100000, thus our density
estimate is 8.35 individuals/1000 $km^2$.  The parameter $\sigma$ is given in units of
meters, and so this corresponds to
 $6.283$ $km$.  Both of these estimates are very similar to those
obtained in our likelihood analysis summarized in Tab. \ref{mle.tab.wolverine}
which, for the $2 \times 2$ $km$ grid, we obtained $\hat{D} = 8.31$
with a SE of $100000 \times 
1.915674e-05 = 1.9156$
and, accounting for the scale difference (1 unit = 10000 $m$ in the
previous analysis), $\hat{\sigma} = \sqrt{1/(2\hat{\alpha}_{1})}*10000
= 6.289$ $km$. 
The difference in the MLE between Tab. \ref{mle.tab.wolverine} and those
produced by \mbox{\tt secr} are likely due to subtle differences in  internal
tuning of optimization algorithms, starting values or other numerical
settings. In addition, see the next section.
On the other hand, the SE is slightly larger based on \mbox{\tt secr}
which is due to a subtle difference in the interpretation of $D$ under
the \mbox{\tt secr} model (See below). 







\begin{comment}
As an
exercise, run this analysis for 30 and 40 km buffers and compare those
found in section 4.6 under {\bf WinBUGS}.  
NOTE: The function \mbox{\tt
  secr.fit} 
will return a
warning when the buffer size appears to be too small.  This is useful
particularly with the different units being used between programs and
packages.
\end{comment}

\subsection{Likelihood Analysis in the \mbox{\tt secr} Package}

The \mbox{\tt secr} package does likelihood analysis of SCR models for
most classes of models as developed by
\citet{borchers_efford:2008}. Their formulation deviates slightly from
the binomial form we presented in sec.  \ref{mle.sec.Nunknown} above
(though \citet{borchers_efford:2008} mention the binomial form).
Specifically, the likelihood that \mbox{\tt secr} implements is that
based on removing $N$ from the likelihood by integrating the binomial
likelihood (Eq.  \ref{mle.eq.binomialform} above) over a Poisson prior
for $N$ -- what we will call the {\it Poisson-integrated likelihood} as
opposed to the conditional-on-$N$ ({\it binomial-form}) considered
previously.

To develop the Poisson-integrated likelihood 
we compute the marginal
probability of each ${\bf y}_{i}$ and the probability of an all-0
encounter history, $\pi_{0}$, as before, 
to arrive at the  marginal likelihood in the binomial-form:
\[
 {\cal L}({\bm \alpha},n_{0} | {\bf y})  = \frac{N!}{n! n_{0}!} 
 \left\{ \prod_{i}  [{\bf y}_{i}|{\bm \alpha}] 
\right\}
 \pi_{0}^{n_{0}}
\]
Now, what \citet{borchers_efford:2008} do is
assume that $N \sim \mbox{Poisson}(\Lambda)$ and they do a further level
of marginalization over this prior distribution:
\[
\sum_{n_{0}=0}^{\infty}  
\frac{N!}{n! n_{0}!} 
 \left\{ \prod_{i}  [{\bf y}_{i}|{\bm \alpha}] \right\}
 \pi_{0}^{n_{0}}
\frac{\exp(-\Lambda) \Lambda^{N}}{N!}
\]
We emphasize there are two marginalizations
 going on here: (1) the
integration to remove the latent variables ${\bf s}$; and, (2) 
summation to remove the parameter $N$. 
This produces exactly this likelihood:
\[
{\cal L}_{2}({\bm \alpha}, \Lambda | {\bf y}) = 
 \left\{ \prod_{i}  [{\bf y}_{i}|{\bm \alpha}] \right\}  \Lambda^{n}   \exp( - \Lambda \pi_{0} )
\]
which is Eq. 2 of \citet{borchers_efford:2008} except for notational
differences. It also resembles the binomial-form of the likelihood in
Eq. \ref{mle.eq.binomialform} except with 
$\Lambda^{n}   \exp( - \Lambda \pi_{0} )$ replacing the combinatorial
term and the $\pi_{0}^{n_{0}}$ term. We provide a function for
computing this in the \mbox{\tt scrbook} package called \mbox{\tt
  intlik3Poisson}. The help file for that function shows how to
conduct a small simulation study to compare the MLE under the
Poisson-integrated likelihood with that from the binomial form. 

The essential distinction between our MLE and Borchers and Efford
as implemented in \mbox{\tt secr}
is whether you keep $N$ in the model or remove it by
integration over a Poisson prior. If you have prescribed a state-space
explicitly with a sufficiently larger buffer, then we imagine there
should be hardly any difference at all between the MLEs obtained by
either the Poisson-integrated likelihood or the binomial-form of the likelihood.
There is a subtle distinction in the sense that under the binomial
form, we estimate the realized population size $N$ for the state-space
whereas, for the Poisson-integrated form we estimate the {\it prior} 
expected value which would apply to a hypothetical new study of a
similar population.


Both models (likelihoods) assume ${\bf s}$ is uniformly distributed over space, but
for the binomial model we  make no additional assumption about $N$
whereas we assume $N$ is Poisson using the formulation in \mbox{\tt
  secr} from 
\citep{borchers_efford:2008}.
Using data augmentation we could do a similar kind of integration but 
integrate $N$ over a binomial ($M,\psi$) prior. So obviously this is
approximately the same as $M$ gets large. However, doing a Bayesian
analysis by MCMC ,  we obtain an
estimate of both $N$ and the parameter controlling its expected value
$\psi$ which are, in fact, both identifiable from the data even using
likelihood analysis \citep{royle_etal:2007}.   That said we can integrate N
out completely and just estimate $\psi$ as we noted in sec.
\ref{mle.sec.intlikDA} above.
And we could make a prediction for a new study which would be based on
the posterior distribution of $M \psi$ which, we imagine, should have
slightly larger uncertainty associated with it. 



\subsection{Multi-Session Models in \mbox{\tt secr}}
\label{mle.sec.multisession}

In practice we will often deal with SCR data that have some meaningful
stratification or group structure.  For example, we might conduct
mist-netting of birds on $K$ consecutive days, repeated, say, $T$ times
during a year, or perhaps over $T$ years. Or we might collect data
from
 $R$ distinct trapping grids.  In these cases, we have $T$ or $R$
 groups which we might reasonably regard as being samples of indepdent
 populations. 
While the groups might be distinct sites, year, or periods within
years, they could also be other biological groups such as sex or age. 
Conveniently, 
\mbox{\tt secr} fits a specific model for stratified populations -- referred
to as {\it multi-session} models. These models build on 
the Poisson assumption which underlies the integrated likelihood used
in \mbox{\tt secr} (as described in the previous section).  To
understand the technical framework, let 
$N_{g}$ be the population size of 
group $g$ and {\it assume}
\[
 N_{g} \sim \mbox{Poisson}(\Lambda_{g}).
\]
Naturally, we model group-specific covariates on $\Lambda_{g}$:
\[
 log(\Lambda_{g}) = \beta_{0} + \beta_{1} z_{g}.
\]
Under this model,  we can marginalize {\it all} $N_{g}$ parameters out
of the likelihood to concentrate the likelihood on the parameters
$\beta_{0}$ and $\beta_{1}$ precisely as discussed in the previous
section. This Poisson hierarchical model 
is the basis of the multi-session models in \mbox{\tt secr}.

To implement a multi-session model (or stratified population model) in
\mbox{\tt secr},
we provide the relevant stratification information in the 
 'Session' variable of the input encounter data file (EDF). If
 'Session' has multiple values then a
``multi-session'' object is created by default and session-specific variables can
be described in the model. For example, if the session has 2 values
for males and females then we have sex-specific densities , and
baseline encounter probability $p_{0}$ (named $g_{0}$  in \mbox{\tt
  secr}) by just doing this:
\begin{verbatim}
out<-secr.fit(wolvcapt,model=list(D~session, g0~session, sigma~1), buffer=20000)
\end{verbatim}
More detailed analysis is given in sec. \ref{gof.sec.aic} where we fit
a number of different models and apply methods of model selection to
obtain model-averaged estimates of density.

We can also easily implement stratified population models in the
various {\bf BUGS} engines using data augmentation
\citep{converse_royle:2012,converse_royle:2013} which we address, with
examples,  in Chapt. \ref{chapt.hscr}.


\begin{comment}
\subsection{Analysis of Efford's Possum Data}

Demonstrates an explicit model misspecification.
Or maybe -- likelihood easy to apply to multinomial likelihood from
previous chapter which is probably what secr does (checking with efford)

Use the mask he provides (show picture)
Use rectangular mask.

Cite above material on state-space grid. Use our likelihood function
with his state-space grid.

Need to focus on a specific illustration here. I think using
secr to fit the basic model using the state-space grid or no
state-space
grid, and using our likelihood function, would be fine.
Thats 4 estimates. 

Secr + grid
secr + no grid -- really a fine grid I think 
my likelihood + secr grid
my likelihood + really fine grid

Note: should not compare AIC across analysis platforms because the
likelihoods can be scaled arbitrarily -- depending on what to leave in
or leave out.
\end{comment}


\section{Summary and Outlook}

In this chapter, we discussed basic concepts
related to classical analysis of SCR models based
on likelihood methods. Analysis is
based on the so-called integrated or marginal likelihood in which the individual
activity centers (random effects) are removed from the
conditional-on-{\bf s} likelihood by integration. We showed how to construct
the integrated likelihood and fit some simple models in the {\bf R}
programming language.  In addition, likelihood analysis for some broad
classes of SCR models can be accomplished using the
{\bf R}
library \mbox{\tt secr} which we provided a brief introduction of. In
later 
chapters we provide more detailed analyses of SCR data likelihood
methods and the
\mbox{\tt secr}
package.

\begin{comment}
To compute the marginal (integrated) likelihood we have to precisely describe the
state-space of the underlying point process. In practice, this leads
to a ``buffer'' around the trap array. We note that this is not really a
``buffer strip'' in the sense of \citet{wilson_anderson:1985a},  
but it is somewhat more general here. In particular,
it establishes the support of the integrand and, 
in SCR models, it is an element of the model that
provides an explicit
linkage between population size $N$ and density $D$.
As a practical 
matter, it will typically be the case that, while estimates of $N$
increase with the area of the state-space (as they should!), estimates of density
stabilize. This is not a feature of the classical methods based on
using model $M_0$ or model $M_h$ and buffering the trap array.
\end{comment}

Why or why not use likelihood inference exclusively? For certain
specific models, it is probably more computationally efficient to
produce MLEs (for an example see Chapt. \ref{chapt.ecoldist}). However, {\bf BUGS} is extremely flexible in terms of
describing models, although it sometimes can be quite inefficient. We can
devise models in the {\bf BUGS} language easily that we cannot fit in
\mbox{\tt secr}. E.g.,
random individual effects of various types
(Chapt. \ref{chapt.covariates}), we can 
handle missing covariates in complete generality and seamlessly, and
impose arbitrary distributions on random variables. Moreover, models
can easily be adapted to include auxiliary data types. For example, we
might have camera trapping and genetic data and we can describe the
models directly in {\bf BUGS} and fit a joint model \citep{gopalaswamy_etal:2012}. For the MLE we have
to write a custom new piece of code for each model or hope someone has
done it for us, although you should be able to do this with the tools
we have provided here.  Later we consider open population models which are
straightforward to develop in {\bf BUGS} but, so far, there is no
available platform for doing MLE of such models, although we imagine one could develop
this.  On
the other hand, likelihood analysis makes it easy to do
model-selection by AIC and in some cases compute standard errors or
carry-out goodness-of-fit evaluations. 
\begin{comment}
Another thing that is more conceptual here is non-CSR point
processes (Chapt. \ref{chapt.state-space}) and generating predictions of how many
individuals have home range centers in any particular polygon.  Basic
benefits of Bayesian analysis have been discussed elsewhere (XXXXXXXX Chapter
2? BPA book? Link and Barker?) and we believe these are compelling.
\end{comment}

\begin{comment}
In summary, basic SCR models are easy to implement by either
likelihood or Bayesian methods but some users might
realize much more flexibility in model development using existing
platforms for Bayesian analysis. While these tend to be slow
(sometimes excruciatingly slow), this will probably not be an
impediment in most problems, especially at some near point in the
future as computers continue to improve.  
Since we spent a lot of time here talking about specific
technical details on how to implement likelihood analysis of SCR
models, we provided a corresponding treatment in the next chapter on
how to devise MCMC algorithms for SCR models. This is a bit more
tedious and requires more coding, but is not technically challenging
(except perhaps to develop highly efficient algorithms which we don't
excel at).
\end{comment}


