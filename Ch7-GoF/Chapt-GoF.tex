\chapter{
Model Selection and Assessment
}
\markboth{Model Assessment}{}
\label{chapt.gof}

\vspace{.3in}

% XXXX Great opener XXXX
Our purpose in life is to analyze models. By that, we mean one or more
of the following basic 4 tasks: (1) estimate parameters, (2) make predictions
of unobserved random variables, (3) evaluate the
relative merits of different models or choosing a best model (model
selection), and (4)  checking whether a specific model appears to provide a
reasonable description of our data or not (model checking, assessment,
or ``goodness-of-fit'').  In previous chapters we
addressed the problems of estimation of model parameters, 
and also making
predictions of latent variables, ${\bf s}$ or $z$, or
functions of these variables such as density or population size.
In this chapter, we focus on the last two of these basic
inference tasks: model selection (which model or models should be
favored), and model assessment (do the data appear to be consistent
with a particular model?).


In this chapter we review  basic strategies of model selection 
using both likelihood methods (as
implemented in the \mbox{\tt secr} package) and also Bayesian
analysis.
Specifically,
we review a number of standard methods of model selection that apply
to ``variable selection'' problems, when our set of models
consists of distinct covariate effects and they represent constraints
of some larger model.
For classical analysis based on likelihood, model selection by
AIC is the standard approach \citep{burnham_anderson:2002}.  For
Bayesian analysis we rely on a number of different methods.  We
demonstrate the use of the deviance information criterion (DIC)
\citep{spiegelhalter_etal:2002} for variable selection problems
although it has deficiencies when applied to hierarchical models in
some cases \citep{millar:2009}. 
We use the
Kuo and Mallick indicator variable selection approach
\citep{kuo_mallick:1998} which
produces direct statements
of posterior model probabilities which we think are the most useful,
and leads directly to model-averaged estimates of density.  There is a
good review paper recently by \citet{ohara_sillanpaa:2009} that hits
on these and many more related ideas for variable selection.
 In addition to \citet{ohara_sillanpaa:2009} we
also recommend \citet[][Chapt. 7]{link_barker:2010} for general
information on model selection and assessment.

 To check model adequacy in a Bayesian framework, or whether a
specific model provides a satisfactory description of our data set,
we rely exclusively on the Bayesian
p-value framework \citep{gelman_etal:1996}.  For assessing fit of 
SCR models, part of the challenge is coming up with good measures of
model fit, and there does not appear much definitive guidance 
in the literature on this point.  Following \citet{royle_etal:2011mee}, we break the
problem up into 2 components which we attack separately: (1)
Conditional on the underlying point process, does the encounter model
fit? (2) Do the uniformity and independence assumptions appear
adequate for the point process model of activity centers? The latter
component of model fit has a considerable precedence in the
ecological literature as it is analogous to the classical problem of
testing ``complete spatial randomness'' \citep{cressie:1992, illian_etal:2008}.


We apply some of these methods to the wolverine camera trapping
data first introduced in Chapt. \ref{chapt.scr0}
to investigate sex specificity of model parameters and whether
there is a behavioral response to encounter. We note that individuals
are drawn to the camera trap devices by food bait and therefore it
stands to reason that once an individual discovers a trap, it  might
be more likely to return subsequently -- the response of
trap happiness. We evaluate whether certain models for encounter
probability  appear to be
adequate descriptions of the data, and we evaluate the uniformity
assumption for the underlying point process.



\begin{comment}
A basic problem with these two objectives of model selection and model
assessment is their simultaneous use implies a kind of contradiction
which we call the {\it model selectors paradox}: Inferences are always
achieved using standard paradigms of parametric inference (Bayesian or
% XXXX Perhaps we should be open to non-parametric methods. Bob
% Dorazio seems to interested in non-parameteric approaches.
frequentist) which assert that the model is properly specified. That
is, we assume that the model is truth. This is paradoxical because we
all know that ``all models are wrong'' but, possibly, ``some are
useful.'' In fact, the notion that an ``assumption'' could even be
correct is itself something of an oxymoron.
\end{comment}

\begin{comment}
%XXX I love this quote -- would be nice to use it somewhere: XXXX
  Therefore we don't expect
or hope to make assumptions that are ``correct'' in any way. Gelman
and Shalizi (2010) say it this way: ``there is general agreement that,
in this domain, all models in use are wrong -- not just merely
falsifiable, but actually false.''  We should therefore refrain from
over-stating the relevance of any model.  [not sure where I was going
with this point]
\end{comment}



% XXXX Suggest un-commenting this material
\begin{comment}
A few specific questions to resolve:
\begin{itemize}
\item[(1)]  Can AIC or DIC choose among detection models and does it matter?
\item[(2)]  Can AIC or DIC choose among more substantial models?  We should
 assume it does. Can we rig up a test?  Suggest: (1) trap level
 covariate and trying DIC; (2) Model Mb vs. not
\item[(3)] Simultaneous to above, see if we can do that.......
Posterior model probabilities: Can we demo this?   With detection models?
 Don't do much analysis of the situation, just demo it maybe?
\item[(4)]  Basic framework for Bayesian GoF. Power under alternatives.
(clustering, regularity, uniformity) and misspecified encounter models
-- do they show a lack of fit?
\end{itemize}
\end{comment}



\begin{comment}
\subsection{The Role of model assumptions}

{\bf XXXXX NOTE THIS IS COMMENTED OUT XXXXXXXXXXXXXXXXXXXXXXXXXXX}

Referees are quick to point out that bivariate normal distributions as
models for home range are simply wrong. As if using wrong models
somehow invalidates something. (is that a statistical paradigm that I
missed?). The issues are whether its an adequate model, whether we can
refute it, whether resulting inferences are sensitive to it, and
whether the model is useful for its intended purpose regardless.
If  we really wanted a great description of a home range then we would do
something different besides conduct an SCR study. The purpose of most
SCR models is not to study home range geometry and morphology but, rather,
to estimate density and possibly other vital rates such as survival and
recruitment. In addition, it is my experience that the same people who
criticize models as implying overly simple models actually cannot describe
alternative models except using jargon and procedures like "kernel
smoothing", "utilization distributions" and "neural networks". So, on
the one hand, the bivariate normal distribution is overly simple, and
therefore we should use "neural networks"?  What irritates me even more
is referees who emphatically assert that "real home ranges aren't bivariate
normal" and then they go on to cite references who somehow "proved" they
are not. These people have no clue what statistics is good for and what
the point of SCR models is, nor can they grasp the relevance of the home
range model - namely, as a model for explaining heterogeneity in
capture-probability. To be sure, the bivariate normal model is an
over-simplification but so far it has not been shown to be ineffective
in SCR problems, and besides  substantially more flexible models have been
developed \citep{royle_etal:2012ecol}.

What we care about in models of capture-recapture data is whether the
bivariate normal (or any model) provides an adequate description of the
encounter process. That is the question we will evaluate here.
\end{comment}





\begin{comment}

%\section{Strategies for Model Selection}

We review a number of standard methods of model selection that apply
to ``variable selection'' problems. That is, when our set of models
consists of distinct covariate effects and they represent constraints
of some larger model.
For classical analysis based on likelihood, model selection using 
the Akaike information criterion (AIC)
is the standard approach \citep{burnham_anderson:2002}.  For
Bayesian analysis we rely on a number of different methods.  We
demonstrate the use of the deviance information criterion (DIC)
\citep{spiegelhalter_etal:2002} for variable selection problems
although it has deficiencies when applied to hierarchical models in
some cases \citep{millar:2009}. 
We use the
Kuo and Mallick indicator variable selection approach
\citep{kuo_mallick:1998} which
produces direct statements
of posterior model probabilities which we think are the most useful,
and leads directly to model-averaged estimates of density.  There is a
good review paper recently by \citet{ohara_sillanpaa:2009} that hits
on these and many more related ideas for variable selection.
 In addition to \citet{ohara_sillanpaa:2009} we
also recommend \citet[][Chapt. 7]{link_barker:2010} for general
information on model selection and assessment.

%  We have not done a comprehensive evaluation of different methods for
%  effect and efficiency.

\end{comment}

\begin{comment}
\subsection{Scope of the model selection problem}

There are two distinct classes of problems that we encounter in SCR
models which might require some type of model selection or ranking
effort: (1) Choosing among models that represent distinct, meaningful
biological hypotheses; and, (2) choosing among different parametric
encounter probability models. We believe that the importance of model
selection depends on which type of problem we have.

{\flushleft {\bf Choosing among biological models:}}
SCR models that represent extensions of the basic null model by
including specific covariates or other effects often represent
explicit biological hypotheses. Examples include models with a
behavioral response, or seasonal variation in encounter probability,
or sex-specificity of model parameters.
We anticipate that such basic biological factors
could be important, and therefore it can be useful to choose among (or
rank) a set of models that represent these hypotheses.

{\flushleft {\bf Choosing among models for encounter probability:}} In
Sec. \ref{scr0.sec.implied} we introduced the notion that encounter
probability models imply specific models of space usage, an idea we
expand on and generalize in Chapt. \ref{chapt.rsf}. Because of this
linkage between the model for encounter probability and space usage,
it is tempting to want to choose among the models believing them to be
biological models.  Our feeling is that
% XXXX Should we say "most" enc models are not biologiacal constructs?
% Otherwise it seems as though we are having it both
% ways. i.e. sometimes we think they are related to movement, and
% sometimes not. Also, Murray has a few models that allow for "sound
% attenuation due to spherical spreading" XXXX
the encounter probability
models are not biological constructs (not motived by biological
considerations) but, rather, purely phenomenological descriptions of
home range. Moreover, as the standard models are all stationary and
isotropic they are simply unrealistic models. Therefore, it seems to
us that choosing among a dozen or more arbitrary parametric forms that
have no biological motivation should tend to lead to an over-fitting.
So we will apply ideas of model selection to some problems below (and
elsewhere in this book) but we avoid the problem of choosing among
detections functions and we discourage people from doing that.
\end{comment}



\section{Model Selection by AIC}
\label{gof.sec.aic}

Using classical analysis based on likelihood, model selection
is easily accomplished using AIC \citep{burnham_anderson:2002}
which we demonstrate below. The AIC of a model is simply twice the
negative log-likelihood evaluated at the MLE, penalized by the number of parameters
($np$) in the model:
\[
 \mbox{AIC} = -2 \mbox{logL}(\hat{\bm \theta}|{\bf y})  + 2 np
\]
Models with small values of AIC are preferred.
It is common to use a modified (``corrected'') AIC referred to as $AIC_{c}$ for small
sample sizes which is
\[
 \mbox{AIC}_{c}  =
-2 \mbox{logL}(\hat{\bm \theta}|{\bf y})  + \frac{2 np
  (np+1)}{n-np-1}
\]
where $n$ is the sample size.  Two important problems with the use of
AIC and AIC$_{c}$ are that they don't apply directly to hierarchical
models that contain random effects, unless they are computed directly
from the marginal likelihood (for SCR models we can do this, see
Chapt. \ref{chapt.mle}). Moreover, it is not clear what should be the
effective sample size $n$ in calculation of AIC$_{c}$, as there can be
covariates that affect individuals, that vary over time, or space.
We do not offer strict guidelines as to when to use a small sample
size adjustment.

The {\bf R} package \mbox{\tt secr} computes and outputs AIC
automatically for each model fitted and it provides some capabilities
for producing a model selection table (function \mbox{\tt AIC}) and
also doing model-averaging (function \mbox{\tt model.average}), which
we recommend for obtaining estimates of density from multiple models.

\subsection{AIC analysis of the wolverine data}

We provide an example of model selection for the wolverine camera
trapping data using \mbox{\tt secr}.
 We consider a model set with  distinct models to accommodate
various types of sex specificity of model parameters:
\hspace{.5in} \begin{itemize}
\item[] Model 0: model SCR0 with constant density and constant
  encounter model parameters;

\item[] Model 1: model SCR0 with constant parameter
values for both male and female wolverines but with sex-specific
density only;

\item[] Model 2: Sex-specific density {\it and} sex-specific intercept
$p_{0}$ but constant $\sigma$;

\item[] Model 3: Sex-specific density, sex-specific $\sigma$ but constant
$p_{0}$

\item[] Model 4: Sex-specific density, sex-specific $p_{0}$ {\it and} $\sigma$.
\end{itemize}

To model sex-specific abundance (density), we  use the multi-session models  provided by
\mbox{\tt secr} (introduced in Sec. \ref{mle.sec.multisession}), which
allow one to model session-specific effects on density, baseline
encounter probability, $p_{0}$ (labeled $g_{0}$ in \mbox{\tt secr}), and also the scale
parameter $\sigma$ of the encounter probability model. Using this
formulation, we define the ``Session'' variable to be a {\it
  categorical} sex code (having value 1 or 2)
(demonstrated below) and
thus {\it session}-specific parameters represent {\it sex}-specific parameters.
For example, if we model session-specific density, $D$, then this
corresponds to Model 1 in our list above.  We note that ``Model 0'' in
our list corresponds to a model where all of the encounter
histories have the same session ID. This model is one of constant
density, which implies that the population sex ratio is fixed at 0.5, i.e.,
$\psi_{sex} = 0.5$. 

For fitting these models in {\bf BUGS} we use dummy variables. We
model  covariates on $p_{0}$ on
the logit-scale, and covariates on $\sigma$ on the
log-scale.
 Thus,  we will express  models allowing for sex-specificity
using a dummy variable \mbox{\tt Sex} and new parameters
($\alpha_{sex}$, $\beta_{sex}$) which
represent the {\it effect} of \mbox{\tt Sex} at level 1:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \alpha_{sex} \mbox{\tt Sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \beta_{sex} \mbox{\tt Sex}_{i}
\]
In these expressions, the sex variable $\mbox{\tt Sex}_{i}$ is a
binary variable where $\mbox{\tt Sex}_{i}= 0$ corresponds to female,
and $\mbox{\tt Sex}_{i} = 1$ corresponds to male. Although \mbox{\tt
  secr} also uses the logit/log linear predictors as the default for
modeling covariates on baseline encounter probability and the scale
parameter, respectively, \mbox{\tt secr} does something different with
the multi-session models. It reports estimates in a {\it session mean}
parameterization (equivalent to, in {\bf BUGS}, using an index
variable instead of a set of dummy variables), and not the {\it
  session effect} (i.e., deviation from the intercept) which arises
from the use of dummy variables.


To fit these models using \mbox{\tt secr}, 
we load the wolverine data and do 
a slight bit of formatting to prepare the data objects for analysis by
\mbox{\secr}. The key difference from our analysis in
Chapt. \ref{chapt.mle} is, here, we grab the wolverine sex information
(\mbox{\tt wolverine\$wsex}) (a binary 0/1 variable, 1=male) which we add
1 to so that we can define a categorical  ``Session'' variable (having
values 1 or 2). We also have a function \mbox{\tt scr2secr} which
converts a standard trap-deployment file (TDF) matrix into a \mbox{\tt secr}
object of class ``\mbox{\tt traps}.''
The {\bf R} commands are as follows (contained in the help file
\mbox{\tt ?secr\_wolverine}): 
% XXXX Should we try to force R code chunks to be on 1 page? If so,
% there is a "samepage" environement that does this, but it seems to
% occassionally create odd-looking spacing.
{\small
\begin{verbatim}

> library(secr)
> library(scrbook)
> data(wolverine)
> traps <- as.matrix(wolverine$wtraps)

## Name variables as required by secr
> dimnames(traps) <- list(NULL,c("trapID","x","y",paste("day",1:165,sep="")))
## Convert trap information to a secr "traps" object
> trapfile <- scr2secr(scrtraps=traps,type="proximity")

## Grab the wolverine state-space grid (2km here)
> gr <- as.matrix(wolverine$grid2)
> dimnames(gr) <- list(NULL,c("x","y"))
> gr2 <- read.mask(data=gr)

## Grab the encounter data, and re-name variables
> wolv.dat <- wolverine$wcaps
> dimnames(wolv.dat) <- list(NULL,c("Session","ID","Occasion","trapID"))

## Convert binary 0/1 sex variable to categorical 1/2 for "session"
> wolv.dat[,1] <- wolverine$wsex[wolv.dat[,2]]+1
> wolv.dat <- as.data.frame(wolv.dat)

## Convert to capthist object
> wolvcapt <- make.capthist(wolv.dat,trapfile,fmt="trapID",noccasions=165)
\end{verbatim}
}

Once the data have been prepared in this way, we use the
\mbox{\tt secr} model fitting function \mbox{\tt secr.fit} to fit the
different models, and then the function \mbox{\tt AIC} to
package the models together and summarize them in the form of an AIC
table, with rows of the table ordered from best to worst. The function
\mbox{\tt model.average} performs AIC-based model-averaging of the
parameters specified by the \mbox{\tt realnames} variable (below this
is demonstrated for the parameter density, $D$).  Because this
% XXXX Earlier, AIC was in math-mode. Should we stick with $AIC_c$?
function defaults to averaging by AIC$_c$, we slightly modified
this function (called \mbox{\tt model.average2}) to do model averaging
by either  AIC or AIC$_c$ as specified by the user. The model fitting
commands look like this (for Model 0 and Model 1):
{\small
\begin{alltt}
> model0 <- secr.fit(wolvcapt, model=list(D\(\sim\)1, g0\(\sim\)1, sigma\(\sim\)1), 
                  buffer=20000)
> model1 <- secr.fit(wolvcapt, model=list(D\(\sim\)session, g0\(\sim\)1, sigma\(\sim\)1), 
                  buffer=20000)
\end{alltt}
}
Next we use the function \mbox{\tt AIC}, passing the fit objects from
all 5 models, and that produces the following output (abbreviated
horizontally to fit on the page):
{\small
\begin{verbatim}
> AIC (model0,model1,model2,model3,model4)
            model         ... npar  logLik    AIC     AICc dAICc  AICwt
model0  D~1 g0~1 sigma~1  ...  3 -627.2603 1260.521 1261.932 0.000 0.5831
model2      ..            ...  5 -624.9051 1259.810 1263.810 1.878 0.2280
model1      ..            ...  4 -627.2365 1262.473 1264.973 3.041 0.1275
model4      ..            ...  6 -624.6632 1261.326 1267.326 5.394 0.0393
model3      ..            ...  5 -627.2358 1264.472 1268.472 6.540 0.0222
\end{verbatim}
}
Model averaging the results is done as follows:
{\small 
\begin{verbatim}
> model.average (model0,model1,model2,model3,model4,realnames="D")
              estimate  SE.estimate          lcl          ucl
session=1 2.707190e-05 7.913577e-06 1.544474e-05 4.745224e-05
session=2 2.927423e-05 8.270402e-06 1.700631e-05 5.039193e-05
\end{verbatim}
}
As usual, estimates and standard errors of the individual model
parameters can be obtained from the \mbox{\tt secr.fit} summary output
of any of the \mbox{\tt modelX} objects shown above.
The default output of estimated density is in individuals per ha, so
we have to scale this up to something more reasonable. To get into
units of per 1000 km$^2$, we need to first multiply by 100 to get to units of
km$^2$ and then multiply by 1000. This produces an estimated density of
about 2.71 for \mbox{\tt session=1} (females) and 2.93 for
\mbox{\tt session=2} (males).  We can use the generic {\bf R} function
\mbox{\tt predict} applied to the \mbox{\tt secr.fit} output to obtain
 specific information about the MLEs on the natural scale.

 We don't necessarily agree with the use of AIC$_c$ here and think its
 better to use AIC, in general. This is because, as noted previously,
 it is not clear what the effective sample size is for most
 capture-recapture problems. While we have 21 individuals in the data
 set, most of the model structure has to do with encounter probability
 samples and for that there are hundreds of observations. We do note
 that the AIC and AIC$_c$ results are not entirely consistent.  By
 looking at the best model by AIC (Table \ref{gof.tab.aic}), we find
 that the model with density varying by sex, and sex specific baseline
 encounter probability $p_{0}$, is preferred (Model 2). This is just
 slightly better than the model with no sex effect on density
 (implying a fixed sex ratio of $\psi_{sex} = 0.50$).


\begin{table}[ht]
\centering
\caption{
  Model selection results for the  wolverine models of sex-specificity,
  with/without habitat mask.  Fitting was done 
  using  \mbox{\tt secr} with a half-normal (Gaussian) encounter probability
  model. Models are ordered by
  $AIC$. Density, $D$, is
  reported in units of individuals per 1000 km$^2$. Model abbreviations
  indicate which parameters are sex-specific in order $D/p_{0}/\sigma$.
}
\begin{tabular}{lccccccccc}
\hline \hline
\multicolumn{10}{c}{NO HABITAT MASK} \\ \hline
        &      &     &      & \multicolumn{3}{c}{Female} & \multicolumn{3}{c}{Male} \\ 
  model & npar & AIC & AICc & D & $p_0$ & $\sigma$ & D & $p_0$ &  $\sigma$  \\ \hline
2: sex/sex/1    &  5&  1259.8& 1263.8 &2.45& 0.08& 6435.51& 3.16& 0.04& 6435.51\\
0: 1/1/1        &  3&  1260.5& 1261.9 &2.83& 0.06& 6298.66& 2.83& 0.06& 6298.66\\
4: sex/sex/sex  &  6&  1261.3& 1267.3 &2.59& 0.08& 6080.70& 2.99& 0.04& 6833.16\\
1: sex/1/1      &  4&  1262.5& 1265.0 &2.69& 0.06& 6298.69& 2.96& 0.06& 6298.69\\
3: sex/1/sex    &  5&  1264.5& 1268.5 &2.70& 0.06& 6280.49& 2.95& 0.06& 6319.03\\
\hline \hline
\multicolumn{10}{c}{WITH HABITAT MASK} \\ \hline
        &      &     &      & \multicolumn{3}{c}{Female} & \multicolumn{3}{c}{Male} \\ 
  model & npar & AIC & AICc & D & $p_0$ & $\sigma$ & D & $p_0$ &  $\sigma$ \\ \hline
2: sex/sex/1   &  5& 1268.1& 1272.1 &  3.64& 0.07& 6382.88& 4.73& 0.03& 6382.88 \\
4: sex/sex/sex &  6& 1268.7& 1274.7 &3.87& 0.07& 5859.40& 4.41& 0.03& 7039.09\\
0: 1/1/1       &  3& 1271.2& 1272.6 &4.18& 0.05& 6282.62& 4.18& 0.05& 6282.62\\
1: sex/1/1     &  4& 1273.1& 1275.6 &3.98& 0.05& 6282.65& 4.38& 0.05& 6282.65\\
3: sex/1/sex   &  5& 1275.1& 1279.1 &3.93& 0.05& 6357.26& 4.41& 0.05& 6220.22\\
\hline
\end{tabular}
\label{gof.tab.aic}
\end{table}


We fit the same models but now using a modified state-space which
excludes the ocean (this is a habitat mask in \mbox{\tt secr}).
 Results are shown in Table \ref{gof.tab.aic} along with
the previous models without a mask.  We see AIC values are smaller for
the model without the mask. It is probably acceptable to compare these
different fits (with and without habitat mask) by AIC because we
recognize the mask as having the effect of modifying the random
effects distribution (i.e., of the activity centers, ${\bf s}$) and
the results should be sensitive to choice of the distribution for
${\bf s}$. That said, we may not like the non-mask model because it
makes sense to exclude the water area from the state-space of ${\bf
  s}$.  For females the model-averaged density is 3.88 individuals per
1000 km$^2$ and for males the model-averaged density estimate is 4.46
individuals per 1000 km$^2$ as we see here:
{\small
\begin{verbatim}
> model.average (model0b,model1b,model2b,model3b,model4b,realnames="D")

              estimate  SE.estimate          lcl          ucl
session=1 3.876615e-05 1.189102e-05 2.153795e-05 6.977518e-05
session=2 4.459658e-05 1.323696e-05 2.523280e-05 7.882022e-05
\end{verbatim}
}
This is quite a bit higher than that based on the rectangular state-space
(i.e., not specifying a habitat mask). This is not surprising given
that {\bf the state-space is part of the model} and the specific
state-space modification we made here should be extremely important
from a biological standpoint.

\begin{comment}
\begin{verbatim}
             without mask
                              female?            male?
    model                   D    p0  sigma    D   p0   sigma
D(sex),g0(sex),\sigma      2.45 0.08 6435.51 3.16 0.04 6435.51
D,g0,\sigma                2.83 0.06 6298.66 2.83 0.06 6298.66
D(sex),g0,\sigma           2.69 0.06 6298.69 2.96 0.06 6298.69
D(sex),g0(sex),\sigma(sex) 2.59 0.08 6080.70 2.99 0.04 6833.16
D(sex),g0, \sigma(sex)     2.70 0.06 6280.49 2.95 0.06 6319.03


             with mask
                              female?            male?
    model                   D    p0  sigma    D   p0   sigma
D(sex),g0(sex),\sigma      3.64 0.07 6382.88 4.73 0.03 6382.88
D(sex),g0(sex)\sigma(sex)  3.87 0.07 5859.40 4.41 0.03 7039.09
D, g0, \sigma              4.18 0.05 6282.62 4.18 0.05 6282.62
D(sex),g0,\sigma           3.98 0.05 6282.65 4.38 0.05 6282.65
D(sex),g0,\sigma(sex)      3.93 0.05 6357.26 4.41 0.05 6220.22
\end{verbatim}
\end{comment}




\section{Bayesian Model Selection}

Model selection is somewhat less straightforward as a Bayesian, and
there is no canned all-purpose method like AIC. As such we
recommend a pragmatic approach, in general, for all problems,
based on a number of basic considerations:
% XXXX Somewhere I think we should mention cross-validation and the
% papers of Stone, which compare AIC with x-validation. I realize that
% there a variety of different discrepancy stats that one could use,
% but I still think this is probably the gold-standard in terms of
% model selection... if you want a model that is good for prediction.
\begin{itemize}
\item[(1)] For a small number of fixed effects we think it is
  reasonable to adopt a conventional ``hypothesis testing'' approach
  -- i.e., if the posterior for a parameter overlaps zero
  substantially, then it is probably reasonable to discard that
  effect from the model.
\item[(2)] Calculation of posterior model probabilities: In some cases
  we can implement methods which allow calculation of posterior model
  probabilities. One such idea is the indicator variable selection
  method from \citet{kuo_mallick:1998}.  For this, we introduce a latent
  variable $w \sim \mbox{Bern}(.5)$ and expand the model to include
  the variable $w$ as follows:
\[
 \mbox{logit}(p_{ijk}) = \alpha_{0} + w*\alpha_{1}*C_{ijk}.
\]
The importance of the covariate $C$ is then measured by the posterior
probability that $w=1$.
\item[(3)] DIC -- the Deviance Information Criterion: Bayesian model
  selection is now routinely carried out using the Deviance
  Information Criterion (DIC; \citet{spiegelhalter_etal:2002})
  although its
  effectiveness in hierarchical models depends very much on the manner
  in which it is constructed \citep{millar:2009}.  We recommend using
  it if it leads to sensible results, but we think it should be
  calibrated to the extent possible for specific classes of models.
  This has not yet been done in the literature for SCR models, to our knowledge.
\item[(4)] Logical argument: For something like sex-specificity of
  certain parameters, it seems to make sense to leave an extra
  parameter in the model no matter what because, biologically, we might
 expect a difference (e.g., home range size).
In some cases failure to apply logical argument leads to
  meaningless tests of gratuitous hypotheses \citep{johnson:1999}.
\end{itemize}
In all modeling activities, as in life itself, the use of logical argument should not be under-utilized.

\subsection{Deviance information criterion (DIC) }

The availability of AIC makes the use of likelihood methods convenient
for problems where likelihood estimation is achievable.  For Bayesian
analysis, the deviance information criterion (DIC) seemed like a
general-purpose equivalent, at least for a brief period of time after
its invention.  However, there seem to be many variations of DIC, and
a consistent version is not always reported across computing
platforms.
%Our own experience with
%calibration has indicated highly variable effectiveness of DIC.
Even statisticians don't have general agreement on practical issues
related to the use of DIC \citep{millar:2009}.
% XXXX Martyn Plummer has a paper or two worth citing here
Despite this, it is
still widely reported. We think DIC is probably reasonable for certain
classes of models that contain only fixed effects, or for which the
latent variable structure is the same across models so that only the
fixed effects are varied (this covers many SCR model selection
problems).  However, it would be useful to see some calibration of DIC
for some standardized model selection problems.

Model deviance is defined as negative twice the log-likelihood;
i.e., for a given model with parameters $\theta$: $\mbox{Dev}(\theta) =
-2*\mbox{logL}(\theta|{\bf y})$.  The DIC is defined as the
posterior mean of the deviance, $\overline{\mbox{Dev}}(\theta)$, plus a measure of model complexity,
$p_{D}$:
\[
 \mbox{DIC} = \overline{\mbox{Dev}}(\theta) + p_{D}
\]
The standard definition of $p_{D}$ is
\[
 p_{D} = \overline{\mbox{Dev}}(\theta) - \mbox{Dev}(\bar{\theta})
\]
where the 2nd term is the deviance evaluated at the posterior mean of
the model parameter(s), $\bar{\theta}$. The $p_{D}$ here is interpreted as the effective
number of parameters in the model.  \citet{gelman_etal:2004} suggest a
different version of $p_{D}$ based on one-half the posterior variance
of the deviance:
\[
 p_{V} = \mbox{Var}(\mbox{Dev}(\theta)|{\bf y})/2.
\]
% XXXX From what I have read, there isn't much theory to justify
% Gelman's method.
This is what is produced from {\bf WinBUGS} and {\bf JAGS} if they are
run from \mbox{\tt R2WinBUGS} or \mbox{\tt R2jags}, respectively.  It
is less easy to get DIC summaries from \mbox{\tt rjags}, so we have
used \mbox{\tt R2jags} in our analyses below.


\subsection{DIC analysis of the wolverine data}

We repeated the analysis of the wolverine models with sex-specificity,
but this time doing a Bayesian analysis paralleling the likelihood
analysis we did above in \mbox{\tt secr}, using the logit/log
parameterization of the model parameters:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \alpha_{sex} \mbox{\tt Sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \beta_{sex} \mbox{\tt Sex}_{i}
\]

Unlike the multi-session model in \mbox{\tt secr}, we carry out the
analysis of the sex-specific model here by putting all of the data
into a single data set, and explicitly accounting for the covariate
'sex' in the model by assigning it a Bernoulli prior distribution with
$\psi_{sex}$ being the proportion of males in the population. In this
case, we produce ``Model 0'' above, the model with no sex effect on
density, by setting the population proportion of males at one-half: $\psi_{sex} = 0.5$
(see also Sec. \ref{covariates.sec.sex}).
%This parallels our treatment of
%the ovenbird data in Sec. \ref{poisson-mn.sec.ovenbird} (see also
%Sec. \ref{covariates.sec.sex}).  
As usual, handling of missing values
of the sex variable is done seamlessly which might be a practical
advantage of Bayesian analysis in situations where sex is difficult to
record in the field which may lead to individuals of unknown sex
(i.e., missing values).  

The {\bf BUGS} model specification for the
most complex model, Model 4, is shown in Panel
\ref{gof.panel.sexmodel}.  This model has sex-specific intercept and
scale parameter, $\sigma$.  We provide an {\bf R} script named
\mbox{\tt wolvSCR0ms} in the \mbox{\tt scrbook} package which will fit
each model.  The function uses {\bf JAGS} by default for the fitting,
using the \mbox{\tt R2jags} package.  The kernel of this function is
the model specification in Panel \ref{gof.panel.sexmodel}, which gets
modified depending on the model we wish to fit using a command line
option \mbox{\tt model}. For example, \mbox{\tt model = 1} fits the
model with constant parameter values for males and females, but
sex-specific population sizes (\mbox{\tt model = 0} constrains the
male probability parameter, $\psi_{sex}$, to be $0.5$).  The {\bf R} function
fits each of the 5 models using a binary indicator variable to turn
`on' or `off' each effect.  Here is how we obtain the MCMC output for
each of the 5 models: {\small
\begin{verbatim}
> toad0 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=0)
> toad1 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=1)
> toad2 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=2)
> toad3 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=3)
> toad4 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=4)
\end{verbatim}
}



\begin{panel}[tp]
\centering
\rule[0.15in]{\textwidth}{.03in}
%\begin{minipage}{5in}
{\small
\begin{alltt}
alpha.sex \(\sim\) dunif(-3,3)            ## Prior distributions 
beta.sex  \(\sim\) dunif(-3,3)
sigma0 \(\sim\) dunif(0,50)
alpha0 \(\sim\) dnorm(0,.1)
psi \(\sim\) dunif(0,1)                   ## Data augmentation parameter
psi.sex  \(\sim\) dunif(0,1)              ## Probability of ``male''

for(i in 1:M)\{                      ## DA loop
  wsex[i] \(\sim\) dbern(psi.sex)         ## Latent sex state (male = 1)
  z[i] \(\sim\) dbern(psi)                ## DA variables
  s[i,1] \(\sim\) dunif(Xl,Xu)
  s[i,2] \(\sim\) dunif(Yl,Yu)
  logit(p0[i]) <- alpha0 + alpha.sex*wsex[i]
  log(sigma.vec[i]) <- log(sigma0) + beta.sex*wsex[i]
  alpha1[i] <- 1/(2*sigma.vec[i]*sigma.vec[i])
  for(j in 1:ntraps)\{
    mu[i,j] <- z[i]*p[i,j]
    y[i,j] \(\sim\) dbin(mu[i,j],K[j])
    dd[i,j] <- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2)
    p[i,j]  <-  p0[i]*exp( - alpha1[i]*dd[i,j] )
   \}
 \}
\end{alltt}
}
%\end{minipage}
\rule[-0.15in]{\textwidth}{.03in}
\caption{
Part of the {\bf BUGS} specification for a complete sex-specificity of model
parameters. This is a simplified version of the model contained in the 
\mbox{\tt wolvSCR0ms} script, because it does not contain the on/off
switches for creating the various sub-models. 
}
\label{gof.panel.sexmodel}
\end{panel}


We fitted the 5 models to the wolverine data and summarize
the DIC computation results in Table \ref{gof.tab.DIC}. 
The model rank has model 0, model 2, model 1, model 4, model 3.
Interestingly, this is the same order as the models based on AIC$_c$
which we found above
(see Table \ref{gof.tab.aic}).
The posterior mean and SD of model parameters under the 5 models are
given in Table \ref{gof.tab.dic}. 

\begin{table}[ht]
%%%%%XXX 01/17/2013 -- updated 
\centering
\caption{
DIC results for the 5 models of sex-specificity fitted to the
wolverine camera trapping data, using the function
\mbox{\tt wolvSCR0ms}. Results are based on 3 chains of length 61000
yielding 180000 posterior samples. 
}
\begin{tabular}{ccccc} \hline \hline
      &  meandev  &  pd   &    DIC  &   rank \\ \hline
Model 0&  441.01 & 77.09&518.10&    1 \\
Model 1& 441.78 &77.504 &519.28&    3\\
Model 2& 440.12 &78.440 &518.56&    2\\
Model 3& 443.31 &79.478 &522.79&    5\\
Model 4& 441.24 &80.078 &521.32&    4\\
\end{tabular}
\label{gof.tab.DIC}
\end{table}

\begin{comment}
\begin{verbatim}
DIC info (using the rule, pD = var(deviance)/2)
pD = 80.1 and DIC = 521.3
DIC is an estimate of expected predictive error (lower deviance is better).
            mean    sd   mean    sd   mean    sd   mean    sd   mean    sd
D           5.79  1.15   5.81  1.15   5.72  1.15   5.75  1.15   5.66  1.13
N          60.02 11.91  60.24 11.93  59.37 11.97  59.67 11.97  58.77 11.75
alpha0     -2.81  0.18  -2.82  0.17  -2.44  0.25  -2.82  0.18  -2.43  0.25
alpha.sex   0.00  1.73   0.00  1.73  -0.75  0.34   0.00  1.73  -0.79  0.36
%%beta        1.25  0.21   1.26  0.21   1.19  0.21   1.24  0.29   1.30  0.32
beta.sex    0.00  1.73  -0.01  1.73   0.01  1.74  -0.01  0.17   0.10  0.18
sigma0       0.64  0.06   0.64  0.05   0.66  0.06   0.65  0.08   0.63  0.09
psi         0.30  0.07   0.30  0.07   0.30  0.07   0.30  0.07   0.30  0.07
psi.sex     0.50  0.29   0.52  0.10   0.56  0.10   0.52  0.11   0.54  0.11
deviance  441.01 12.42 441.78 12.45 440.12 12.53 443.31 12.61 441.24 12.66
\end{verbatim}
\end{comment}

\begin{table}[ht]
\centering
\caption{
Posterior summaries of model parameters for models with varying
sex-specificity of model parameters. Model 0 = no sex specificity,
model 4 = fully sex specific (see text). Models are based on the 
 Gaussian encounter probability model), each with 21000 iterations,
 1000 burn-in, 3 chains for a total of 60000 posterior samples. }
{\tiny
\begin{tabular}{crrrrrrrrrr} \hline \hline
parameter & \multicolumn{2}{c}{model 0} &
\multicolumn{2}{c}{model 1} &
\multicolumn{2}{c}{model 2} &
\multicolumn{2}{c}{model 3} &
\multicolumn{2}{c}{model 4}  \\
          &    Mean &   SD &      Mean &   SD &      Mean &   SD &
          Mean&     SD  & Mean & SD \\ \hline
$N$       &  60.02& 11.91&  60.24& 11.93&  59.37& 11.97&  59.67& 11.97&  58.77& 11.75\\
$D$       &   5.79&  1.15&   5.81&  1.15&   5.72&  1.15&   5.75&  1.15&   5.66&  1.13\\
$\alpha_0$&  -2.81&  0.18&  -2.82&  0.17&  -2.44&  0.25&  -2.82&  0.18&  -2.43&  0.25\\
$\alpha_{sex}$ &   0.00&  1.73&   0.00&  1.73&  -0.75&  0.34&   0.00&  1.73&  -0.79&  0.36\\
%%%$\beta$        1.25&  0.21&   1.26&  0.21&   1.19&  0.21&   1.24&  0.29&   1.30&  0.32\\
$\sigma_0$    &  0.64&  0.06&   0.64&  0.05&   0.66&  0.06&   0.65&  0.08&   0.63&  0.09\\
$\beta_{sex} $ &  0.00&  1.73&  -0.01&  1.73&   0.01&  1.74&  -0.01&  0.17&   0.10&  0.18\\
$\psi$         &0.30&  0.07&   0.30&  0.07&   0.30&  0.07&   0.30&  0.07&   0.30&  0.07\\
$\psi_{sex}$    & 0.50&  0.29&   0.52&  0.10&   0.56&  0.10&   0.52&  0.11&   0.54&  0.11\\
deviance   &441.01& 12.42& 441.78& 12.45& 440.12& 12.53& 443.31& 12.61&441.24& 12.66 \\
%\end{tabular}
%\begin{tabular}{crrrrrrrrrr}
& \multicolumn{2}{c}{pD = 77.1} &\multicolumn{2}{c}{pD = 77.5} & \multicolumn{2}{c}{pD = 78.4}& \multicolumn{2}{c}{pD =79.5}  & \multicolumn{2}{c}{pD =80.1}  \\
& \multicolumn{2}{c}{DIC = 518.1} & \multicolumn{2}{c}{DIC = 519.3} &\multicolumn{2}{c}{DIC = 518.6} &    \multicolumn{2}{c}{DIC = 522.8} &\multicolumn{2}{c}{DIC = 521.3} \\ \hline
\end{tabular}
}
%\hline
\label{gof.tab.dic}
\end{table}





\begin{comment}
\begin{verbatim}
          beta param                  unif -3, 3 prior  dnorm(0,.1) prior
                                     sigma param        sigma param
           dev  np     dic         dev    np     dic    dev    np     dic
Model 1  441.78 78.3  520.0       441.73 78.1   519.8  441.74  77.5 519.2
Model 2  440.97 82.4  523.4       439.81 78.3   518.1  440.38  79.5 519.9
Model 3: 443.68 80.9  524.6       443.29 79.9   523.2  443.18  79.3 522.5
Model 4: 441.07 77.3  518.3       441.27 81.1   522.4  441.69  80.6 522.3

beta param:   4 1 2 3
sigmaparma1   2 1 4 3
sigmaparm2    1 2 4 3

dev     2  4  1  3 only
DIC1:   4, 1, 2, 3
DIC2:   2, 1, 4, 3

AIC:    2, 4, 1, 3
AICc    2, 1, 4, 3
AICmask 4  2  3  1
AICcmsk 2  4  1  3
\end{verbatim}
\end{comment}













\begin{comment}
\subsubsection{Checking some things}

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
check priors
A final point: We have dunif(-3,3) priors. Check normal prior and
check dunif(-5,5).

status: running dunif(0,.1)
[see results above and model weights below]

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Check WinBUGS too.  Got this job queued up ....
RESULT: Pretty similar overall and in terms of DIC. Probably things
are working right.
model selection results:
a
   00    01    10    11
22599  1328 33535  2538
> table(a)/length(a)
a
        00         01         10         11
0.37665000 0.02213333 0.55891667 0.04230000


XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\end{comment}



\begin{comment}
As is pointed out in one of those posts, it's interesting to me that
both AIC and DIC are attempts at mimicking leave-one-out
cross-validation. Perhaps we could make the point that if DIC cannot
be trusted then one could always implement their own x-val
procedure... assuming it doesn't take months to complete.

Richard

On Thu, May 24, 2012 at 9:57 AM, Jeffrey Royle <jaroyle@gmail.com> wrote:
> hi all -- DIC is a hopeless black hole with, I think, no real possibility of
> making it seem reasonable to people.
> There is some good discussion on Gelman's blog, along with a number of
> papers (all cited there)
> http://andrewgelman.com/2006/07/number_of_param/
> http://andrewgelman.com/2011/06/plummer_on_dic/
> http://andrewgelman.com/2011/06/deviance_dic_ai/
>
> I gather that there are 3 version of DIC. The original one, and then
> Plummer's version which seems to be in JAGS and then the dumb version based
> on .5*var(deviance) which, I gather, you should never use -- I guess it
> sucks.
>
> I still can't compute Plummer's version using JAGS -- I think maybe I need
> to update my JAGS version or something.
\end{comment}





\subsection{Bayesian model averaging with indicator variables}

A convenient way to deal with model selection and averaging problems
in Bayesian analysis by MCMC is to use the method of model indicator
variables \citep{kuo_mallick:1998}. Using this approach, we expand the
model to include a set of prescribed models as specific reductions of
a larger model.  This has been demonstrated in some specific
capture-recapture models in \citet[][Sec. 3.4.3]{royle_dorazio:2008},
and \citet{royle:2009} and in the context of SCR by
\citet{tobler_etal:2012}.  A useful aspect of this method is that
model-averaged parameters are produced by default. We emphasize the
need to be careful of reporting model-averaged parameters that don't
have a common interpretation in the different models because they are
meaningless (averaging apples and oranges....).
For example, if a regression parameter is in a specific
model then the posterior is informed by the data and a specific MCMC
draw is from the appropriate posterior distribution. On the other
hand, if the regression parameter is not in the model then the MCMC
draw is obtained directly from the prior distribution, and so we need
to think carefully about whether it makes sense to report an average
of such a thing (in the vast majority of cases the answer is no). But
some parameters like $N$ or density, $D$, do have a consistent
interpretation and we support producing model-averaged results of
those parameters. % XXXX Model averaging is therefore a useful endeavor in
% spatial capture-recapture problems.

To implement the Kuo and Mallick approach, we expand the model to
include the latent indicator variables, say $w_{m}$, for variable $m$
in the model, such that
% XXXX Some of this was stated earlier. Suggest removing the details
% from above, and explaining them here.
\begin{eqnarray*}
w_{m} = \left\{
\begin{array}{cl} 1 &  \mbox{ linear predictor includes  covariate $m$} \\
                  0 &  \mbox{ linear predictor does not
                                include covariate $m$}
 \end{array}
\right.
\end{eqnarray*}
We assume that the indicator variables $w_{m}$ are mutually
independent with
\[
w_m \sim \mbox{Bernoulli}(0.5)
\]
for each variable $m=1,2,\ldots,$ in the model.
The expanded model has the linear predictor, e.g., for 2 variables:
\[
\mbox{logit}(p_{ijk}) = \alpha_{0} + \alpha_{1}w_{1} C_{1,i} + \alpha_{2}w_{2} C_{2,ijk}
\]
where, let's suppose, $C_{1,i}$ is an individual covariate such
as sex, and $C_{2,ijk}$ is a behavioral response covariate which is
individual-, trap-, and occasion-specific.  We can assume a parallel
model specification on the parameter $\sigma$ which is liable to vary
by individual level covariates such as sex:
\[
 \log(\sigma_{i}) = \beta_{0} + \beta_{1} w_{3} C_{1,i}
\]

Using this indicator variable formulation of the model selection
problem we can characterize unique models by the sequence of $w$
variables. In this case, each unique sequence $(w_{1},w_{2},w_{3})$
represents a model, and we can tabulate the posterior frequencies of
each model by post-processing the MCMC histories of
$(w_{1},w_{2},w_{3})$, as we demonstrate shortly.

Conceptually, analysis of this expanded model within the data
augmentation framework does not pose any additional difficulty. One
broader, technical consideration is that posterior model probabilities
are well known to be sensitive to priors on parameters
\citep{aitkin:1991, link_barker:2006}. See also 
\citet[][Sec. 3.4.3]{royle_dorazio:2008} and
\citet[][Sec. 7.2.5]{link_barker:2010}.  What might normally be viewed
as vague or non-informative priors, are not usually innocuous or
uninformative when evaluating posterior model probabilities. The use
of AIC seems to avoid this problem largely by imposing a specific and
perhaps undesirable prior that is a function of the sample size
\citep{kadane_lazar:2004}. One solution is to compute posterior model
probabilities under a model in which the prior for parameters is fixed
at the posterior distribution under the full model
\citep{aitkin:1991}. At a minimum, one should evaluate the sensitivity
of posterior model probabilities to different prior specifications.
%It was recently suggested to us by W.A. Link (see
%\citet{link_barker:2012inpress}) that a reasonably general solution to
%the ``prior sensitivity'' issue can be achieved by using a two-stage
%prior construction in which, when a parameter is not in the model, a
%different prior distribution is used, and one that is reasonably we


\subsubsection{Analysis of the wolverine data}

The {\bf R} script \mbox{\tt wolvSCR0ms} in \mbox{\tt scrbook}
provides the model indicator variable implementation for the fully
sex-specific SCR model.  It is run by setting \mbox{\tt model=5} in
the function call. We note again that it is not very useful to report most
parameter estimates from this model because their marginal posterior
is a mixture from the prior (when a value of the indicator variable of
0 is sampled) and draws informed by the data (i.e., from the
posterior, when a 1 is drawn for the indicator variable $w$).
 On the other hand, the parameters
$N$ and density $D$ should be reported and they represent marginal
posteriors over all models in the model set. In effect, model
averaging is done as part of the MCMC sampling.  The variable `mod'
contains the two binary indicator variables ($w$ above) which
pre-multiply the 'sex' term in each of the $p_{0}$ and $\sigma$ model
components, like this:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \mbox{\tt mod}[1] \alpha_{sex} \mbox{\tt sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \mbox{\tt mod}[2] \beta_{sex} \mbox{\tt sex}_{i}
\]
The third element of \mbox{\tt mod} determines whether the
$\psi_{sex}$ parameter is estimated or fixed at $\psi_{sex} =
0.5$ which is accomplished with the line of {\bf BUGS} code as
follows:
\newline 
\mbox{\tt sex.ratio <- psi.sex*mod[3] + .5*(1-mod[3])}.
The MCMC output for `mod' was post-processed to obtain the
model-weights using the following  {\bf R} commands:
\begin{verbatim}

>  mod <- toad5$BUGSoutput$sims.list$mod
>  mod <- paste(mod[,1],mod[,2],mod[,3],sep="")
>
>  table(mod)
mod
  000   001   010   011   100   101   110   111
17181  4935  1057   296 25211  8337  2275   708

> round( table(mod)/length(mod) , 3)
mod
  000   001   010   011   100   101   110   111
0.286 0.082 0.018 0.005 0.420 0.139 0.038 0.012

\end{verbatim}
We see that the best model is that labeled \mbox{\tt 100} which,
according to our construction above, has \mbox{\tt mod[1]=1},
\mbox{\tt mod[2]=0} and \mbox{\tt mod[3]=0}. This is the model 
having sex-specific baseline
encounter probability $p_{0}$, and $\psi_{sex} = 0.5$. This model has 
posterior model probability $0.420$. The model with no sex-specificity
at all (the model with label \mbox{\tt 000}) has
posterior probability $0.286$ and the remaining posterior mass is
distributed over the other six models. We could arrive at a
qualitatively similar conclusion using a more ad hoc approach based
on looking at the posterior mass for each parameter under the
full model (model 4; see Table \ref{gof.tab.dic}, in part). Considering
the sex-specific intercept, it appears to be very important as its
posterior mass is mostly away from 0.  On the other hand, the
coefficient on log-sigma is concentrated around 0, and the estimated
$\psi_{sex}$ (probability that an individual is a male) is $0.54$ with
a large posterior standard deviation.  We might therefore be inclined
to discard the sex effect on $\log(\sigma)$ based on classical
thinking-like-a-hypothesis-testing-guy and settle for the model with a
sex-specific intercept in the encounter probability model. This is consistent with our indicator variable
approach which found that model (1,0,0) has posterior probability of
0.420. So we're not really misled too much in looking at the
posteriors for each parameter to thin the model down.  We can obtain model-averaged estimates
from the indicator variable approach, which produces direct
model-averaged estimates of $N$ and $D$:
% done 12/12/2012
{\small
\begin{verbatim}
   mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
D    5.695   1.133   3.759   4.916   5.591   6.362   8.193 1.002  3600
N   59.077  11.758  39.000  51.000  58.000  66.000  85.000 1.002  3600
\end{verbatim}
}
We obtain a model-averaged estimate (posterior mean) for density of $D=5.695$
which is hardly any different from our
model specific estimates (Table \ref{gof.tab.dic}) and, in particular, from model 2
which has only a sex-specific intercept.



\begin{comment}
XXXX MIGHT BE GOOD TO HAVE THIS BUT OPTIONAL XXXXXXXXXXX
\subsection{Sensitivity to prior distributions}

{\bf XXXXXXXXXXX TO BE DONE XXXXXXXXXX}

Discussion of sensitivity to prior ......
Results of DIC and model indicator variable analyses were based on
unif(-3,3) priors for the parameters. This keeps them in the ballpark
when they are not in the model. We tried a different prior:
normal(0,.1) which you can do just by editing the function
 \mbox{\tt wolvSCR0ms}.
Doing the DIC analysis changes the result to XXXXXXXXXXXXX

The indicator variable analysis was rerun and that produces XXXXXXXXXXXXXXX
We modified
 the {\bf R} script to run the same model
but with \mbox{\tt dnorm(0,.1)} prior distributions, producing the
following results:  {\bf XXXXX RERUN THIS XXXXXXX}
\begin{verbatim}
> mod <- toad5$BUGSoutput$sims.list$mod
> mod <- paste(mod[,1],mod[,2])
> table(mod)/length(mod)

mod
       0 0        0 1        1 0        1 1
0.43945000 0.02185000 0.50708333 0.03161667
\end{verbatim}
\end{comment}

\subsection{Choosing among detection functions}


Another approach to implementing model indicator variables is to
introduce a categorical ``model identity'' variable which is itself
 a parameter of the model. Using this approach, then each 
distinct model is associated with a unique set of covariates or
other set of model features. This is convenient especially when we
cannot specify the linear predictor as some general model that reduces
to various alternative sub-models simply by switching binary variables
on or off. In the context of SCR models, choosing among different
encounter probability models would be an example.  For this case we do
something like this \mbox{\tt mod  $\sim$  dcat(probs[])}
where \mbox{\tt probs} is a vector with elements $1/(\# models)$, and
the encounter probability matrix is filled in depending on the value
 of \mbox{\tt mod}.
In particular, instead of a 2-dimensional array
 \mbox{\tt p[i,j]},  we build \mbox{\tt p[i,j,m]} for each of
$m=1,2,\ldots,M$ models. An example with 3 distinct models is:
{\small
\begin{alltt}
  mod  \(\sim\) dcat(probs[])
\#\#
\#\# Using a double loop construction fill-in p[,,] for each model:
\#\#
  p[i,j,1] <-  p0[1]*exp( - alpha1[1]*dist2[i,j] )
  p[i,j,2] <-  1-exp(-p0[2]*exp( - alpha1[2]*dist2[i,j] ) )
  logit(p[i,j,3]) <- p0[3] - alpha1[3]*dist2[i,j]

  mu[i,j] <- z[i]*p[i,j,mod]
  y[i,j] \(\sim\) dbin(mu[i,j],K[j])
\end{alltt}
}
% XXXX Should you show mod ~ dcat(probs[]) in the code above?
% XXXX Isn't this reversible-jump MCMC? We should probably use that
% term (and cite Green, and possibly R. King) or else some num-nut
% like Gelfand will say: "the authors
% apparently did not know about RJMCMC.
%% my sense is that it is not RJMCMC but, rather, plain vanilla gibbs
%% sampling or MH-within-Gibbs


As before the posterior probabilities can be highly sensitive to
priors on the different model parameters and sometimes mixing is
really poor and, in general, we've experienced mixed success trying to
carry out model selection using this construction.
We do provide a template {\bf R}/{\bf JAGS} script (\mbox{\tt
  wolvSCR0ms2}) in the \mbox{\tt scrbook} package which has an
example of choosing among 3 different encounter probability
models:
% shown in Panel
%\ref{gof.panel.modsel}:
The Gaussian encounter probability, Gaussian hazard, and logistic
model with the square of distance (defined in Sec. \ref{scr0.sec.binomial}). The key things to note are that
there are 3 intercepts and 3 different `\mbox{\tt alpha1}' parameters
(the coefficient on distance). The parameters should not be regarded
as equivalent across the models, so it is important to have them
separately defined (and estimated) for each model.  In our analysis we
used a vague normal prior (precision = 0.1) for the intercept
parameter (either log or logit-scale of baseline encounter probability
$p_{0}$) and a \mbox{\tt Uniform}(0,5) prior for one-half the inverse of the
coefficient on distance-squared.
In the {\bf BUGS} model specification the priors look like this:
\begin{alltt}
 for(i in 1:3)\{
   alpha0[i] \(\sim\) dnorm(0,.1)
   sigma[i] \(\sim\) dunif(0,5)
   alpha1[i] <- 1/(2*sigma[i]*sigma[i])
 \}
\end{alltt}
Then,  we create a probability of encounter for each
individual, trap {\it and} model so that the holder object ``\mbox{\tt p}'' in the
model description is a 3-dimensional array (sometimes this would have to be a 4
or 5-d array in more complex models with time effects, etc..), so that
construction of the encounter probability models look like this:
\begin{verbatim}
 p[i,j,1]       <-  p0[1]*exp( - alpha1[1]*dist2[i,j] )
 p[i,j,2]       <-  1-exp(-p0[2]*exp( - alpha1[2]*dist2[i,j] ) )
 logit(p[i,j,3])<- p0[3] - alpha1[3]*dist2[i,j]
\end{verbatim}
where
\begin{verbatim}
 logit(p0[1]) <- alpha0[1]
   log(p0[2]) <- alpha0[2]
       p0[3]  <- alpha0[3]
\end{verbatim}
You can experiment with the \mbox{\tt wolvSCR0ms2} script to
investigate the importance of different models of encounter
probability and whether they have an affect on the inferences.
%Results of fitting the multiple-encounter probability model to the
%wolverine camera trapping data are summarized as follows:









\begin{comment}

{\bf XXXX OLD VERSION. CHANGED PRIOR SPECIFICATION XXXXX}

\begin{panel}[htp]
\centering
\rule[0.15in]{\textwidth}{.03in}
%\begin{minipage}{5in}
{\small
\begin{verbatim}
model {
for(i in 1:3){
  alpha0[i] ~ dnorm(0,.1)
  alpha1[i] ~ dunif(0,2)
  mod.probs[i] <- 1/3
}
psi ~ dunif(0,1)
psi.sex  ~ dunif(0,1)
catmod~dcat(mod.probs[])

for(i in 1:M){
 w[i]~dbern(psi)
 s[i,1]~dunif(Xl,Xu)
 s[i,2]~dunif(Yl,Yu)
 logit(beta0.vec[i,1])<- alpha0[1]
 log(beta0.vec[i,2])<- alpha0[2]
 beta0.vec[i,3]<- alpha0[3]

 log(alpha1.vec[i,1])<- log( alpha1[1] )
 log(alpha1.vec[i,2])<- log( alpha1[2] )
 log(alpha1.vec[i,3])<- log( alpha1[3] )

 for(j in 1:ntraps){
   dd[i,j]<- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2)

   p[i,j,1]  <-  beta0.vec[i,1]*exp( - alpha1.vec[i,1]*dd[i,j] )
   p[i,j,2] <-  1-exp(-beta0.vec[i,2]*exp( - alpha1.vec[i,2]*dd[i,j] ) )
   logit(p[i,j,3])<- beta0.vec[i,3] - alpha1.vec[i,3]*dd[i,j]

   mu[i,j]<-w[i]*p[i,j,catmod]
   ncaps[i,j]~ dbin(mu[i,j],K[j])
  }
}
N<-sum(w[1:M])
D<-N/area
}
\end{verbatim}
}
%\end{minipage}
\rule[-0.15in]{\textwidth}{.03in}
\caption{
Part of the BUGS specification
of the indicator variable idea to choose among
  different detection models. A template {\bf R} script that fits this model
  to the wolverine data is called \mbox{\tt wolvSCR0ms2}.
}
\label{gof.panel.modsel}
\end{panel}
\end{comment}



\begin{comment}

{\bf XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX IF TIME PERMITS I WILL ADD
  FURTHER ANALYSIS OF THE WOLVERINE DATA SET USING A BEHAVIORAL
  RESPONSE MODEL..... AND REPEATE THE AIC/DIC/MODEL WEIGHTS ANALYSIS
  GIVEN ABOVE XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX}


\subsection{Further analysis of the wolverine data}



We did a bunch of analysis previously with models that involved
sex-specific parameters. Here we expand the model set to include a
behavioral response. This is a little more difficult doing Bayesian
analysis because we have to do the 3-d version of the model which can
be a time-consuming task in WinBUGS. But lets do it anyway.
There are in this case 8 models (right?)


4 models with sex: DIC, model weights, AIC.

expanded model with behavioral response...... DIC , model weights, AIC......

\begin{verbatim}
Node statistics
      node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.894	1.009	0.0171	4.18	5.822	8.062	1001	6600
	N	39.48	6.755	0.1145	28.0	39.0	54.0	1001	6600
	X1new	8.996	2.711	0.03361	4.551	8.695	15.15	1001	6600
	X1obs	11.87	3.354	0.05475	6.382	11.49	19.53	1001	6600
	X3new	13.15	3.232	0.03914	7.725	12.92	20.4	1001	6600
	X3obs	21.43	2.115	0.03256	17.8	21.24	26.07	1001	6600
	Xnew	61.78	6.517	0.1004	49.48	61.7	75.22	1001	6600
	Xobs	88.97	5.972	0.08912	78.08	88.63	101.5	1001	6600
	alpha.sex	-0.4506	1.158	0.02228	-2.571	-0.6846	2.594	1001	6600
	beta	2.423	4.111	0.3034	0.06847	1.165	17.51	1001	6600
	beta.sex	0.07415	1.761	0.1045	-2.85	0.0155	2.868	1001	6600
	deviance	439.3	12.16	0.2192	418.0	438.4	465.5	1001	6600
	logitp0	-2.577	0.2909	0.0114	-3.079	-2.597	-1.979	1001	6600
	mod[1]	0.6236	0.4845	0.01885	0.0	1.0	1.0	1001	6600
	mod[2]	0.5035	0.5	0.03697	0.0	1.0	1.0	1001	6600
	psi	0.2657	0.05634	8.482E-4	0.1653	0.2617	0.3855	1001	6600
	psi.sex	0.5449	0.1044	0.00231	0.336	0.5472	0.7417	1001	6600
	sigma	0.8442	0.6291	0.0507	0.1698	0.6551	2.704	1001	6600

run 2

Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.939	1.009	0.01712	4.18	5.822	8.211	1001	27000
	N	39.78	6.756	0.1147	28.0	39.0	55.0	1001	27000
	X1new	9.006	2.701	0.02151	4.603	8.699	15.18	1001	27000
	X1obs	11.98	3.396	0.04019	6.437	11.61	19.69	1001	27000
	X3new	13.13	3.218	0.01885	7.748	12.84	20.26	1001	27000
	X3obs	21.4	2.116	0.02102	17.83	21.2	26.11	1001	27000
	Xnew	61.54	6.483	0.0668	49.39	61.26	74.8	1001	27000
	Xobs	88.74	6.003	0.06467	77.85	88.46	101.4	1001	27000
	alpha.sex	-0.4355	1.192	0.01962	-2.624	-0.6607	2.605	1001	27000
	beta	1.257	2.088	0.1191	0.06376	1.036	6.652	1001	27000
	beta.sex	0.5605	1.675	0.06955	-2.723	0.7576	2.92	1001	27000
	deviance	438.9	12.25	0.1858	417.0	438.0	464.9	1001	27000
	logitp0	-2.598	0.287	0.01129	-3.088	-2.624	-1.998	1001	27000
	mod[1]	0.5939	0.4911	0.01784	0.0	1.0	1.0	1001	27000
	mod[2]	0.5251	0.4994	0.02614	0.0	1.0	1.0	1001	27000
	psi	0.3312	0.06925	0.001093	0.2101	0.3264	0.4776	1001	27000
	psi.sex	0.5425	0.1038	0.002119	0.3366	0.5443	0.7389	1001	27000
	sigma	1.045	0.6963	0.04014	0.2743	0.6947	2.801	1001	27000
\end{verbatim}

\end{comment}




\section{Evaluating Goodness-of-Fit}

In practical settings, we estimate parameters of a desirable model, or
maybe fit a bunch of models and report estimates from all of them or a
model-averaged summary of density.  An important question is: Is our
model worth anything?  In other words, does the model appear to be an
adequate description of our data?
%Put another way, we might ask, are
%the data we have consistent with realizations from the model which we
%just fitted and upon which our inferences are based?
Formal assessment of model adequacy or goodness-of-fit is a
challenging problem and there are no all-purpose algorithms for doing
this in either frequentist or Bayesian paradigms. Moreover, there are
some philosophical challenges to evaluating model fit, such as, if we
do model averaging then should all of the models have to fit? Or
should the averaged model have to fit? What if none of the models fit?
We don't know the answers to these questions and we won't try to
answer them. Instead, we will provide what guidance we can on taking
the first steps to evaluating fit, of a single model, as if it were a
cherished family heirloom of great importance.  We suggest that if you
have a model that you really like, a single model, then it is a
sensible thing to check that the model is a good fit to your data. If
it is not, we do not imagine that the model is useless but just that
some thought should be put into why the model doesn't fit so that,
perhaps, some remediation might happen as future data are
collected. After all, you may have spent 2, 3 or many more years of
your life collecting that data set, perhaps thousands of hours, and
therefore it seems a reasonable proposition to expect to do some
estimation and analysis of the model regardless of model fit. You can
still learn something from a model that does not pass some technical
litmus test of model fit.
% XXXX Say something about the problems associated with a model that
% doesn't fit. Namely, that it isn't likely to yield good
% predictions.... with good coverage or whatever

Conceptually, we can think of evaluation of model fit as follows: if
we simulate data under the model in question, do the simulated
realizations resemble the data set that we actually have?  For either
Bayesian or classical inference, the basic strategy to assessing model
fit is to come up with a fit statistic that depends on the parameters
and the data set, which we denote by $T({\bf y}, \theta)$, and then we
compute this for the observed data set, and compare its value to that
computed for perfect data sets simulated under the correct model.  In
the case of classical inference, we will often rely on the standard
practice of parametric bootstrapping \citep{dixon:2002}, where we
simulate data sets conditional on the MLE $\hat{\theta}$ and compare
realizations with what we've observed.  The {\bf R} package \mbox{\tt
  unmarked} \citep{fiske_chandler:2011} contains generic bootstrapping
methods for certain hierarchical models, including distance sampling
\citep[e.g., see][for an application]{sillett_etal:2012}.  In simple
cases, using classical inference methods, it is sometimes possible to
identify a test statistic of theoretical merit, perhaps with a known
asymptotic distribution.  For examples from capture-recapture see
\citet{burnham_etal:1987}, \citet{lebreton_etal:1992}, and Chapt. 5 of
  \citet{cooch_white:2006}.  For Bayesian analysis we use the Bayesian
  p-value method \citep{gelman_etal:1996} (we introduced the Bayesian
  p-value in sec. \ref{glms.sec.gof}).  Using this approach, datasets
  are simulated based on a posterior sample of the model parameters
  $\theta$ and some fit statistic for the simulated data sets, usually
  based on the discrepancy of the observed data from its expected
  values, is compared to that for the actual data.  In most cases,
  whether Bayesian or frequentist, the main idea for assessing model
  fit is the same: We compare data sets from the model we're
  interested in with the data set we have in hand. If they appear to
  be consistent with one another, then our faith in the model
  increases, at least to some extent, and we say ``the model fits.''


To date, we are unaware of any goodness-of-fit applications based on
likelihood analysis of SCR models. For 
Bayesian analysis of SCR models, there has not been a definitive or
general proposal for a fit statistic or even a class of fit
statistics, although a few specialized implementations of Bayesian
p-values have been provided \citep{royle:2009,royle_etal:2011mee,
  gopalaswamy_etal:2012mee,gopalaswamy_etal:2012ecol,russell_etal:2012}.
While we universally adopt the Bayesian p-value approach, and suggest
some fit statistics in the following text, we caution that there is
no general expectation to support how well they should do.
As such, one might consider doing some kind of custom
evaluation or calibration when using such methods, if the power of the
test (ability to reject under specific departures from the model) is
of paramount interest.  We note that this uncertain power or
performance of the Bayesian p-value is not a weakness of the Bayesian
approach because the same issue applies in using bootstrap approaches
applied to classical analysis of models, if we were to devise such
methods.



\section{The Two Components of Model Fit}

For most SCR models, there are at least two distinct components of
model fit, and we propose to evaluate these two distinct components
individually.  First, we can ask, are the data consistent with the
 {\it
  observation} model, conditional on the underlying point process?
We can evaluate this based on the encounter frequencies of individuals
{\it conditional} on (posterior samples of) the underlying point
process ${\bf s}_{1}, \ldots, {\bf s}_{N}$.  We discuss some potential
fit statistics for addressing this in the next section.  Second, we
can evaluate whether the data appear consistent with the
{\it state} process model (i.e., the ``uniformity'' assumption of 
the point process).  For the simple
model of independence and uniformity, this is similar to the
assumption of {\it complete spatial randomness} (CSR) which we
consider in Sec. \ref{gof.sec.csr} below. Actually, this is not
strictly the assumption of CSR because of the binomial assumption on
$N$ under data augmentation, so we instead use the term {\it spatial
  randomness}.
% but we refer to it as CSR because it is
%practically equivalent in most cases and CSR is more concise than
%saying ``independent and uniformly distributed''.


\subsection{Testing uniformity or spatial randomness}
\label{gof.sec.csr}


Historically, especially in ecology, there has been an extraordinary
amount of interest in whether a realization of a point process
indicates ``complete spatial randomness,'' i.e., that the points are
distributed uniformly and independently in space.  A good reference
for such things is \citet[][Ch. 8]{cressie:1992} and
\citet{illian_etal:2008}\footnote{We also like Tony Smith's lecture
  notes (Univ. of Penn. ESE 502), which can be found at
  \url{http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf}.
  XXXX
Kimmy: look this up make sure still active put access date here
  XXXXX.  }. In the context of animal capture-recapture studies, the
spatial randomness hypothesis is manifestly false, purely on
biological grounds. Typically individuals will be clustered, or more
uniform (for territorial species), than expected under spatial
randomness and heterogeneous habitat will generate the appearance of
clustering even if individuals are distributed independently of one
another. While we recommend modeling spatial structure explicitly when
possible (Chapts. \ref{chapt.state-space}, \ref{chapt.ecoldist},
\ref{chapt.rsf}), the uniformity assumption may be an adequate
description of data sets in some situations. Further, we find that it
is generally flexible enough to reflect non-uniform patterns in the
data, because we do observe some direct information about some of the point locations.


%Before proceeding with the development of a framework for evaluating
%adequacy of the point process model we note that, if $N$ is fixed, the resulting
%point process is not, strictly speaking, one of "complete spatial
%randomness". This is because when $N$ is fixed, a slight bit of
%correlation is induced in the number of points within any particular
%subset of the state-space. That said, this is negligible for most
%purposes and, besides, we use a simulation based approach to testing
%in which we simulate under the appropriate model.  But the point is
%that CSR is not really the conventional term for a binomial point
%process, but it does imply independent and uniformity.

The basic technical framework for evaluating the spatial randomness
hypothesis is based on counts of activity centers in cells or bins.
For that we use any standard goodness-of-fit test statistic, based on
griding (binning) the state-space of the point process into
$g=1,2,\ldots,G$ cells or bins,
% XXXX cells, bins, or quadrats?
and we tabulate $N_{g} \equiv N({\bf
  x}_{g})$ the number of activity centers in bin $g$, centered at
coordinate ${\bf x}_{g}$.
Specifically, let $B({\bf x})$ indicate a bin centered at
coordinate ${\bf x}$, then\footnote{$1(arg)$ is the indicator function
  which evaluates to 1 if $arg$
  is true, otherwise 0} $N({\bf x})=\sum_{i=1}^{N} 1({\bf s}_{i} \in
B({\bf x}))$ is the population size of bin $B({\bf x})$.
In
Sec. \ref{scr0.sec.mapping}, we used the summaries $N({\bf x})$
for producing density maps from MCMC output. Here, we use them for
constructing a fit statistic.
We have used the 
Freeman-Tukey statistic of this form:
\[
T({\bf N}, \theta) =  \sum_{g}  (\sqrt{N_{g}} - \sqrt{\mathbb{E}(N_{g})})^{2}
\]
where $\mathbb{E}(N_{g})$ is estimated by the mean bin count.  An
alternative conventional assessment of fit is based on the following
statistic: Conditional on $N$, the total number of activity centers in
the state-space ${\cal S}$, the bin counts $N_{g}$ should have a
binomial distribution.  It will usually suffice to approximate the
binomial cell counts by Poisson cell counts, in which case we can use
the classical ``index-of-dispersion'' test
\citep[][p. 87]{illian_etal:2008}, based on the variance-to-mean ratio:
\[
   I =  (G -1)*s^2/\bar{N}
\]
where $s^{2}$ is the sample variance of the bin counts and $\bar{N}$
is the sample mean. When the point process realization is {\it
  observed}, as in classical point pattern modeling (but not in SCR),
this statistic has approximately a Chi-square distribution on $(G- 1)$ 
degrees-of-freedom under the spatial randomness
hypothesis.  If $s^2/\bar{N} > 1$, clustering is suggested whereas,
$s^2/\bar{N} <1$ suggests the point process is too regular.


Whatever statistic we choose as our basis for assessing spatial
randomness, {\it the} important technical issue is that we don't
observe the point process and so the standard statistics for
evaluating spatial randomness cannot be computed directly.  However,
using Bayesian analysis, we do have a posterior sample of the
underlying point process and so we suggest computing the posterior
distribution of any statistic in a Bayesian p-value framework.
For a given
posterior draw of all model parameters, $N$ is known, based on the
value of the data augmentation variables $z_{i}$, and so we can obtain
a posterior sample of $N({\bf x})$ by taking all of the output for
MCMC iterations $m=1,2,\ldots,$ and doing this:
\[
   N({\bf x})^{(m)} = \sum_{z_{i}^{(m)}=1} 1({\bf s}_{i}^{(m)} \in B({\bf x}))
\]
Thus, $N({\bf x})^{(1)}, N({\bf x})^{(2)}, \ldots,$ is the Markov
chain for the derived parameter $N({\bf x})$.

In addition to computing the bin counts for each iteration of the MCMC
algorithm, at the same time we generate a realization of the activity
centers ${\bf s}_{i}$ under the spatial randomness model, and we
obtain bin counts for these ``new'' data, $\tilde{N}({\bf x})$. For each of
the posterior samples -- that of the real data, and that of the
posterior simulated data, we compute the fit-statistic. The fit 
statistic based on the actual data is:
\[
T({\bf N},\theta) = \sum_{x}  (\sqrt{N({\bf x})} - \sqrt{ \bar{N}({\bf x})})^2
\]
whereas the fit statistic based on a simulated realization of points under the
spatial randomness hypothesis is:
\[
T(\tilde{\bf N},\theta) = \sum_{x}  (\sqrt{\tilde{N}({\bf x})} - \sqrt{
  \bar{N}({\bf x})})^2
\]
And we compute the Bayesian p-value by tallying up the proportion of
times that $T(\tilde{\bf N},\theta)$ is larger than $T({\bf
  N},\theta)$, as an estimate of: $p = \Pr(T(\tilde{\bf N},\theta) >
T({\bf N},\theta))$.  The {\bf R} function {\tt SCRgof} in our package
\mbox{\tt scrbook} will do this, given
the output from {\bf JAGS}
(see below).



\subsubsection{Sensitivity to bin size}

Evaluating fit based on bin counts in point process models are
sensitive to the number of bins \citep[][p. 87-88]{illian_etal:2008}.
This is related to the classical problem of fit testing for binary
regression because in a point process model, as the number of grid
cells gets small, the grid cell counts go to 0 or 1 and standard fit
statistics (e.g., based on deviance or Pearson residuals) are known
not to be very useful.  There is some good discussion of this in
\citet[][Sec. 4.4.5]{mccullagh_nelder:1989}.
 What it boils down to is, using
the example of the Pearson residual statistic considered by
\citet{mccullagh_nelder:1989}, the fit statistic is exactly a
deterministic function of the sample size only, which clearly should
not be regarded as useful for model fit. This is why, in order to do a
check of model fit when you have a binary response, one must always
aggregate the data in some fashion.  In the context of testing spatial
randomness, computing the test statistic we described above has us chop up the
region ${\cal S}$ into bins, and tally up $N_{g}$, the
frequency of activity centers in each bin $g$.  % XXXX In testing CSR the
% investigator has to choose the bin size and s XXXX
Suppose that we choose the
bin size to be extremely small such that % XXXX.  In this case,
$\mathbb{E}(N_{g})$
tends to $N/G$ ($N$ being the number of activity centers).  Further,
$N_{g}$ tends to a binary outcome. Therefore the fit statistic has $N$
components that have value $N_{g} = 1$, and it has
 $G-N$
components that have value $N_{g} = 0$. Therefore, the fit statistic
resembles:
\[
T({\bf N},\theta) = \sum_{g \ni N_{g} = 1}^{N}  (1 - \sqrt{N/G})^2 +
\sum_{g \ni N_{g} = 0}^{G-N} (N/G)^2
 = N(1 + (G-N)/G)
\]
(here $\ni$ means ``such that''). If $G$ is huge
relative to $N$, then we see that this tends to about $2*N$, which
does not provide any meaningful assessment of model fit.  So if you
look at this in the limit in which the bin counts become binary, the
fit statistic loses all its variability to the specific model used and
is just a deterministic function of $N$. As a practical matter, it
probably makes sense to restrict the number of bins to {\it fewer}
than the number of observed individuals in the sample size. In typical
SCR
applications this will therefore result, usually, in very large (and
few) bins, and presumably not much power.

%The analysis of fit
%for SCR models has barely begun, and we think there is considerable
%research to be done on this problem which .

There are some extensions that help resolve the issue of sensitivity
to bin size. We can construct fit statistics based not just
on quadrat counts but also the neighboring quadrat counts -- this is
the Greig-Smith method \citep{greig-smith:1964}.
In addition, there are a myriad of ``distance
methods'' for evaluating point process models,
% XXXX Perhaps list a few
 and we believe that many
of these can (and will) be adapted to SCR models.
% XXXX Suggest reworking the next two sentences
Again the main
feature is that the point process on which inference is focused is
completely latent in SCR models -- so this makes the fit assessment
slightly different than in classical point processes. That said, the
methods should be adaptable, e.g., in a Bayesian p-value kind of way.


\subsubsection{Sensitivity to state-space extent}

An issue that we have not investigated is that any model assessment
that applies to a {\it latent} point process is probably sensitive to
the size of the state-space. As the size of the state-space increases
then the cell counts (far away from the data) {\it are} independent
binomial counts with constant density, and so we can overwhelm the fit
statistic with extraneous ``data'' simulated from the posterior, which
is equal to the prior as we move away from the data, and therefore
uninformed by the observed data  that live in the vicinity of the trap array.
Therefore we recommend computing these goodness-of-fit statistics in the vicinity
of the trap array only. Perhaps, as a rule-of-thumb, less than the
average trap spacing from the rectangle enclosing the trap array.
For example, if the average trap spacing is, say,
10 km, then the bins used to obtain the observed and predicted
activity centers should not extend any further from the traps than 5
km.
% XXXX This seems too ad hoc to be very useful. Perhaps we should say
% that it is a subject for future research. One idea could be to
% weight each point by encounter probability... so points far away
% from traps don't contribute to the fit-stat.




\subsection{Assessing  fit of the observation model}
\label{gof.sec.obsfit}

In evaluating the spatial randomness hypothesis, we could draw on
well-established ideas from point process modeling. On the other hand, 
it is less clear how to approach goodness-of-fit evaluation of the
observation model.  For most SCR problems, we have a 3-dimensional
data array of {\it binary} observations, $y_{ijk}$ for individual $i$,
trap $j$ and sample occasion $k$. As  discussed
in the previous section, we need to construct fit statistics based on
observed and expected frequencies that are aggregated in some fashion.
In practice, the data will be too sparse to have much power, unless
the 
data are highly aggregated. We recommend focusing on summary
statistics that represent aggregated versions of $y_{ijk}$ over 1 or 2
of the dimensions. We describe 3 such fit statistics below.  We
recognize that, depending on the model, some information about model
fit will be lost by summarizing the data in this way. For example if
there is a behavioral response and we aggregate over time to focus on
the individual and trap level summaries then some information about
lack of fit due to temporal structure in the data is lost.

{\bf Fit statistic 1: individual x trap frequencies} We summarize the
data by individual and trap-specific counts $y_{ijk}$
% XXXX I would call this something other than "y". Perhaps "C" or "n".
aggregated over
all sample occasions. Using standard ``dot notation'' to represent
summed quantities, we express that as:
$y_{ij.} = \sum_{k=1}^{K} y_{ijk}$.
 Conditional on ${\bf s}_{i}$, the expected value
under any encounter model is:
\[
 \mathbb{E}(y_{ij.}) = p_{ij} K
\]
(or $K_{j}$ if the traps are operational for variable periods). If
there is time-varying structure to the model, then expected values
would have to be computed according to $\mathbb{E}(y_{ij.}) = \sum_{k} p_{ijk}$.
Then we can define a fit statistic from the Freeman-Tukey residuals
according to:
\[
 T_{1}({\bf y}, \theta) = \sum_{i}\sum_{j} (\sqrt{ y_{ij.} } - \sqrt{ \mathbb{E}(y_{ij.}) })^2
\]
where we use $\theta$ here to represent the collection of all
parameters in the model.  This is conditional on ${\bf s}$ as well as
on the data augmentation variables ${\bf z}$. We compute this
statistic for {\it each} iteration of the MCMC algorithm for the
observed data set and also for a new data set simulated from the posterior
distribution, say $\tilde{\bf y}$.

We could also use a similar fit statistic derived from summarizing
over traps to obtain an \mbox{\tt nind} $\times$ $K$ matrix of count
statistics.  We imagine that either summary of the data will probably
be too disaggregated (have mostly values of 0) in most practical settings to have much power.


{\bf Fit statistic 2: Individual encounter frequencies. } SCR models
represent a type of model for heterogeneous encounter probability, like model $M_h$, but with an explicit
factor (space) that explains part of the heterogeneity. For model $M_h$, the
individual encounter frequencies are the sufficient statistic for
model parameters, and so it makes intuitive sense to provide some kind
of omnibus fit assessment of the core heuristic that SCR model is
adequately explaining the heterogeneity using a model $M_h$-like
statistic based on individual encounter frequencies.  So, we build a
fit statistic based on the individual total encounters
\citep{russell_etal:2012}, $y_{i..} = \sum_{j} \sum_{k} y_{ijk}$. In
addition, the expected value is a similar summary over traps and
occasions: $\mathbb{E}(y_{i..}) = \sum_{j} \sum_{k}
p_{ijk}$. Then, we define statistic $T_{2}$ according to:
\[
 T_{2}({\bf y}, \theta) = \sum_{i} (\sqrt{ y_{i..} } - \sqrt{ \mathbb{E}(y_{i..}) })^2
\]
We imagine this test statistic should provide an omnibus test of
extra-binomial variation and should therefore capture some effect of
variable exposure to encounter of individuals, although we have not
carried out any evaluations of power under specific alternatives.
Obviously, in using this statistic, we lose information on departures
from the model that might only be trap- or time-specific.


{\bf Fit Statistic 3: Trap frequencies. } We construct an analogous
statistic based on aggregating over individuals and replicates to form
trap encounter frequencies: $y_{.j.} = \sum_{i} \sum_{k} y_{ijk}$
\citep{gopalaswamy_etal:2012ecol} and the expected value is a similar
summary over individuals and occasions: $\mathbb{E}(y_{.j.}) = \sum_{i}
\sum_{k} p_{ijk}$.  Then statistic $T_{3}$ is:
\[
 T_{3}({\bf y}, \theta) = \sum_{j} (\sqrt{ y_{.j.} } - \sqrt{ \mathbb{E}(y_{.j.}) })^2
\]
This seems like a sensible fit statistic because we can think of SCR
models as 
spatial models for counts
\citep{chandler_royle:2012}. Therefore, we should seek models that
provide good predictions of the observable spatial data, which are the
trap totals.  In this context, it might even make sense to pursue
cross-validation based methods for model selection.  Cross-validation 
is a standard method of evaluating models such as in kriging or spline
smoothing, so we could as well develop such ideas based on the
trap-specific frequencies.
% OK, cool, but I think something about x-val should be said above, in
% the model selection sections.

\subsection{Does the SCR model fit the wolverine data?}

We use the ideas described in the previous section to evaluate
goodness-of-fit of the SCR model to the wolverine camera trapping data.


We consider first whether the simple model of spatial randomness of
the activity centers is adequate.  We think that the
 encounter model shouldn't have a large  effect on
whether the spatial randomness assumption is adequate or not, so we
fit ``Model 0'' (in which parameters are {\it not} sex specific)
using an {\bf R} script provided
in the function \mbox{\tt wolvSCR0gof} which will default to fitting
the model in {\bf JAGS}.  This is the same script as \mbox{\tt
  wolvSCR0ms} except that it saves the MCMC output for the activity
centers ${\bf s}$ and the data augmentation variables $z$, which are
required in order to compute the Bayesian p-value test of spatial randomness.

The MCMC output is processed with  the {\bf R} function
\mbox{\tt SCRgof} which
computes the test of spatial randomness based on bin counts, using the 
Bayesian p-value calculation. The function \mbox{\tt SCRgof} requires
 a few things as inputs: (1) the output from a
{\bf BUGS} run (in particular, the activity center coordinates and the
data augmentation variables); (2) the number of bins to create for
computing spatial frequencies of activity centers;  (3) the trap
locations and, (4) the buffer
around the trap array to use in computing the bin counts.  This buffer could be that used
in defining the state-space for the model fitting, but we think it should be
relatively tighter to the trap array than the state-space used in
model-fitting. For the wolverine analysis,  where we're using 10-km grid cells
(1 unit = 10 km) and a 20 km buffer for model fitting, we'll use a
state-space buffer of  0.4 units (4 km) for
computing the fit statistic.
The {\bf R} code to fit the model and obtain the goodness-of-fit
result is as follows:
%%%RUN on 12/20/2012
%%%%%This is wolvSCR0gof-testing.R script off of desktop. Needs put in package.
{\small
\begin{verbatim}
> toad1 <- wolvSCR0gof(nb=1000,ni=6000,buffer=2,M=200,model=0)

> bugsout <- toad1$BUGSoutput$sims.list

> traplocs <- wolverine$wtraps[,2:3]
> traplocs[,1] <- traplocs[,1] - min(traplocs[,1])
> traplocs[,2] <- traplocs[,2] - min(traplocs[,2])
> traplocs <- traplocs/10000

> set.seed(2013)   # set seed so Bayesian p-value is the same each time

> SCRgof(bugsout,5,5,traplocs=traplocs,buffer=.4)

Cluster index observed:  1.099822
Cluster index simulated:  1.000453
P-value  index of dispersion:  0.408
P-value2 freeman-tukey:  0.6842667
\end{verbatim}
}
The output produced by \mbox{\tt SCRgof} is the cluster index based on
the ratio of the variance to the mean
(see above), which is computed as the posterior mean index of
dispersion for the latent point process, and also the average value
for simulated data. If this value is $>1$ then clustering is
suggested, which we see a (very) minor amount of evidence for here. Two
Bayesian p-values are produced: the first is based on the cluster
index, and the 2nd is based on the Freeman-Tukey statistic calculated
as described in Sec. \ref{gof.sec.csr}.  Because our p-values aren't
close to 0 or 1, we judge that the model of spatial randomness
provides an adequate fit to the data. You can verify that a similar result is obtained if
we use the model with fully sex-specific parameters (Model 4).

Next, we did a Bayesian p-value analysis of the observation component
of the model, using the 3 fit statistics
described
in Sec. \ref{gof.sec.obsfit}.
These statistics can be calculated as part of
the {\bf BUGS} model specification or by post-processing the MCMC
output returned from a {\bf BUGS} run.
The {\bf R} script \mbox{\tt wolvSCR0gof} contains the relevant
calculations.  For example, to compute fit statistic 1, we have to add
some commands to the {\bf
  BUGS} model specification such as this (note: this is only a
fraction of the model specification):
\begin{alltt}
.......
for(j in 1:ntraps)\{
 mu[i,j] <- w[i]*p[i,j]

 y[i,j] \(\sim\) dbin(mu[i,j],K[j])
ynew[i,j] \(\sim\) dbin(mu[i,j],K[j])

err[i,j] <-  pow(pow(y[i,j],.5) - pow(K[j]*mu[i,j],.5),2)
errnew[i,j] <- pow(pow(ynew[i,j],.5) - pow(K[j]*mu[i,j],.5),2)
\}

T1obs <- sum(err[,])
T1new <- sum(errnew[,])
.......
\end{alltt}
Similar calculations are carried out to obtain the posterior samples
of test statistics 2 (individual totals) and 3 (trap totals). For the
wolverine data, the Bayesian p-value calculations produce:
{\small
\begin{verbatim}
> mean(toad1$BUGSoutput$sims.list$T1new>toad1$BUGSoutput$sims.list$T1obs)
[1] 0

> mean(toad1$BUGSoutput$sims.list$T2new>toad1$BUGSoutput$sims.list$T2obs)
[1] 0.17

> mean(toad1$BUGSoutput$sims.list$T3new>toad1$BUGSoutput$sims.list$T3obs)
[1] 0.02066667
\end{verbatim}
}
Based on statistic  $T_2$, we might conclude that the model
is adequate for explaining individual heterogeneity although the other
two statistics 
 suggest a general lack of fit of the observation model.
A similar result is obtained using the fully sex-specific
model.
We 
note that one individual was captured 8 times in one trap, which is
pretty extreme under a model which assumes independent Bernoulli
trials. We summarize that the trap-counts simply are not
well-explained by this model.

\begin{comment}
This produces:
\begin{verbatim}
fully sex-specific model:
>  mean(toad4$BUGSoutput$sims.list$T1new>toad4$BUGSoutput$sims.list$T1obs)
[1] 0
>  mean(toad4$BUGSoutput$sims.list$T2new>toad4$BUGSoutput$sims.list$T2obs)
[1] 0.234
>  mean(toad4$BUGSoutput$sims.list$T3new>toad4$BUGSoutput$sims.list$T3obs)
[1] 0.02133333
\end{verbatim}
which doesn't improve fit very much at all.
\end{comment}

In attempt to resolve this problem, we extended the model to include a
local (trap-specific)
behavioral response (following \citet{royle_etal:2011jwm}) which can
be fitted using the sample {\bf R} script \mbox{\tt
  wolvSCRMb}. 
To fit a model using {\bf WinBUGS}, and then compute the
Bayesian p-values we do this:
%%%{\bf NOTE: THIS IS SCRIPT  wolverine_GOF_behave.R  in R script and
%%%%utilities directory/Chapter8 - GoF-Modelsel }
\begin{verbatim}
> wolv.Mb <- wolvSCRMb(nb=1000,ni=6000,buffer=2,M=200)

> mean(wolv.Mb$sims.list$T1new>wolv.Mb$sims.list$T1obs)
[1] 0.9666667

> mean(wolv.Mb$sims.list$T2new>wolv.Mb$sims.list$T2obs)
[1] 0.3644667

> mean(wolv.Mb$sims.list$T3new>wolv.Mb$sims.list$T3obs)
[1] 0.4990667
\end{verbatim}
Given that this model seems to fit better, we might prefer reporting
estimates under this model, which we do in Table \ref{gof.tab.wolvMb}.
(the behavioral response parameter is labeled $\alpha_2$ in the
Table).  Estimated density is about 1 individual higher per 1000
km$^2$ compared with the various models that lack a behavioral
response.  It might be useful to try these fit assessment exercises
using the habitat mask as described in
Sec. \ref{scr0.sec.discrete}. That takes an extremely long time to run
in \mbox{\bf BUGS} though, especially for the behavioral response
model.



\begin{table}[ht]
\centering
\caption{
Posterior summary statistics for local (trap-specific) behavioral
response model $M_{b}$ fitted to the wolverine camera trapping data
using {\bf WinBUGS}. The parameter $\alpha_{2}$ is the local
(trap-specific) behavioral
response parameter. $T_{x}()$ are the posterior summaries of fit
statistics $x=1,2,3$ used in the Bayesian p-value analysis (See text for
definitions). Results are based on
 3 chains, each with 6000 iterations (first 1000 discarded) for a
 total of 15000 posterior samples. 
}
\begin{tabular}{lrrrrrrr} \hline \hline 
Parameter   & Mean  & SD  & 2.5\% & 50\% & 97.5\% & Rhat &n.eff \\ \hline
$N$          & 71.32 &19.07 &42.00 &69.00 &114.02 &1.00  &2100 \\
$D$          &  6.87 & 1.84 & 4.05 & 6.65 & 10.99 &1.00  &2100\\ \hline
$\sigma$    &  0.88 & 0.13 & 0.68 & 0.86 & 1.17  &1.00  & 730 \\
$p_0$        &  0.01 & 0.00 & 0.01 & 0.01 & 0.02  &1.01  & 530\\
$\alpha_1$   &  0.69 & 0.19 & 0.37 & 0.67 &  1.10 &1.00  & 730\\
$\alpha_2$   &  2.50 & 0.27 & 1.99 & 2.50 &  3.04 &1.00  & 700\\
$\psi$       &  0.36 & 0.10 & 0.20 & 0.35 & 0.58  &1.00  &2600  \\
$T_{1}^{obs}$  & 54.71 & 6.12 &43.69 &54.39 & 67.47 &1.00  &3900\\
$T_{1}^{new}$  & 64.73 & 7.62 &50.93 &64.39 & 80.96 &1.00  &3900\\
$T_{2}^{obs}$  & 13.93 & 4.07 & 7.25 &13.53 & 23.04 &1.00  &5700\\
$T_{2}^{new}$  & 12.65 & 3.35 & 6.93 &12.36 & 20.07 &1.00  &2000\\
$T_{3}^{obs}$  & 12.80 & 1.74 & 9.80 &12.64 & 16.61 &1.00  &2400\\
$T_{3}^{new}$  & 12.94 & 3.05 & 7.77 &12.67 & 19.58 &1.00 &15000\\ \hline
%deviance  1128.02 15.38 1101.00 1127.00  1161.00 1.00   640
\end{tabular}
\label{gof.tab.wolvMb}
\end{table}

\section{Quantifying Lack-of-fit and Remediation}

\citet{kery_etal:inreview} used a strategy for assessing model
fit in dynamic occupancy models \citep{royle_kery:2007} similar to that which we suggested above.
They constructed a fit statistic based on aggregating the data over
replicate samples ($k$), to obtain the total detections per site $i$
and year $j$.  They used a Bayesian p-value analysis based on a
Chi-squared test statistic \citep[also
see][Chapt. 12]{kery_schaub:2011}.  Their analysis suggested a model
that didn't fit, and, so they computed the ``lack-of-fit ratio''
\cite[see][Sec. 12.3]{kery_schaub:2011} -- the ratio of the fit
statistic computed for the actual data to that of the replicate data
sets.  They interpret this analogous to the over-dispersion coefficient
in generalized linear models \citep{mccullagh_nelder:1989}, usually
called the c-hat statistic in capture-recapture literature
\citep[see][Chapt. 5]{cooch_white:2006}.  \citet{kery_etal:inreview}
reported the lack-of-fit ratio for their model to be 1.14 which
suggests a minor lack-of-fit, compared to perfect data having a value
of 1, because the posterior standard deviations will be too small by a
factor of $\sqrt{1.14} = 1.07$.
In classical capture-recapture
applications of goodness-of-fit assessment, inference for non-fitting
models is dealt with by inflating the resulting SEs (of the non-fitting model),
by the square-root of c-hat. 
We believe that these ideas related to quantifying lack-of-fit and
understanding its effect could also be applied to SCR models, although
we have not yet explored this. 


\begin{comment}
actually, we used that idea of quantifying the degree of lack of fit using some c-hat-like statistic in the BPA book in the comparison of the series of Nmix models in section 12.3. (p. 396 and later). We call this a "lack of fit ratio": e.g., p. 401 in the middle, and then later on p. 404 and 407. I think that this makes sense intuitively, but I wonder whether there is some more theoretical foundation for it ?
\end{comment}


\section{ Summary and Outlook  }



In this chapter, we offered some general strategies for model
selection and model checking, or assessment of model fit.  We think
the strategies we outlined for model selection are fairly standard
and can be effectively applied to many SCR modeling problems.
Some technical issues of Bayesian analysis need to be addressed (in
general) before Bayesian methods are more generally useful and
accessible.  For one thing, Bayesian model selection based on the
indicator variable approach of \citet{kuo_mallick:1998} can be
tediously slow even for small data sets, and so improved computation
will improve our ability to do Bayesian model selection in practical
situations.  Also, and most importantly, sensitivity to prior
distributions is an important issue. Further research and practice
might identify preferred prior configurations for SCR that provide a
good calibration in relevant model selection problems.
Finally, we believe that 
cross-validation should prove to be a useful method in model
assessment and selection, 
as SCR models are a form of spatial model of counts, and so it is
natural to pick models that predict the observable spatial counts
(i.e., at trap locations) well.

For Bayesian model assessment, or goodness-of-fit checking, we
suggested a framework based on independent testing of the spatial
model of independence and uniformity, and testing fit of the
observation model conditional on the underlying point process.  These
ideas are based on mostly {\it ad hoc} attempts in a number of
published applications \citep[e.g.,]{royle_etal:2009ecol,
  royle_etal:2011mee, gopalaswamy_etal:2012ecol, russell_etal:2012}.
While we think this general strategy should be fruitful, we know of no
studies on the power to detect various model departures, and so the
ideas should be viewed as experimental. We have not discussed
assessment of model fit for SCR models using likelihood methods,
although we imagine that standard bootstrapping ideas should be
effective, perhaps based on the fit statistics (or similar ones) we
suggested here for computing Bayesian p-values.

Clearly there is much research to be done on assessment of model fit
in SCR models. For testing the spatial randomness hypothesis, we used
a classical approach based on count frequencies, in which point
locations are put into spatial bins. Other approaches from spatial
point process modeling should be pursued including nearest-neighbor
methods or distance-based methods. In addition, studies to evaluate
the power to detect relevant departures from the standard
assumptions, and the robustness of inferences about $N$ or density,
need to be conducted.  If the spatial randomness model appears
inadequate, 
it is possible to fit models that allow for a non-uniform
distribution of points (see Chapt. \ref{chapt.state-space}) and even
point process models that allow for interactions among points
\citep{reich_etal:2012}. On the other hand, we expect that most of
these Bayesian p-value tests will have low power in typical data
sets consisting of a few to a few dozen individuals. As such, failure
to detect a lack of fit may not be that meaningful. But, on the other
hand, it may not make a difference in terms of density estimates
either.  We think inference about density should be relatively
insensitive to departures from spatial randomness, because we get to
observe direct information on some component of the population,
component of density is {\it observed}. For those activity
centers, the assumed model of the point process should exert little
influence on 
the placement of the activity centers.  Conversely, as is the case
with classical closed population models \citep{otis_etal:1978,dorazio_royle:2003,
  link:2003}, inferences may be somewhat more sensitive to
bad-fitting models for the observation process.

% XXXX I may have missed it, but did you mention inhomogenenous point
% process models? We should recommend that these be fitted if one
% fails the "spatial randomness" test.











