\chapter{
Model Selection and Assessment
}
\markboth{Model Assessment}{}
\label{chapt.gof}

\vspace{.3in}


Our purpose in life is to analyze models.  By that, we mean one or
more of the following basic 4 tasks: (1) estimate parameters, (2) make
predictions of unobserved random variables, (3) evaluate the relative
merits of different models or choosing a best model (model selection),
and (4) checking whether a specific model appears to provide a
reasonable description of the data or not (model checking, assessment,
or ``goodness-of-fit'').  In previous chapters we addressed the
problems of estimation of model parameters, and also making
predictions of latent variables, ${\bf s}$ or $z$, or functions of
these variables such as density or population size.  In this chapter,
we focus on the last two of these basic inference tasks: model
selection (which model or models should be favored), and model
assessment (do the data appear to be consistent with a particular
model).


In this chapter we review basic strategies of model selection using
both likelihood methods (as implemented in the \mbox{\tt secr}
package) and Bayesian analysis.  Specifically, we review a number of
standard methods of model selection that apply to ``variable
selection'' problems, when our set of models consists of distinct
covariate effects and they represent constraints of some larger model.
For classical analysis based on likelihood, model selection by Aikaike
Information Criterion (AIC) is the standard approach
\citep{burnham_anderson:2002}.  For Bayesian analysis we rely on a
number of different methods.  We demonstrate the use of the deviance
information criterion (DIC) \citep{spiegelhalter_etal:2002} for
variable selection problems although it has deficiencies when applied
to hierarchical models in some cases \citep{millar:2009}.  We use the
Kuo and Mallick indicator variable selection approach
\citep{kuo_mallick:1998} which produces direct statements of posterior
model probabilities which we think are the most useful, and leads
directly to model-averaged estimates of density.  There is a good
review paper recently by \citet{ohara_sillanpaa:2009} that discusses
these and many other related ideas for variable selection.  In
addition to \citet{ohara_sillanpaa:2009} we also recommend
\citet[][Chapt. 7]{link_barker:2010} for general information on model
selection and assessment.

To check model adequacy in a Bayesian framework, or whether a specific
model provides a satisfactory description of our data set, we rely
exclusively on the Bayesian p-value framework
\citep{gelman_etal:1996}.  For assessing fit of SCR models, part of
the challenge is coming up with good measures of model fit, and there
does not appear much definitive guidance in the literature on this
point.  Following \citet{royle_etal:2011mee}, we break the problem up
into 2 components which we attack separately: (1) Conditional on the
underlying point process, does the encounter model fit? (2) Do the
uniformity and independence assumptions appear adequate for the point
process model of activity centers? The latter component of model fit
has a considerable precedence in the ecological literature as it is
analogous to the classical problem of testing ``complete spatial
randomness'' \citep{cressie:1992, illian_etal:2008}.


We apply some of these methods to the wolverine camera trapping data
first introduced in Chapt. \ref{chapt.scr0} to investigate sex
specificity of model parameters and whether there is a behavioral
response to encounter. We note that individuals are drawn to the
camera trap devices by bait and therefore it stands to reason that
once an individual discovers a trap, it might be more likely to return
subsequently, a response termed ``trap happiness". We evaluate whether
certain models for encounter probability appear to be adequate
descriptions of the data, and we evaluate the uniformity assumption
for the underlying point process.



\section{Model Selection by AIC}
\label{gof.sec.aic}

Using classical analysis based on likelihood, model selection is
easily accomplished using AIC \citep{burnham_anderson:2002} which we
demonstrate below. The AIC of a model is simply twice the negative
log-likelihood evaluated at the MLE, penalized by the number of
parameters ($np$) in the model:
\[
 \mbox{AIC} = -2 \mbox{logL}(\hat{\bm \theta}|{\bf y})  + 2 np
\]
Models with small values of AIC are preferred.
It is common to use a modified (``corrected'') AIC referred to as $AIC_{c}$ for small
sample sizes which is
\[
 \mbox{AIC}_{c}  =
-2 \mbox{logL}(\hat{\bm \theta}|{\bf y})  + \frac{2 np
  (np+1)}{n-np-1}
\]
where $n$ is the sample size.  Two important problems with the use of
AIC and AIC$_{c}$ are that they don't apply directly to hierarchical
models that contain random effects, unless they are computed directly
from the marginal likelihood (for SCR models we can do this, see
Chapt. \ref{chapt.mle}). Moreover, it is not clear what should be the
effective sample size $n$ in calculation of AIC$_{c}$, as there can be
covariates that affect individuals, that vary over time, or space.  We
do not offer strict guidelines as to when to use a small sample size
adjustment.

The {\bf R} package \mbox{\tt secr} computes and outputs AIC
automatically for each model fitted and it provides some capabilities
for producing a model selection table (function \mbox{\tt AIC}) and
also doing model-averaging (function \mbox{\tt model.average}), which
we recommend for obtaining estimates of density from multiple models.

\subsection{AIC analysis of the wolverine data}

We provide an example of model selection for the wolverine camera
trapping data using \mbox{\tt secr}.
 We consider a model set with  distinct models to accommodate
various types of sex specificity of model parameters:
\hspace{.5in} \begin{itemize}
\item[] Model 0: model SCR0 with constant density and constant
  encounter model parameters;

\item[] Model 1: model SCR0 with constant parameter
values for both male and female wolverines but with sex-specific
density only;

\item[] Model 2: Sex-specific density, sex-specific $p_{0}$ but constant $\sigma$;

\item[] Model 3: Sex-specific density, sex-specific $\sigma$ but constant
$p_{0}$;

\item[] Model 4: Sex-specific density, sex-specific $p_{0}$ and sex-specific $\sigma$.
\end{itemize}

To model sex-specific abundance (density), we  use the multi-session models  provided by
\mbox{\tt secr} (introduced in Sec. \ref{mle.sec.multisession}), which
allow one to model session-specific effects on density, baseline
encounter probability, $p_{0}$ (labeled $g_{0}$ in \mbox{\tt secr}), and also the scale
parameter $\sigma$ of the encounter probability model. Using this
formulation, we define the ``Session'' variable to be a {\it
  categorical} sex code having value 1 or 2 (demonstrated below) and
thus {\it session}-specific parameters represent {\it sex}-specific parameters.
For example, if we model session-specific density, $D$, then this
corresponds to Model 1 in our list above.  We note that ``Model 0'' in
our list corresponds to a model where all of the encounter
histories have the same session ID. This model is one of constant
density, which implies that the population sex ratio is fixed at 0.5, i.e.,
$\psi_{sex} = 0.5$. 

Although \mbox{\tt secr} also uses the logit/log linear predictors as
the default for modeling covariates on baseline encounter probability
and the scale parameter, respectively, \mbox{\tt secr} does something
different with the multi-session models. It reports estimates in a
{\it session mean} parameterization (equivalent to, in {\bf BUGS},
using an index variable instead of a set of dummy variables), and not
the {\it session effect} (i.e., deviation from the intercept) which
arises from the use of dummy variables.  We show this \bugs~ model
description in Sec. \ref{gof.sec.dicwolverine}.


To fit these models using \mbox{\tt secr}, we load the wolverine data
and do a slight bit of formatting to prepare the data objects for
analysis by \mbox{\secr}. The key difference from our analysis in
Chapt. \ref{chapt.mle} is, here, we use the wolverine sex information
(\mbox{\tt wolverine\$wsex}) which is a binary 0/1 variable (1=male)
and we add 1 so that we can define a categorical ``Session'' variable
(having values 1 or 2). We also have a function \mbox{\tt scr2secr}
which converts a standard trap-deployment file (TDF) matrix into a
\mbox{\tt secr} object of class ``\mbox{\tt traps}.''  The {\bf R}
commands are as follows (contained in the help file \mbox{\tt
  ?secr\_wolverine}):
{\small
\begin{verbatim}

> library(secr)
> library(scrbook)
> data(wolverine)
> traps <- as.matrix(wolverine$wtraps)

## Name variables as required by secr
> dimnames(traps) <- list(NULL,c("trapID","x","y",paste("day",1:165,sep="")))
## Convert trap information to a secr "traps" object
> trapfile <- scr2secr(scrtraps=traps,type="proximity")

## Grab the wolverine state-space grid (2km here)
> gr <- as.matrix(wolverine$grid2)
> dimnames(gr) <- list(NULL,c("x","y"))
> gr2 <- read.mask(data=gr)

## Grab the encounter data, and re-name variables
> wolv.dat <- wolverine$wcaps
> dimnames(wolv.dat) <- list(NULL,c("Session","ID","Occasion","trapID"))

## Convert binary 0/1 sex variable to categorical 1/2 for "session"
> wolv.dat[,1] <- wolverine$wsex[wolv.dat[,2]]+1
> wolv.dat <- as.data.frame(wolv.dat)

## Convert to capthist object
> wolvcapt <- make.capthist(wolv.dat,trapfile,fmt="trapID",noccasions=165)
\end{verbatim}
}

Once the data have been prepared in this way, we use the
\mbox{\tt secr} model fitting function \mbox{\tt secr.fit} to fit the
different models, and then the function \mbox{\tt AIC} to
package the models together and summarize them in the form of an AIC
table, with rows of the table ordered from best to worst. The function
\mbox{\tt model.average} performs AIC-based model-averaging of the
parameters specified by the \mbox{\tt realnames} variable (below this
is demonstrated for the parameter density, $D$).  Because this
% XXXX Earlier, AIC was in math-mode. Should we stick with $AIC_c$?
function defaults to averaging by AIC$_c$, we slightly modified
this function (called \mbox{\tt model.average2}) to do model averaging
by either  AIC or AIC$_c$ as specified by the user. The model fitting
commands look like this (for Model 0 and Model 1):
{\small
\begin{verbatim}
> model0 <- secr.fit(wolvcapt, model=list(D~1, g0~1, sigma~1), 
                  buffer=20000)
> model1 <- secr.fit(wolvcapt, model=list(D~session, g0~1, sigma~1), 
                  buffer=20000)
\end{verbatim}
}
Next we use the function \mbox{\tt AIC}, passing the fit objects from
all 5 models, and that produces the following output (abbreviated
horizontally to fit on the page):
{\small
\begin{verbatim}
> AIC (model0,model1,model2,model3,model4)
            model         ... npar  logLik    AIC     AICc dAICc  AICwt
model0  D~1 g0~1 sigma~1  ...  3 -627.2603 1260.521 1261.932 0.000 0.5831
model2      ..            ...  5 -624.9051 1259.810 1263.810 1.878 0.2280
model1      ..            ...  4 -627.2365 1262.473 1264.973 3.041 0.1275
model4      ..            ...  6 -624.6632 1261.326 1267.326 5.394 0.0393
model3      ..            ...  5 -627.2358 1264.472 1268.472 6.540 0.0222
\end{verbatim}
}
Model averaging the results is done as follows:
\begin{samepage}
{\small 
\begin{verbatim}
> model.average (model0,model1,model2,model3,model4,realnames="D")
              estimate  SE.estimate          lcl          ucl
session=1 2.707190e-05 7.913577e-06 1.544474e-05 4.745224e-05
session=2 2.927423e-05 8.270402e-06 1.700631e-05 5.039193e-05
\end{verbatim}
}
\end{samepage}
As usual, estimates and standard errors of the individual model
parameters can be obtained from the \mbox{\tt secr.fit} summary output
of any of the \mbox{\tt modelX} objects shown above.
The default output of estimated density is in individuals per ha, so
we have to scale this up to something more reasonable. To get into
units of per 1000 km$^2$, we need to first multiply by 100 to get to units of
km$^2$ and then multiply by 1000. This produces an estimated density of
about 2.71 for \mbox{\tt session=1} (females) and 2.93 for
\mbox{\tt session=2} (males).  We can use the generic {\bf R} function
\mbox{\tt predict} applied to the \mbox{\tt secr.fit} output to obtain
 specific information about the MLEs on the natural scale.

 We don't necessarily agree with the use of AIC$_c$ here and think its
 better to use AIC, in general. This is because, as noted previously,
 it is not clear what the effective sample size is for most
 capture-recapture problems. While we have 21 individuals in the data
 set, most of the model structure has to do with encounter probability
 samples and for that there are hundreds of observations. We do note
 that the AIC and AIC$_c$ results are not entirely consistent.  By
 looking at the best model by AIC (Table \ref{gof.tab.aic}), we find
 that the model with sex specfic density and sex-specific baseline
 encounter probability, $p_{0}$, is preferred (Model 2). This is just
 slightly better than the null model (Model 0) with no sex effects at all
 and hence an implied fixed sex ratio of $\psi_{sex} = 0.50$).


\begin{table}[ht]
\centering
\caption{
  Model selection results for the  wolverine models of sex specificity,
  with/without habitat mask.  Fitting was done 
  using  \mbox{\tt secr} with a half-normal (Gaussian) encounter probability
  model. Models are ordered by
  $AIC$. Density, $D$, is
  reported in units of individuals per 1000 km$^2$. Model abbreviations
  indicate which parameters are sex-specific in order $D/p_{0}/\sigma$.
}
\begin{tabular}{lccccccccc}
\hline \hline
\multicolumn{10}{c}{NO HABITAT MASK} \\ \hline
        &      &     &      & \multicolumn{3}{c}{Female} & \multicolumn{3}{c}{Male} \\ 
  model & npar & AIC & AICc & D & $p_0$ & $\sigma$ & D & $p_0$ &  $\sigma$  \\ \hline
2: sex/sex/1    &  5&  1259.8& 1263.8 &2.45& 0.08& 6435.51& 3.16& 0.04& 6435.51\\
0: 1/1/1        &  3&  1260.5& 1261.9 &2.83& 0.06& 6298.66& 2.83& 0.06& 6298.66\\
4: sex/sex/sex  &  6&  1261.3& 1267.3 &2.59& 0.08& 6080.70& 2.99& 0.04& 6833.16\\
1: sex/1/1      &  4&  1262.5& 1265.0 &2.69& 0.06& 6298.69& 2.96& 0.06& 6298.69\\
3: sex/1/sex    &  5&  1264.5& 1268.5 &2.70& 0.06& 6280.49& 2.95& 0.06& 6319.03\\
\hline \hline
\multicolumn{10}{c}{WITH HABITAT MASK} \\ \hline
        &      &     &      & \multicolumn{3}{c}{Female} & \multicolumn{3}{c}{Male} \\ 
  model & npar & AIC & AICc & D & $p_0$ & $\sigma$ & D & $p_0$ &  $\sigma$ \\ \hline
2: sex/sex/1   &  5& 1268.1& 1272.1 &  3.64& 0.07& 6382.88& 4.73& 0.03& 6382.88 \\
4: sex/sex/sex &  6& 1268.7& 1274.7 &3.87& 0.07& 5859.40& 4.41& 0.03& 7039.09\\
0: 1/1/1       &  3& 1271.2& 1272.6 &4.18& 0.05& 6282.62& 4.18& 0.05& 6282.62\\
1: sex/1/1     &  4& 1273.1& 1275.6 &3.98& 0.05& 6282.65& 4.38& 0.05& 6282.65\\
3: sex/1/sex   &  5& 1275.1& 1279.1 &3.93& 0.05& 6357.26& 4.41& 0.05& 6220.22\\
\hline
\end{tabular}
\label{gof.tab.aic}
\end{table}


We fit the same models but now using a modified state-space which
excludes the ocean (this is a habitat mask in \mbox{\tt secr}).
 Results are shown in Table \ref{gof.tab.aic} along with
the previous models without a mask.  We see AIC values are smaller for
the model without the mask. It is probably acceptable to compare these
different fits (with and without habitat mask) by AIC because we
recognize the mask as having the effect of modifying the random
effects distribution (i.e., of the activity centers, ${\bf s}$) and
the results should be sensitive to choice of the distribution for
${\bf s}$. That said, we tend to prefer the mask model because it
makes sense to exclude the areas of open water from the state-space of ${\bf
  s}$.  For females the model-averaged density is 3.88 individuals per
1000 km$^2$ and for males the model-averaged density estimate is 4.46
individuals per 1000 km$^2$ as we see here:
{\small
\begin{verbatim}
> model.average (model0b,model1b,model2b,model3b,model4b,realnames="D")

              estimate  SE.estimate          lcl          ucl
session=1 3.876615e-05 1.189102e-05 2.153795e-05 6.977518e-05
session=2 4.459658e-05 1.323696e-05 2.523280e-05 7.882022e-05
\end{verbatim}
}
This is quite a bit higher than that based on the rectangular state-space
(i.e., not specifying a habitat mask). This is not surprising given
that {\bf the state-space is part of the model} and the specific
state-space modification we made here, which reduces the area from the
rectangular state-space, should be extremely important
from a biological standpoint (i.e., wolverines are not actively using 
open ocean). 



\section{Bayesian Model Selection}

Model selection is somewhat less straightforward as a Bayesian, and
there is no canned all-purpose method like AIC. As such we
recommend a pragmatic approach, in general, for all problems,
based on a number of basic considerations:
\begin{itemize}
\item[(1)] For a small number of fixed effects we think it is
  reasonable to adopt a conventional ``hypothesis testing'' approach
  -- i.e., if the posterior for a parameter overlaps zero
  substantially, then it is probably reasonable to discard that
  effect from the model.
\item[(2)] Calculation of posterior model probabilities: In some cases
  we can implement methods which allow calculation of posterior model
  probabilities. One such idea is the indicator variable selection
  method from \citet{kuo_mallick:1998}.  For this, we introduce a latent
  variable $w \sim \mbox{Bern}(.5)$ and expand the model to include
  the variable $w$ as follows:
\[
 \mbox{logit}(p_{ijk}) = \alpha_{0} + w*\alpha_{1}*C_{ijk}.
\]
The importance of the covariate $C$ is then measured by the posterior
probability that $w=1$.
\item[(3)] The Deviance Information Criterion (DIC): Bayesian model
  selection is now routinely carried out using DIC (\citep{spiegelhalter_etal:2002}),
  although its
  effectiveness in hierarchical models depends very much on the manner
  in which it is constructed \citep{millar:2009}.  We recommend using
  it if it leads to sensible results, but we think it should be
  calibrated to the extent possible for specific classes of models.
  This has not yet been done in the literature for SCR models, to our knowledge.
\item[(4)] Logical argument: For something like sex specificity of
  certain parameters, it seems to make sense to leave an extra
  parameter in the model no matter what because, biologically, we might
 expect a difference (e.g., home range size).
In some cases failure to apply logical argument leads to
  meaningless tests of gratuitous hypotheses \citep{johnson:1999}.
\end{itemize}
In all modeling activities, as in life itself, the use of logical argument should not be under-utilized.

\subsection{Model selection by DIC }

The availability of AIC makes the use of likelihood methods convenient
for problems where likelihood estimation is achievable.  For Bayesian
analysis, DIC seemed like a general-purpose equivalent, at least for a
brief period of time after its invention.  However, there seem to be
many variations of DIC, and a consistent version is not always
reported across computing platforms.  Even statisticians don't have
general agreement on practical issues related to the use of DIC
\citep{millar:2009}.  Despite this, it is still widely reported. We
think DIC is probably reasonable for certain classes of models that
contain only fixed effects, or for which the latent variable structure
is the same across models so that only the fixed effects are varied
(this covers many SCR model selection problems).  However, it would be
useful to see some calibration of DIC for some standardized model
selection problems.

Model deviance is defined as negative twice the log-likelihood;
i.e., for a given model with parameters $\theta$: $\mbox{Dev}(\theta) =
-2*\mbox{logL}(\theta|{\bf y})$.  The DIC is defined as the
posterior mean of the deviance, $\overline{\mbox{Dev}}(\theta)$, plus a measure of model complexity,
$p_{D}$:
\[
 \mbox{DIC} = \overline{\mbox{Dev}}(\theta) + p_{D}
\]
The standard definition of $p_{D}$ is
\[
 p_{D} = \overline{\mbox{Dev}}(\theta) - \mbox{Dev}(\bar{\theta})
\]
where the 2nd term is the deviance evaluated at the posterior mean of
the model parameter(s), $\bar{\theta}$. The $p_{D}$ here is interpreted as the effective
number of parameters in the model.  \citet{gelman_etal:2004} suggest a
different version of $p_{D}$ based on one-half the posterior variance
of the deviance:
\[
 p_{V} = \mbox{Var}(\mbox{Dev}(\theta)|{\bf y})/2.
\]
This is what is produced from {\bf WinBUGS} and {\bf JAGS} if they are
run from \mbox{\tt R2WinBUGS} or \mbox{\tt R2jags}, respectively.  It
is less easy to get DIC summaries from \mbox{\tt rjags}, so we 
used \mbox{\tt R2jags} in our analyses below.


\subsection{DIC analysis of the wolverine data}
\label{gof.sec.dicwolverine}


We repeated the analysis of the wolverine models with sex specificity,
but this time doing a Bayesian analysis paralleling the likelihood
analysis we did above in \mbox{\tt secr}, using the logit/log
parameterization of the model parameters.  To do so in \bugs, we 
used dummy variables.
Thus, we can express models allowing for sex specificity
using a dummy variable \mbox{\tt Sex} and new parameters
($\alpha_{sex}$, $\beta_{sex}$) which
represent the {\it effect} of \mbox{\tt Sex} at level 1:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \alpha_{sex} \mbox{\tt Sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \beta_{sex} \mbox{\tt Sex}_{i}.
\]

In these expressions, the sex variable $\mbox{\tt Sex}_{i}$ is a
binary variable where $\mbox{\tt Sex}_{i}= 0$ corresponds to female,
and $\mbox{\tt Sex}_{i} = 1$ corresponds to male. 

Unlike the multi-session model in \mbox{\tt secr}, we carry out the
analysis of the sex-specific model here by putting all of the data
into a single data set, and explicitly accounting for the covariate
'sex' in the model by assigning it a Bernoulli prior distribution with
$\psi_{sex}$ being the proportion of males in the population. In this
case, we produce ``Model 0'' above, the model with no sex effect on
density, by setting the population proportion of males at one-half:
$\psi_{sex} = 0.5$ (see also Sec. \ref{covariates.sec.sex}).  As
usual, handling of missing values of the sex variable is done
seamlessly which might be a practical advantage of Bayesian analysis
in situations where sex is difficult to record in the field which may
lead to individuals of unknown sex (i.e., missing values).

The {\bf BUGS} model specification for the most complex model, Model
4, is shown in Panel \ref{gof.panel.sexmodel}.  This model has
sex-specific intercept, scale parameter, $\sigma$, and density.  We
provide an {\bf R} script named \mbox{\tt wolvSCR0ms} in the \mbox{\tt
  scrbook} package which will fit each model.  The function uses {\bf
  JAGS} by default for the fitting, using the \mbox{\tt R2jags}
package.  The kernel of this function is the model specification in
Panel \ref{gof.panel.sexmodel}, which gets modified depending on the
model we wish to fit using a command line option \mbox{\tt model}. For
example, \mbox{\tt model = 1} fits the model with constant parameter
values for males and females, but sex-specific population sizes
(\mbox{\tt model = 0} constrains the male probability parameter,
$\psi_{sex}$, to be $0.5$).  The {\bf R} function fits each of the 5
models using a binary indicator variable to turn `on' or `off' each
effect.  Here is how we obtain the MCMC output for each of the 5
models: {\small
\begin{verbatim}
> wolv0 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=0)
> wolv1 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=1)
> wolv2 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=2)
> wolv3 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=3)
> wolv4 <- wolvSCR0ms(nb=1000,ni=21000,buffer=2,M=200,model=4)
\end{verbatim}
}



\begin{panel}[tp]
\centering
\rule[0.15in]{\textwidth}{.03in}
%\begin{minipage}{5in}
{\small
\begin{verbatim}
alpha.sex ~ dunif(-3,3)            ## Prior distributions 
beta.sex  ~ dunif(-3,3)
sigma0 ~ dunif(0,50)
alpha0 ~ dnorm(0,.1)
psi ~ dunif(0,1)                   ## Data augmentation parameter
psi.sex  ~ dunif(0,1)              ## Probability of ``male''

for(i in 1:M){                     ## DA loop
  wsex[i] ~ dbern(psi.sex)         ## Latent sex state (male = 1)
  z[i] ~ dbern(psi)                ## DA variables, activity centers, etc..
  s[i,1] ~ dunif(Xl,Xu)
  s[i,2] ~ dunif(Yl,Yu)
  logit(p0[i]) <- alpha0 + alpha.sex*wsex[i]
  log(sigma.vec[i]) <- log(sigma0) + beta.sex*wsex[i]
  alpha1[i] <- 1/(2*sigma.vec[i]*sigma.vec[i])
  for(j in 1:ntraps){
    mu[i,j] <- z[i]*p[i,j]
    y[i,j] ~ dbin(mu[i,j],K[j])
    dd[i,j] <- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2)
    p[i,j]  <-  p0[i]*exp( - alpha1[i]*dd[i,j] )
   }
 }
\end{verbatim}
}
%\end{minipage}
\rule[-0.15in]{\textwidth}{.03in}
\caption{
Part of the {\bf BUGS} specification for a complete sex specificity of model
parameters. This is a simplified version of the model contained in the 
\mbox{\tt wolvSCR0ms} script, because it does not contain the on/off
switches for creating the various sub-models. 
}
\label{gof.panel.sexmodel}
\end{panel}


We fitted the 5 models to the wolverine data and summarize
the DIC computation results in Table \ref{gof.tab.DIC}. 
The model rank has model 0, model 2, model 1, model 4, model 3.
Interestingly, this is the same order as the models based on AIC$_c$
which we found above
(see Table \ref{gof.tab.aic}).
The posterior mean and SD of model parameters under the 5 models are
given in Table \ref{gof.tab.dic}. 

\begin{table}[ht]
\centering
\caption{
DIC results for the 5 models of sex specificity fitted to the
wolverine camera trapping data, using the function
\mbox{\tt wolvSCR0ms}. Results are based on 3 chains of length 61000
yielding 180000 posterior samples. 
}
\begin{tabular}{ccccc} \hline \hline
Model      &  Meandev  &  $p_{D}$ &    DIC  &   Rank \\ \hline
Model 0&  441.01 & 77.09&518.10&    1 \\
Model 1& 441.78 &77.504 &519.28&    3\\
Model 2& 440.12 &78.440 &518.56&    2\\
Model 3& 443.31 &79.478 &522.79&    5\\
Model 4& 441.24 &80.078 &521.32&    4\\ \hline
\end{tabular}
\label{gof.tab.DIC}
\end{table}

\begin{table}[ht]
\centering
\caption{
Posterior summaries of model parameters for models with varying
sex specificity of model parameters. Model 0 = no sex specificity,
model 4 = fully sex-specific (see text). Models are based on the 
 Gaussian encounter probability model, each with 21000 iterations,
 1000 burn-in, 3 chains for a total of 60000 posterior samples. }
{\tiny
\begin{tabular}{crrrrrrrrrr} \hline \hline
Parameter & \multicolumn{2}{c}{model 0} &
\multicolumn{2}{c}{model 1} &
\multicolumn{2}{c}{model 2} &
\multicolumn{2}{c}{model 3} &
\multicolumn{2}{c}{model 4}  \\
          &    Mean &   SD &      Mean &   SD &      Mean &   SD &
          Mean&     SD  & Mean & SD \\ \hline
$N$       &  60.02& 11.91&  60.24& 11.93&  59.37& 11.97&  59.67& 11.97&  58.77& 11.75\\
$D$       &   5.79&  1.15&   5.81&  1.15&   5.72&  1.15&   5.75&  1.15&   5.66&  1.13\\
$\alpha_0$&  -2.81&  0.18&  -2.82&  0.17&  -2.44&  0.25&  -2.82&  0.18&  -2.43&  0.25\\
$\alpha_{sex}$ &   0.00&  1.73&   0.00&  1.73&  -0.75&  0.34&   0.00&  1.73&  -0.79&  0.36\\
$\sigma_0$    &  0.64&  0.06&   0.64&  0.05&   0.66&  0.06&   0.65&  0.08&   0.63&  0.09\\
$\beta_{sex} $ &  0.00&  1.73&  -0.01&  1.73&   0.01&  1.74&  -0.01&  0.17&   0.10&  0.18\\
$\psi_{sex}$    & 0.50&  0.29&   0.52&  0.10&   0.56&  0.10&   0.52&  0.11&   0.54&  0.11\\
$\psi$         &0.30&  0.07&   0.30&  0.07&   0.30&  0.07&   0.30&  0.07&   0.30&  0.07\\
deviance   &441.01& 12.42& 441.78& 12.45& 440.12& 12.53& 443.31&
12.61&441.24& 12.66 \\ \hline
& \multicolumn{2}{c}{pD = 77.1} &\multicolumn{2}{c}{pD = 77.5} & \multicolumn{2}{c}{pD = 78.4}& \multicolumn{2}{c}{pD =79.5}  & \multicolumn{2}{c}{pD =80.1}  \\
& \multicolumn{2}{c}{DIC = 518.1} & \multicolumn{2}{c}{DIC = 519.3} &\multicolumn{2}{c}{DIC = 518.6} &    \multicolumn{2}{c}{DIC = 522.8} &\multicolumn{2}{c}{DIC = 521.3} \\ \hline
\end{tabular}
}
%\hline
\label{gof.tab.dic}
\end{table}






\subsection{Bayesian model averaging with indicator variables}

A convenient way to deal with model selection and averaging problems
in Bayesian analysis by MCMC is to use the method of model indicator
variables \citep{kuo_mallick:1998}. Using this approach, we expand the
model to include a set of prescribed models as specific reductions of
a larger model.  This has been demonstrated in some specific
capture-recapture models in \citet[][Sec. 3.4.3]{royle_dorazio:2008},
and \citet{royle:2009} and in the context of SCR by
\citet{tobler_etal:2012}.  A useful aspect of this method is that
model-averaged parameters are produced by default. We emphasize the
need to be careful of reporting model-averaged parameters that don't
have a common interpretation in the different models because they are
meaningless (averaging apples and oranges....).  For example, if a
regression parameter is in a specific model then the posterior is
informed by the data and a specific MCMC draw is from the appropriate
posterior distribution. On the other hand, if the regression parameter
is not in the model then the MCMC draw is obtained directly from the
prior distribution, and so we need to think carefully about whether it
makes sense to report an average of such a thing (in the vast majority
of cases the answer is no). But some parameters like $N$ or density,
$D$, do have a consistent interpretation and we support producing
model-averaged results of those
parameters. 

To implement the Kuo and Mallick approach, we expand the model to
include the latent indicator variables, say $w_{m}$, for variable $m$
in the model, such that
\begin{eqnarray*}
w_{m} = \left\{
\begin{array}{cl} 1 &  \mbox{ linear predictor includes  covariate $m$} \\
                  0 &  \mbox{ linear predictor does not
                                include covariate $m$}
 \end{array}
\right.
\end{eqnarray*}
We assume that the indicator variables $w_{m}$ are mutually
independent with
\[
w_m \sim \mbox{Bernoulli}(0.5)
\]
for each variable $m=1,2,\ldots,$ in the model. For example,
with 2 variables, the 
expanded model has the linear predictor:
\[
\mbox{logit}(p_{ijk}) = \alpha_{0} + \alpha_{1}w_{1} C_{1,i} + \alpha_{2}w_{2} C_{2,ijk}
\]
where, let's suppose, $C_{1,i}$ is an individual covariate such
as sex, and $C_{2,ijk}$ is a behavioral response covariate which is
individual-, trap-, and occasion-specific.  We can assume a parallel
model specification on the parameter $\sigma$ which is liable to vary
by individual level covariates such as sex:
\[
 \log(\sigma_{i}) = \beta_{0} + \beta_{1} w_{3} C_{1,i}.
\]

Using this indicator variable formulation of the model selection
problem we can characterize unique models by the sequence of $w$
variables. In this case, each unique sequence $(w_{1},w_{2},w_{3})$
represents a model, and we can tabulate the posterior frequencies of
each model by post-processing the MCMC histories of
$(w_{1},w_{2},w_{3})$, as we demonstrate shortly. This method then 
evaluates all possible combinations of covariates or $2^m$ models.

Conceptually, analysis of this expanded model within the data
augmentation framework does not pose any additional difficulty. One
broader, technical consideration is that posterior model probabilities
are well known to be sensitive to priors on parameters
\citep{aitkin:1991, link_barker:2006}. See also 
\citet[][Sec. 3.4.3]{royle_dorazio:2008} and
\citet[][Sec. 7.2.5]{link_barker:2010}.  What might normally be viewed
as vague or non-informative priors, are not usually innocuous or
uninformative when evaluating posterior model probabilities. The use
of AIC seems to avoid this problem largely by imposing a specific and
perhaps undesirable prior that is a function of the sample size
\citep{kadane_lazar:2004}. One solution is to compute posterior model
probabilities under a model in which the prior for parameters is fixed
at the posterior distribution under the full model
\citep{aitkin:1991}. At a minimum, one should evaluate the sensitivity
of posterior model probabilities to different prior specifications.


\subsubsection{Analysis of the wolverine data}

The {\bf R} script \mbox{\tt wolvSCR0ms} in the package \mbox{\tt scrbook}
provides the model indicator variable implementation for the fully
sex-specific SCR model.  It is run by setting \mbox{\tt model=5} in
the function call. We note again that it is not very useful to report most
parameter estimates from this model because their marginal posterior
is a mixture from the prior (when a value of the indicator variable of
0 is sampled) and draws informed by the data (i.e., from the
posterior, when a 1 is drawn for the indicator variable $w$).
 On the other hand, the parameters
$N$ and density $D$ should be reported and they represent marginal
posteriors over all models in the model set. In effect, model
averaging is done as part of the MCMC sampling.  The variable `mod'
contains the two binary indicator variables ($w$ above) which
pre-multiply the 'sex' term in each of the $p_{0}$ and $\sigma$ model
components, like this:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \mbox{\tt mod}[1] \alpha_{sex} \mbox{\tt sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \mbox{\tt mod}[2] \beta_{sex} \mbox{\tt sex}_{i}
\]
The third element of \mbox{\tt mod} determines whether the
$\psi_{sex}$ parameter is estimated or fixed at $\psi_{sex} =
0.5$ which is accomplished with the line of {\bf BUGS} code as
follows:
\newline 
\mbox{\tt sex.ratio <- psi.sex*mod[3] + .5*(1-mod[3])}.
\newline
The MCMC output for `mod' was post-processed to obtain the
model-weights using the following  {\bf R} commands:
\begin{verbatim}
>  mod <- wolv5$BUGSoutput$sims.list$mod
>  mod <- paste(mod[,1],mod[,2],mod[,3],sep="")
>
>  table(mod)
mod
  000   001   010   011   100   101   110   111
17181  4935  1057   296 25211  8337  2275   708

> round( table(mod)/length(mod) , 3)
mod
  000   001   010   011   100   101   110   111
0.286 0.082 0.018 0.005 0.420 0.139 0.038 0.012
\end{verbatim}
This results in a comparison of all 8 possible models (based on $m=3$ covariates) instead 
of just the 5 models we originally proposed. 
We see that the best model is that labeled \mbox{\tt 100} which,
according to our construction above, has \mbox{\tt mod[1]=1},
\mbox{\tt mod[2]=0} and \mbox{\tt mod[3]=0}. This is the model 
having sex-specific baseline
encounter probability $p_{0}$, and $\psi_{sex} = 0.5$. This model has 
posterior model probability $0.420$. The model with no sex specificity
at all (the model with label \mbox{\tt 000}) has
posterior probability $0.286$ and the remaining posterior mass is
distributed over the other six models. We could arrive at a
qualitatively similar conclusion using a more ad hoc approach based
on looking at the posterior mass for each parameter under the
full model (model 4; see Table \ref{gof.tab.dic}, in part). Considering
the sex-specific intercept, it appears to be very important as its
posterior mass is mostly away from 0.  On the other hand, the
coefficient on log-sigma is concentrated around 0, and the estimated
$\psi_{sex}$ (probability that an individual is a male) is $0.54$ with
a large posterior standard deviation.  We might therefore be inclined
to discard the sex effect on $\log(\sigma)$ based on classical
thinking-like-a-hypothesis-testing-person and settle for the model with a
sex-specific intercept in the encounter probability model. This is consistent with our indicator variable
approach which found that model (1,0,0) has posterior probability of
0.420. Looking at the
posteriors for each parameter to thin the model down is consistent with these results.  
We can obtain model-averaged estimates
from the indicator variable approach, which produces direct
model-averaged estimates of $N$ and $D$:
{\small
\begin{verbatim}
   mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
D    5.695   1.133   3.759   4.916   5.591   6.362   8.193 1.002  3600
N   59.077  11.758  39.000  51.000  58.000  66.000  85.000 1.002  3600
\end{verbatim}
}
We obtain a model-averaged estimate (posterior mean) for density of $D=5.695$
which is hardly any different from our
model specific estimates (Table \ref{gof.tab.dic}) and, in particular, from model 2
which has only a sex-specific intercept.


\subsection{Choosing among detection functions}

Another approach to implementing model indicator variables is to
introduce a categorical ``model identity'' variable which is itself a
parameter of the model. Using this approach, then each distinct model
is associated with a unique set of covariates or other set of model
features. This is convenient especially when we cannot specify the
linear predictor as some general model that reduces to various
alternative sub-models simply by switching binary variables on or
off. In the context of SCR models, choosing among different encounter
probability models would be an example.  For this case we do something
like this \mbox{\tt mod $\sim$ dcat(probs[])} where \mbox{\tt probs}
is a vector with elements $1/(\# models)$, and the encounter
probability matrix is filled in depending on the value of \mbox{\tt
  mod}.  In particular, instead of a 2-dimensional array \mbox{\tt
  p[i,j]}, we build \mbox{\tt p[i,j,m]} for each of $m=1,2,\ldots,M$
models. An example with 3 distinct models is: 
{\small
\begin{verbatim}
  mod  ~ dcat(probs[])
##
## Using a double loop construction fill-in p[,,] for each model:
##
  p[i,j,1] <-  p0[1]*exp( - alpha1[1]*dist2[i,j] )
  p[i,j,2] <-  1-exp(-p0[2]*exp( - alpha1[2]*dist2[i,j] ) )
  logit(p[i,j,3]) <- p0[3] - alpha1[3]*dist2[i,j]

  mu[i,j] <- z[i]*p[i,j,mod]
  y[i,j] ~ dbin(mu[i,j],K[j])
\end{verbatim}
}

As before the posterior probabilities can be highly sensitive to
priors on the different model parameters and sometimes mixing is
really poor and, in general, we've experienced mixed success trying to
carry out model selection using this construction.  We do provide a
template {\bf R}/{\bf JAGS} script (\mbox{\tt wolvSCR0ms2}) in the
\mbox{\tt scrbook} package which has an example of choosing among 3
different encounter probability models: The Gaussian encounter
probability, Gaussian hazard, and logistic model with the square of
distance (defined in Sec. \ref{scr0.sec.binomial}). The key things to
note are that there are 3 intercepts and 3 different `\mbox{\tt
  alpha1}' parameters (the coefficient on distance). The parameters
should not be regarded as equivalent across the models, so it is
important to have them separately defined (and estimated) for each
model.  In our analysis we used a vague normal prior (precision = 0.1)
for the intercept parameter (either log or logit-scale of baseline
encounter probability $p_{0}$) and a \mbox{\tt Uniform}(0,5) prior for
one-half the inverse of the coefficient on distance-squared.  In the
{\bf BUGS} model specification the priors look like this:
\begin{verbatim}
 for(i in 1:3){
   alpha0[i] ~ dnorm(0,.1)
   sigma[i] ~ dunif(0,5)
   alpha1[i] <- 1/(2*sigma[i]*sigma[i])
 }
\end{verbatim}
Then,  we create a probability of encounter for each
individual, trap {\it and} model so that the holder object ``\mbox{\tt p}'' in the
model description is a 3-dimensional array (sometimes this would have to be a 4
or 5-d array in more complex models with time effects, etc..), so that
construction of the encounter probability models look like this:
\begin{verbatim}
 p[i,j,1] <-  p0[1]*exp( - alpha1[1]*dist2[i,j] )
 p[i,j,2] <-  1-exp(-p0[2]*exp( - alpha1[2]*dist2[i,j] ) )
 logit(p[i,j,3]) <- p0[3] - alpha1[3]*dist2[i,j]
\end{verbatim}
where
\begin{verbatim}
 logit(p0[1]) <- alpha0[1]
   log(p0[2]) <- alpha0[2]
       p0[3]  <- alpha0[3]
\end{verbatim}
You can experiment with the \mbox{\tt wolvSCR0ms2} script to
investigate the importance of different models of encounter
probability and whether they have an affect on the inferences.








\section{Evaluating Goodness-of-Fit}

In practical settings, we estimate parameters of a desirable model, or
maybe fit a bunch of models and report estimates from all of them or a
model-averaged summary of density.  An important question is: Is our
model worth anything?  In other words, does the model appear to be an
adequate description of our data?
Formal assessment of model adequacy or goodness-of-fit is a
challenging problem and there are no all-purpose algorithms for doing
this in either frequentist or Bayesian paradigms. Moreover, there are
some philosophical challenges to evaluating model fit, such as, if we
do model averaging then should all of the models have to fit? Or
should the averaged model have to fit? What if none of the models fit?
We don't know the answers to these questions and we won't try to
answer them. Instead, we will provide what guidance we can on taking
the first steps to evaluating fit, of a single model, as if it were a
cherished family heirloom of great importance.  We suggest that if you
have a model that you really like, a single model, then it is a
sensible thing to check that the model is a good fit to your data. If
it is not, we do not imagine that the model is useless but just that
some thought should be put into why the model doesn't fit so that,
perhaps, some remediation might happen as future data are
collected. After all, you may have spent 2, 3 or many more years of
your life collecting that data set, perhaps thousands of hours, and
therefore it seems a reasonable proposition to expect to do some
estimation and analysis of the model regardless of model fit. You can
still learn something from a model that does not pass some technical
litmus test of model fit.


Conceptually, we can think of evaluation of model fit as follows: if
we simulate data under the model in question, do the simulated
realizations resemble the data set that we actually have?  For either
Bayesian or classical inference, the basic strategy to assessing model
fit is to come up with a fit statistic that depends on the parameters
and the data set, which we denote by $T({\bf y}, \theta)$, and then we
compute this for the observed data set, and compare its value to that
computed for perfect data sets simulated under the correct model.  In
the case of classical inference, we will often rely on the standard
practice of parametric bootstrapping \citep{dixon:2002}, where we
simulate data sets conditional on the MLE $\hat{\theta}$ and compare
realizations with what we've observed.  The {\bf R} package \mbox{\tt
  unmarked} \citep{fiske_chandler:2011} contains generic bootstrapping
methods for certain hierarchical models, including distance sampling
\citep[e.g., see][for an application]{sillett_etal:2012}.  In simple
cases, using classical inference methods, it is sometimes possible to
identify a test statistic of theoretical merit, perhaps with a known
asymptotic distribution.  For examples from capture-recapture see
\citet{burnham_etal:1987}, \citet{lebreton_etal:1992}, and Chapt. 5 of
  \citet{cooch_white:2006}.  For Bayesian analysis we use the Bayesian
  p-value method \citep{gelman_etal:1996} (we introduced the Bayesian
  p-value in sec. \ref{glms.sec.gof}).  Using this approach, data sets
  are simulated based on a posterior sample of the model parameters
  $\theta$ and some fit statistic for the simulated data sets, usually
  based on the discrepancy of the observed data from its expected
  values, is compared to that for the actual data.  In most cases,
  whether Bayesian or frequentist, the main idea for assessing model
  fit is the same: We compare data sets from the model we're
  interested in with the data set we have in hand. If they appear to
  be consistent with one another, then our faith in the model
  increases, at least to some extent, and we say ``the model fits.''


To date, we are unaware of any goodness-of-fit applications based on
likelihood analysis of SCR models. For 
Bayesian analysis of SCR models, there has not been a definitive or
general proposal for a fit statistic or even a class of fit
statistics, although a few specialized implementations of Bayesian
p-values have been provided \citep{royle:2009, gardner_etal:2010ecol, royle_etal:2011mee,
  gopalaswamy_etal:2012mee,gopalaswamy_etal:2012ecol,russell_etal:2012}.
While we universally adopt the Bayesian p-value approach, and suggest
some fit statistics in the following text, we caution that there is
no general expectation to support how well they should do.
As such, one might consider doing some kind of custom
evaluation or calibration when using such methods, if the power of the
test (ability to reject under specific departures from the model) is
of paramount interest.  We note that this uncertain power or
performance of the Bayesian p-value is not a weakness of the Bayesian
approach because the same issue applies in using bootstrap approaches
applied to classical analysis of models, if we were to devise such
methods.



\section{The Two Components of Model Fit}

For most SCR models, there are at least two distinct components of
model fit, and we propose to evaluate these two distinct components
individually.  First, we can ask, are the data consistent with the
 {\it
  observation} model, conditional on the underlying point process?
We can evaluate this based on the encounter frequencies of individuals
{\it conditional} on (posterior samples of) the underlying point
process ${\bf s}_{1}, \ldots, {\bf s}_{N}$.  We discuss some potential
fit statistics for addressing this in the next section.  Second, we
can evaluate whether the data appear consistent with the
{\it state} process model (i.e., the ``uniformity'' assumption of 
the point process).  For the simple
model of independence and uniformity, this is similar to the
assumption of {\it complete spatial randomness} (CSR) which we
consider in Sec. \ref{gof.sec.csr} below. Actually, this is not
strictly the assumption of CSR because of the binomial assumption on
$N$ under data augmentation, so we instead use the term {\it spatial
  randomness}.



\subsection{Testing uniformity or spatial randomness}
\label{gof.sec.csr}


Historically, especially in ecology, there has been an extraordinary
amount of interest in whether a realization of a point process
indicates ``complete spatial randomness,'' i.e., that the points are
distributed uniformly and independently in space.  Two good references
for such things are \citet[][Ch. 8]{cressie:1992} and
\citet{illian_etal:2008}\footnote{We also like Tony Smith's lecture
  notes (Univ. of Penn. ESE 502), which can be found at
  \url{http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf},
accessed January 24, 2013.}. In the context of animal capture-recapture studies, the
spatial randomness hypothesis is manifestly false, purely on
biological grounds. Typically individuals will be clustered, or more
regular (for territorial species), than expected under spatial
randomness and heterogeneous habitat will generate the appearance of
clustering even if individuals are distributed independently of one
another. While we recommend modeling spatial structure explicitly when
possible (Chapts. \ref{chapt.state-space}, \ref{chapt.ecoldist},
\ref{chapt.rsf}), the uniformity assumption may be an adequate
description of data sets in some situations. Further, we find that it
is generally flexible enough to reflect non-uniform patterns in the
data, because we do observe some direct information about some of the point locations.


The basic technical framework for evaluating the spatial randomness
hypothesis is based on counts of activity centers in cells or bins.
For that we use any standard goodness-of-fit test statistic, based on
griding (i.e., binning) the state-space of the point process into
$g=1,2,\ldots,G$ cells or bins, and we tabulate $N_{g} \equiv N({\bf
  x}_{g})$ the number of activity centers in bin $g$, centered at
coordinate ${\bf x}_{g}$.  Specifically, let $B({\bf x})$ indicate a
bin centered at coordinate ${\bf x}$, then\footnote{$I(arg)$ is the
  indicator function which evaluates to 1 if $arg$ is true, otherwise
  0} $N({\bf x})=\sum_{i=1}^{N} I({\bf s}_{i} \in B({\bf x}))$ is the
population size of bin $B({\bf x})$.  In Sec. \ref{scr0.sec.mapping},
we used the summaries $N({\bf x})$ for producing density maps from
MCMC output. Here, we use them for constructing a fit statistic.  We
have used the Freeman-Tukey statistic of this form:
\[
T({\bf N}, \theta) =  \sum_{g}  (\sqrt{N_{g}} - \sqrt{\mathbb{E}(N_{g})})^{2}
\]
where $\mathbb{E}(N_{g})$ is estimated by the mean bin count.  An
alternative conventional assessment of fit is based on the following
statistic: Conditional on $N$, the total number of activity centers in
the state-space ${\cal S}$, the bin counts $N_{g}$ should have a
binomial distribution.  It will usually suffice to approximate the
binomial cell counts by Poisson cell counts, in which case we can use
the classical ``index-of-dispersion'' test
\citep[][p. 87]{illian_etal:2008}, based on the variance-to-mean ratio:
\[
   ID =  (G -1)*s^2/\bar{N}
\]
where $s^{2}$ is the sample variance of the bin counts and $\bar{N}$
is the sample mean. When the point process realization is {\it
  observed}, as in classical point pattern modeling (but not in SCR),
this statistic has approximately a Chi-square distribution on $(G- 1)$ 
degrees-of-freedom under the spatial randomness
hypothesis.  If $s^2/\bar{N} > 1$, clustering is suggested whereas,
$s^2/\bar{N} <1$ suggests the point process is too regular.


Whatever statistic we choose as our basis for assessing spatial
randomness, {\it the} important technical issue is that we don't
observe the point process and so the standard statistics for
evaluating spatial randomness cannot be computed directly.  However,
using Bayesian analysis, we do have a posterior sample of the
underlying point process and so we suggest computing the posterior
distribution of any statistic in a Bayesian p-value framework.
For a given
posterior draw of all model parameters, $N$ is known, based on the
value of the data augmentation variables $z_{i}$, and so we can obtain
a posterior sample of $N({\bf x})$ by taking all of the output for
MCMC iterations $m=1,2,\ldots,$ and doing this:
\[
   N({\bf x})^{(m)} = \sum_{z_{i}^{(m)}=1} I({\bf s}_{i}^{(m)} \in B({\bf x}))
\]
Thus, $N({\bf x})^{(1)}, N({\bf x})^{(2)}, \ldots,$ is the Markov
chain for the derived parameter $N({\bf x})$.

In addition to computing the bin counts for each iteration of the MCMC
algorithm, at the same time we generate a realization of the activity
centers ${\bf s}_{i}$ under the spatial randomness model, and we
obtain bin counts for these ``new'' data, $\tilde{N}({\bf x})$. For each of
the posterior samples -- that of the real data, and that of the
posterior simulated data, we compute the fit-statistic. The fit 
statistic based on the actual data is:
\[
T({\bf N},\theta) = \sum_{x}  (\sqrt{N({\bf x})} - \sqrt{ \bar{N}({\bf x})})^2
\]
whereas the fit statistic based on a simulated realization of points under the
spatial randomness hypothesis is:
\[
T(\tilde{\bf N},\theta) = \sum_{x}  (\sqrt{\tilde{N}({\bf x})} - \sqrt{
  \bar{N}({\bf x})})^2
\]
And we compute the Bayesian p-value by tallying up the proportion of
times that $T(\tilde{\bf N},\theta)$ is larger than $T({\bf
  N},\theta)$, as an estimate of: $p = \Pr(T(\tilde{\bf N},\theta) >
T({\bf N},\theta))$.  The {\bf R} function {\tt SCRgof} in our package
\mbox{\tt scrbook} will do this, given the output from {\bf JAGS} (see
below).



\subsubsection{Sensitivity to bin size}

Evaluating fit based on bin counts in point process models are
sensitive to the number of bins \citep[][p. 87-88]{illian_etal:2008}.
This is related to the classical problem of fit testing for binary
regression because in a point process model, as the number of grid
cells gets small, the grid cell counts go to 0 or 1 and standard fit
statistics (e.g., based on deviance or Pearson residuals) are known
not to be very useful.  There is some good discussion of this in
\citet[][Sec. 4.4.5]{mccullagh_nelder:1989}.  What it boils down to
is, using the example of the Pearson residual statistic considered by
\citet{mccullagh_nelder:1989}, the fit statistic is exactly a
deterministic function of the sample size only, which clearly should
not be regarded as useful for model fit. This is why, in order to do a
check of model fit when you have a binary response, one must always
aggregate the data in some fashion.  In the context of testing spatial
randomness, computing the test statistic we described above has us
chop up the region ${\cal S}$ into bins, and tally up $N_{g}$, the
frequency of activity centers in each bin $g$.  Suppose that we choose
the bin size to be extremely small such that $\mathbb{E}(N_{g})$ tends
to $N/G$ ($N$ being the number of activity centers).  Further, $N_{g}$
tends to a binary outcome. Therefore the fit statistic has $N$
components that have value $N_{g} = 1$, and it has $G-N$ components
that have value $N_{g} = 0$. Therefore, the fit statistic resembles:
\[
T({\bf N},\theta) = \sum_{g \ni N_{g} = 1}^{N}  (1 - \sqrt{N/G})^2 +
\sum_{g \ni N_{g} = 0}^{G-N} (N/G)^2
 = N(1 + (G-N)/G)
\]
(here $\ni$ means ``such that''). If $G$ is huge relative to $N$, then
we see that this tends to about $2*N$, which does not provide any
meaningful assessment of model fit.  So if you look at this in the
limit in which the bin counts become binary, the fit statistic loses
all its variability to the specific model used and is just a
deterministic function of $N$. As a practical matter, it probably
makes sense to restrict the number of bins to {\it fewer} than the
number of observed individuals in the sample size. In typical SCR
applications this will therefore result, usually, in very large (and
few) bins, and presumably not much power.


There are some extensions that help resolve the issue of sensitivity
to bin size. We can construct fit statistics based not just on quadrat
counts but also the neighboring quadrat counts -- this is the
Greig-Smith method \citep{greig-smith:1964}.  In addition, there are a
myriad of ``distance methods'' for evaluating point process models,
and we believe that many of these can (and will) be adapted to SCR
models.  Again the main feature is that the point process on which
inference is focused is completely latent in SCR models -- so this
makes the fit assessment slightly different than in classical point
processes. That said, the methods should be adaptable, e.g., in a
Bayesian p-value kind of way.


\subsubsection{Sensitivity to state-space extent}

An issue that we have not investigated is that any model assessment
that applies to a {\it latent} point process is probably sensitive to
the size of the state-space. As the size of the state-space increases
then the cell counts (far away from the data) {\it are} independent
binomial counts with constant density, and so we can overwhelm the fit
statistic with extraneous ``data'' simulated from the posterior, which
is equal to the prior as we move away from the data, and therefore
uninformed by the observed data  that live in the vicinity of the trap array.
Therefore we recommend computing these goodness-of-fit statistics in the vicinity
of the trap array only. Perhaps, as an ad hoc rule-of-thumb, less than the
average trap spacing from the rectangle enclosing the trap array.
For example, if the average trap spacing is, say,
10 km, then the bins used to obtain the observed and predicted
activity centers should not extend any further from the traps than 5
km. This should be a matter of future research.





\subsection{Assessing  fit of the observation model}
\label{gof.sec.obsfit}

In evaluating the spatial randomness hypothesis, we could draw on
well-established ideas from point process modeling. On the other hand,
it is less clear how to approach goodness-of-fit evaluation of the
observation model.  For most SCR problems, we have a 3-dimensional
data array of {\it binary} observations, $y_{ijk}$ for individual $i$,
trap $j$ and sample occasion $k$. As discussed in the previous
section, we need to construct fit statistics based on observed and
expected frequencies that are aggregated in some fashion.  In
practice, the data will be too sparse to have much power, unless the
data are highly aggregated. We recommend focusing on summary
statistics that represent aggregated versions of $y_{ijk}$ over 1 or 2
of the dimensions. We describe 3 such fit statistics below.  We
recognize that, depending on the model, some information about model
fit will be lost by summarizing the data in this way. For example if
there is a behavioral response and we aggregate over time to focus on
the individual and trap level summaries then some information about
lack of fit due to temporal structure in the data is lost.

{\bf Fit statistic 1: individual x trap frequencies} We summarize the
data by individual and trap-specific counts $y_{ijk}$ aggregated over
all sample occasions. Using standard ``dot notation'' to represent
summed quantities, we express that as: $y_{ij.} = \sum_{k=1}^{K}
y_{ijk}$.  Conditional on ${\bf s}_{i}$, the expected value under any
encounter model is:
\[
 \mathbb{E}(y_{ij.}) = p_{ij} K
\]
(or $K_{j}$ if the traps are operational for variable periods). If
there is time-varying structure to the model, then expected values
would have to be computed according to $\mathbb{E}(y_{ij.}) = \sum_{k} p_{ijk}$.
Then we can define a fit statistic from the Freeman-Tukey residuals
according to:
\[
 T_{1}({\bf y}, \theta) = \sum_{i}\sum_{j} (\sqrt{ y_{ij.} } - \sqrt{ \mathbb{E}(y_{ij.}) })^2
\]
where we use $\theta$ here to represent the collection of all
parameters in the model.  This is conditional on ${\bf s}$ as well as
on the data augmentation variables ${\bf z}$. We compute this
statistic for {\it each} iteration of the MCMC algorithm for the
observed data set and also for a new data set simulated from the posterior
distribution, say $\tilde{\bf y}$.

We could also use a similar fit statistic derived from summarizing
over traps to obtain an \mbox{\tt nind} $\times$ $K$ matrix of count
statistics.  We imagine that either summary of the data will probably
be too disaggregated (have mostly values of 0) in most practical
settings to have much power.


{\bf Fit statistic 2: Individual encounter frequencies. } SCR models
represent a type of model for heterogeneous encounter probability,
like model $M_h$, but with an explicit factor (space) that explains
part of the heterogeneity. For model $M_h$, the individual encounter
frequencies are the sufficient statistic for model parameters, and so
it makes intuitive sense to provide some kind of omnibus fit
assessment of the core heuristic that SCR model is adequately
explaining the heterogeneity using a model $M_h$-like statistic based
on individual encounter frequencies.  So, we build a fit statistic
based on the individual total encounters \citep{russell_etal:2012},
$y_{i..} = \sum_{j} \sum_{k} y_{ijk}$. In addition, the expected value
is a similar summary over traps and occasions: $\mathbb{E}(y_{i..}) =
\sum_{j} \sum_{k} p_{ijk}$. Then, we define statistic $T_{2}$
according to:
\[
 T_{2}({\bf y}, \theta) = \sum_{i} (\sqrt{ y_{i..} } - \sqrt{ \mathbb{E}(y_{i..}) })^2
\]
We imagine this test statistic should provide an omnibus test of
extra-binomial variation and should therefore capture some effect of
variable exposure to encounter of individuals, although we have not
carried out any evaluations of power under specific alternatives.
Obviously, in using this statistic, we lose information on departures
from the model that might only be trap- or time-specific.


{\bf Fit Statistic 3: Trap frequencies. } We construct an analogous
statistic based on aggregating over individuals and replicates to form
trap encounter frequencies: $y_{.j.} = \sum_{i} \sum_{k} y_{ijk}$
\citep{gopalaswamy_etal:2012ecol} and the expected value is a similar
summary over individuals and occasions: $\mathbb{E}(y_{.j.}) = \sum_{i}
\sum_{k} p_{ijk}$.  Then statistic $T_{3}$ is:
\[
 T_{3}({\bf y}, \theta) = \sum_{j} (\sqrt{ y_{.j.} } - \sqrt{ \mathbb{E}(y_{.j.}) })^2
\]
This seems like a sensible fit statistic because we can think of SCR
models as spatial models for counts
\citep{chandler_royle:2012}. Therefore, we should seek models that
provide good predictions of the observable spatial data, which are the
trap totals.  In this context, it might even make sense to pursue
cross-validation based methods for model selection.  Cross-validation
is a standard method of evaluating models such as in kriging or spline
smoothing, so we could as well develop such ideas based on the
trap-specific frequencies.


\subsection{Does the SCR model fit the wolverine data?}

We use the ideas described in the previous section to evaluate
goodness-of-fit of the SCR model to the wolverine camera trapping data.

We consider first whether the simple model of spatial randomness of
the activity centers is adequate.  We think that the
 encounter model shouldn't have a large  effect on
whether the spatial randomness assumption is adequate or not, so we
fit ``Model 0'' (in which parameters are {\it not} sex-specific)
using an {\bf R} script provided
in the function \mbox{\tt wolvSCR0gof} which will default to fitting
the model in {\bf JAGS}.  This is the same script as \mbox{\tt
  wolvSCR0ms} except that it saves the MCMC output for the activity
centers ${\bf s}$ and the data augmentation variables $z$, which are
required in order to compute the Bayesian p-value test of spatial randomness.

The MCMC output is processed with  the {\bf R} function
\mbox{\tt SCRgof} which
computes the test of spatial randomness based on bin counts, using the 
Bayesian p-value calculation. The function \mbox{\tt SCRgof} requires
 a few things as inputs: (1) the output from a
{\bf BUGS} run (in particular, the activity center coordinates and the
data augmentation variables); (2) the number of bins to create for
computing spatial frequencies of activity centers;  (3) the trap
locations and, (4) the buffer
around the trap array to use in computing the bin counts.  This buffer could be that used
in defining the state-space for the model fitting, but we think it should be
relatively tighter to the trap array than the state-space used in
model-fitting. For the wolverine analysis,  where we're using 10-km grid cells
(1 unit = 10 km) and a 20 km buffer for model fitting, we'll use a
state-space buffer of  0.4 units (4 km) for
computing the fit statistic.
The {\bf R} code to fit the model and obtain the goodness-of-fit
result is as follows:
{\small
\begin{verbatim}
> wolv1 <- wolvSCR0gof(nb=1000,ni=6000,buffer=2,M=200,model=0)

> bugsout <- wolv1$BUGSoutput$sims.list

> traplocs <- wolverine$wtraps[,2:3]
> traplocs[,1] <- traplocs[,1] - min(traplocs[,1])
> traplocs[,2] <- traplocs[,2] - min(traplocs[,2])
> traplocs <- traplocs/10000

> set.seed(2013)   # set seed so Bayesian p-value is the same each time

> SCRgof(bugsout,5,5,traplocs=traplocs,buffer=.4)

Cluster index observed:  1.099822
Cluster index simulated:  1.000453
P-value  index of dispersion:  0.408
P-value2 freeman-tukey:  0.6842667
\end{verbatim}
}
The output produced by \mbox{\tt SCRgof} is the index of dispersion
based on the 
ratio of the variance to the mean
(see above), which is computed as the posterior mean index of
dispersion for the latent point process, and also the average value
for simulated data. If this value is $>1$ then clustering is
suggested, which we see a (very) minor amount of evidence for here. Two
Bayesian p-values are produced: the first is based on the cluster
index, and the 2nd is based on the Freeman-Tukey statistic calculated
as described in Sec. \ref{gof.sec.csr}.  Because our p-values aren't
close to 0 or 1, we judge that the model of spatial randomness
provides an adequate fit to the data. You can verify that a similar result is obtained if
we use the model with fully sex-specific parameters (Model 4).

Next, we did a Bayesian p-value analysis of the observation component
of the model, using the 3 fit statistics
described
in Sec. \ref{gof.sec.obsfit}.
These statistics can be calculated as part of
the {\bf BUGS} model specification or by post-processing the MCMC
output returned from a {\bf BUGS} run.
The {\bf R} script \mbox{\tt wolvSCR0gof} contains the relevant
calculations.  For example, to compute fit statistic 1, we have to add
some commands to the {\bf
  BUGS} model specification such as this (note: this is only a
fraction of the model specification):
\begin{verbatim}
.......
for(j in 1:ntraps){
   mu[i,j] <- w[i]*p[i,j]

   y[i,j] ~ dbin(mu[i,j],K[j])
   ynew[i,j] ~ dbin(mu[i,j],K[j])

   err[i,j] <-  pow(pow(y[i,j],.5) - pow(K[j]*mu[i,j],.5),2)
   errnew[i,j] <- pow(pow(ynew[i,j],.5) - pow(K[j]*mu[i,j],.5),2)
}

T1obs <- sum(err[,])
T1new <- sum(errnew[,])
.......
\end{verbatim}
Similar calculations are carried out to obtain the posterior samples
of test statistics 2 (individual totals) and 3 (trap totals). For the
wolverine data, the Bayesian p-value calculations produce:
{\small
\begin{verbatim}
> mean(wolv1$BUGSoutput$sims.list$T1new>wolv1$BUGSoutput$sims.list$T1obs)
[1] 0

> mean(wolv1$BUGSoutput$sims.list$T2new>wolv1$BUGSoutput$sims.list$T2obs)
[1] 0.17

> mean(wolv1$BUGSoutput$sims.list$T3new>wolv1$BUGSoutput$sims.list$T3obs)
[1] 0.02066667
\end{verbatim}
} 
Based on statistic $T_2$, we might conclude that the model is
adequate for explaining individual heterogeneity although the other
two statistics suggest a general lack of fit of the observation model.
A similar result is obtained using the fully sex-specific model.  We
note that one individual was captured 8 times in one trap, which is
pretty extreme under a model which assumes independent Bernoulli
trials. We summarize that the trap-counts simply are not
well-explained by this model.

In attempt to resolve this problem, we extended the model to include a
local (trap-specific)
behavioral response (following \citet{royle_etal:2011jwm}) which can
be fitted using the sample {\bf R} script \mbox{\tt
  wolvSCRMb}. 
To fit a model using {\bf WinBUGS}, and then compute the
Bayesian p-values we do this:
\begin{verbatim}
> wolv.Mb <- wolvSCRMb(nb=1000,ni=6000,buffer=2,M=200)

> mean(wolv.Mb$sims.list$T1new>wolv.Mb$sims.list$T1obs)
[1] 0.9666667

> mean(wolv.Mb$sims.list$T2new>wolv.Mb$sims.list$T2obs)
[1] 0.3644667

> mean(wolv.Mb$sims.list$T3new>wolv.Mb$sims.list$T3obs)
[1] 0.4990667
\end{verbatim}
Given that this model seems to fit better, we might prefer reporting
estimates under this model, which we do in Table \ref{gof.tab.wolvMb}.
(the behavioral response parameter is labeled $\alpha_2$ in the
table).  Estimated density is about 1 individual higher per 1000
km$^2$ compared with the various models that lack a behavioral
response.  It might be useful to try these fit assessment exercises
using the habitat mask as described in
Sec. \ref{scr0.sec.discrete}. That takes an extremely long time to run
in \mbox{\bf BUGS} though, especially for the behavioral response
model.



\begin{table}[ht]
\centering
\caption{
Posterior summary statistics for local (trap-specific) behavioral
response model $M_{b}$ fitted to the wolverine camera trapping data
using {\bf WinBUGS}. The parameter $\alpha_{2}$ is the local
(trap-specific) behavioral
response parameter. $T_{x}()$ are the posterior summaries of fit
statistics $x=1,2,3$ used in the Bayesian p-value analysis (See text for
definitions). Results are based on
 3 chains, each with 6000 iterations (first 1000 discarded) for a
 total of 15000 posterior samples. 
}
\begin{tabular}{lrrrrrrr} \hline \hline 
Parameter   & Mean  & SD  & 2.5\% & 50\% & 97.5\% & Rhat &n.eff \\ \hline
$N$          & 71.32 &19.07 &42.00 &69.00 &114.02 &1.00  &2100 \\
$D$          &  6.87 & 1.84 & 4.05 & 6.65 & 10.99 &1.00  &2100\\ \hline
$\sigma$    &  0.88 & 0.13 & 0.68 & 0.86 & 1.17  &1.00  & 730 \\
$p_0$        &  0.01 & 0.00 & 0.01 & 0.01 & 0.02  &1.01  & 530\\
$\alpha_1$   &  0.69 & 0.19 & 0.37 & 0.67 &  1.10 &1.00  & 730\\
$\alpha_2$   &  2.50 & 0.27 & 1.99 & 2.50 &  3.04 &1.00  & 700\\
$\psi$       &  0.36 & 0.10 & 0.20 & 0.35 & 0.58  &1.00  &2600  \\
$T_{1}^{obs}$  & 54.71 & 6.12 &43.69 &54.39 & 67.47 &1.00  &3900\\
$T_{1}^{new}$  & 64.73 & 7.62 &50.93 &64.39 & 80.96 &1.00  &3900\\
$T_{2}^{obs}$  & 13.93 & 4.07 & 7.25 &13.53 & 23.04 &1.00  &5700\\
$T_{2}^{new}$  & 12.65 & 3.35 & 6.93 &12.36 & 20.07 &1.00  &2000\\
$T_{3}^{obs}$  & 12.80 & 1.74 & 9.80 &12.64 & 16.61 &1.00  &2400\\
$T_{3}^{new}$  & 12.94 & 3.05 & 7.77 &12.67 & 19.58 &1.00 &15000\\ \hline
%deviance  1128.02 15.38 1101.00 1127.00  1161.00 1.00   640
\end{tabular}
\label{gof.tab.wolvMb}
\end{table}

\section{Quantifying Lack-of-fit and Remediation}

\citet{kery_etal:inreview} used a strategy for assessing model fit in
dynamic occupancy models \citep{royle_kery:2007} similar to that which
we suggested above.  They constructed a fit statistic based on
aggregating the data over replicate samples ($k$), to obtain the total
detections per site $i$ and year $j$.  They used a Bayesian p-value
analysis based on a Chi-squared test statistic \citep[also
see][Chapt. 12]{kery_schaub:2011}.  Their analysis suggested a model
that didn't fit, and, so they computed the ``lack-of-fit ratio''
\cite[see][Sec. 12.3]{kery_schaub:2011} -- the ratio of the fit
statistic computed for the actual data to that of the replicate data
sets.  They interpret this analogous to the over-dispersion
coefficient in generalized linear models
\citep{mccullagh_nelder:1989}, usually called the c-hat statistic in
capture-recapture literature \citep[see][Chapt. 5]{cooch_white:2006}.
\citet{kery_etal:inreview} reported the lack-of-fit ratio for their
model to be 1.14 which suggests a minor lack-of-fit, compared to
perfect data having a value of 1, because the posterior standard
deviations will be too small by a factor of $\sqrt{1.14} = 1.07$.  In
classical capture-recapture applications of goodness-of-fit
assessment, inference for non-fitting models is dealt with by
inflating the resulting SEs (of the non-fitting model), by the
square-root of c-hat.  We believe that these ideas related to
quantifying lack-of-fit and understanding its effect could also be
applied to SCR models, although we have not yet explored this.




\section{ Summary and Outlook  }



In this chapter, we offered some general strategies for model
selection and model checking, or assessment of model fit.  We think
the strategies we outlined for model selection are fairly standard
and can be effectively applied to many SCR modeling problems.
Some technical issues of Bayesian analysis need to be addressed (in
general) before Bayesian methods are more generally useful and
accessible.  For one thing, Bayesian model selection based on the
indicator variable approach of \citet{kuo_mallick:1998} can be
tediously slow even for small data sets, and so improved computation
will improve our ability to do Bayesian model selection in practical
situations.  Also, and most importantly, sensitivity to prior
distributions is an important issue. Further research and practice
might identify preferred prior configurations for SCR that provide a
good calibration in relevant model selection problems.
Finally, we believe that 
cross-validation should prove to be a useful method in model
assessment and selection, 
as SCR models are a form of spatial model of counts, and so it is
natural to pick models that predict the observable spatial counts
(i.e., at trap locations) well.

For Bayesian model assessment, or goodness-of-fit checking, we
suggested a framework based on independent testing of the spatial
model of independence and uniformity, and testing fit of the
observation model conditional on the underlying point process.  These
ideas are based on mostly {\it ad hoc} attempts in a number of
published applications \citep[e.g.,]{royle_etal:2009ecol,
  royle_etal:2011mee, gopalaswamy_etal:2012ecol, russell_etal:2012}.
While we think this general strategy should be fruitful, we know of no
studies on the power to detect various model departures, and so the
ideas should be viewed as experimental. We have not discussed
assessment of model fit for SCR models using likelihood methods,
although we imagine that standard bootstrapping ideas should be
effective, perhaps based on the fit statistics (or similar ones) we
suggested here for computing Bayesian p-values.

Clearly there is much research to be done on assessment of model fit
in SCR models. For testing the spatial randomness hypothesis, we used
a classical approach based on count frequencies, in which point
locations are put into spatial bins. Other approaches from spatial
point process modeling should be pursued including nearest-neighbor
methods or distance-based methods. In addition, studies to evaluate
the power to detect relevant departures from the standard
assumptions, and the robustness of inferences about $N$ or density,
need to be conducted.  If the spatial randomness model appears
inadequate, 
it is possible to fit models that allow for a non-uniform
distribution of points (see Chapt. \ref{chapt.state-space}) and even
point process models that allow for interactions among points
\citep{reich_etal:2012}. On the other hand, we expect that most of
these Bayesian p-value tests will have low power in typical data
sets consisting of a few to a few dozen individuals. As such, failure
to detect a lack of fit may not be that meaningful. But, on the other
hand, it may not make a difference in terms of density estimates
either.  We think inference about density should be relatively
insensitive to departures from spatial randomness, because we get to
observe direct information on some component of the population,
component of density is {\it observed}. For those activity
centers, the assumed model of the point process should exert little
influence on 
the placement of the activity centers.  Conversely, as is the case
with classical closed population models \citep{otis_etal:1978,dorazio_royle:2003,
  link:2003}, inferences may be somewhat more sensitive to
bad-fitting models for the observation process.













