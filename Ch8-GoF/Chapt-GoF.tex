\chapter{
Model Selection and Assessment
}
\markboth{Model Assessment}{}
\label{chapt.gof}

\vspace{.3in}

Our purpose in life is to analyze models. By that, we mean one 
or more of the following basic 4 tasks: estimate parameters, make
predictions of unobserved random variables (e.g., of density),
evaluate the relative merits of different models or choosing a best
model (model selection), and we check whether a specific model appears
to provide a reasonable description of our data or not (assessment).
In previous chapters we have addressed the problem of estimation
extensively, and also making predictions of latent variables either
${\bf s}$ or density or population size.  In this chapter, we focus on
the last two of these inference tasks: model selection (which model or
models should we favor), and model assessment (do the data appear to
be consistent with a particular model?).

In the context of SCR models we discuss some specific methods of model
selection and assessment, and focus on a few specific application
contexts such as choosing among certain covariates, or other aspects
of model structure.  We discuss the use of AIC and DIC for this
purpose, and also the ``indicator model selection'' approach of
\citet{kuo_mallick:1998}.  To check model adequacy, or whether a specific model provides
a satisfactory description of our data set (i.e., ``goodness-of-fit''), we rely 
exclusively on Bayesian p-values \citep{gelman_etal:2006}.
For checking  adequacy of SCR models, 
part of the challenge is coming up with good summaries of model fit, 
and there does not appear to be any guidance on
this in the literature.  Following \citet{royle_etal:2011mee}, we break the problem up into 2 components
which we attack separately: (1) Does the encounter 
model fit? (2) Does the uniformity assumption appear adequate? The latter
component of model fit has a huge amount of precedent in the ecological literature as it is
analogous to the classical 
problem of testing  ``complete spatial randomness.'' 
% Really this is a test only of a
%form of ``spatial randomness'' because CSR implies the use of a
%Poisson point process which, as we discussed, is not universally
%adopated in our implementation of SCR models.

A basic problem with these two objectives of model selection and model
assessment is their simultaneous use implies a kind of contradiction
which we call the
{\it model selectors paradox}: Inferences are always achieved using
standard paradigms of parametric inference (Bayesian or frequentist)
which asssert that the model is properly specified. That is, we assume
that the model is 
truth. This is paradoxical because we all know that ``all models are
wrong'' but possibly, ``some are useful'' In fact, the notion that an
``assumption'' could even be correct is itself something of an oxymoron.
Therefore we don't expect
or hope to make assumptions that are ``correct'' in any way. Gelman
and Shalizi (2010) say it this way: ``there is general agreement that,
in this domain, all models in use are wrong -- not just merely
falsifiable, but actually false.''
We should therefore refrain from over-stating the relevance of any model. 

In this chapter we develop basic strategies of model selection and
model assessment or checking using both likelihood methods (as
implemented in the \mbox{\tt secr} package) and also Bayesian
analysis.  We apply these methods to the wolverine camera trapping
data to investigate sex specificity of model parameters and whether
there is a behavioral response to encounter. We note that individuals
are drawn to the camera trap devices by food bait and therefore it
stands to reason that once an individual discovers a trap, it might
return subsequently for that benefit, a response commonly referred to
as ``trap happiness.''







\begin{comment}
A few specific questions to resolve:
\begin{itemize}
\item[(1)]  Can AIC or DIC choose among detection models and does it matter?
\item[(2)]  Can AIC or DIC choose among more substantial models?  We should
 assume it does. Can we rig up a test?  Suggest: (1) trap level
 covariate and trying DIC; (2) Model Mb vs. not
\item[(3)] Simultaneous to above, see if we can do that.......
Posterior model probabilities: Can we demo this?   With detection models?
 Don't do much analysis of the situation, just demo it maybe?
\item[(4)]  Basic framework for Bayesian GoF. Power under alternatives.
(clustering, regularity, uniformity) and misspecified encounter models
-- do they show a lack of fit?
\end{itemize}
\end{comment}



\begin{comment}
\subsection{The Role of model assumptions}

Referees are quick to point out that bivariate normal distributions as
models for home range are simply wrong. As if using wrong models
somehow invalidates something. (is that a statistical paradigm that I
missed?). The issues are whether its an adequate model, whether we can
refute it, whether resulting inferences are sensitive to it, and
whether the model is useful for its intended purpose regardless.
If  we really wanted a great description of a home range then we would do
something different besides conduct an SCR study. The purpose of most
SCR models is not to study home range geometry and morphology but, rather,
to estimate density and possibly other vital rates such as survival and
recruitment. In addition, it is my experience that the same people who
criticize models as implying overly simple models actually cannot describe
alternative models except using jargon and procedures like "kernel
smoothing", "utilization distributions" and "neural networks". So, on
the one hand, the bivariate normal distribution is overly simple, and
therefore we should use "neural networks"?  What irritates me even more
is referees who emphatically assert that "real home ranges aren't bivariate
normal" and then they go on to cite references who somehow "proved" they
are not. These people have no clue what statistics is good for and what
the point of SCR models is, nor can they grasp the relevance of the home
range model - namely, as a model for explaining heterogeneity in
capture-probability. To be sure, the bivariate normal model is an
over-simplification but so far it has not been shown to be ineffective
in SCR problems, and besides  substantially more flexible models have been
developed \citep{royle_etal:2012ecol}.

What we care about in models of capture-recapture data is whether the
bivariate normal (or any model) provides an adequate description of the
encounter process. That is the question we will evaluate here.
\end{comment}


\section{General Strategies for Model Selection}

We review a number of standard methods of model selection that apply
to ``variable selection'' problems. That is, when our set of models
consists of distinct covariate effects and they represent constraints
of some larger model achieved by setting some of the parameter values
to 0. 
For classical analysis based on likelihood,  model selection by 
AIC is the standard approach \citep{burnham_anderson:2002}.
For Bayesian analysis we rely on a number of different methods.
We demonstrate the use of DIC here for variable selection problems although we recommend against its
general use (see below).
We use the Kuo and
Mallick indicator variable selection approach
 \citep{kuo_mallick:1998} which, although its implementation in the
 {\bf BUGS} packages can be tempramental, it produces direct statemtns of
 posterior model probabilities which we think are the most useful, and
 leads directly to model-averaged estimates of density.
There is a good review paper recently by \citet{ohara_sillanpaa:2009}
that hits on these and many more related ideas for variable selection.
We have not done a comprehensive evaluation of these different methods
for effect and efficiency.
In addition to \citet{ohara_sillanpaa:2009} we also recommend
\citet[][Chapt. 7]{link_barker:2010}
for general information on model selection and assessment.


\subsection{Scope of the model selection problem}

There are two distinct classes of problems that we encounter in SCR
models which might require some type of model selection or ranking
effort: (1) Choosing among models that represent distinct, meaningful
biological hypotheses; and, (2) choosing among different parametric
encounter probability models. We believe that the importance of model
selection depends on which type of problem we have.

{\flushleft {\bf Choosing among biological models:}}
SCR models that represent extensions of the basic null model by
including specific covariates or other effects often represent
explicit biological hypotheses. Examples include models with a
behavioral response, or seasonal variation in encounter probability,
or sex-specificity of model parameters.
We anticipate that such basic biological factors 
could be important, and therefore it can be useful to choose among (or
rank) a set of models that represent these hypotheses. Indeed, they
might sometimes be of direct interest although usually only
secondary to estimating or modeling density.

{\flushleft {\bf Choosing among models for encounter probability:}}
In sec. \ref{chapt.scr0.implied} we introduced the notion that
encounter probability models imply specific models of space usage, an
idea we expand on and generalize in Chapt. \ref{chapt.rsf}. Because of
this linkage it is tempting to want to choose among them so that we
can proclaim something reasonable about space usage. Our feeling about
choosing among encounter probability models is determined by the
following two considerations:  (1) 
usually trap density is low and so we imagine one would have little or
no power to effectively choose among models and (2) the model itself
is not a biological construct, just the implied home range area is. As
stationary and isotropic models, they are all equally dumb as models
for space usage and so it seems to us that choosing among a dozen or
more arbitrary parametric forms that have no biological motivation
should tend to lead to an over-fitting.
So we will apply ideas of model selection to  some problems below (and
elsewhere in this book) but we avoid the problem of choosing among
detections functions and we discourage people from doing that. 



\subsection{Model selection by AIC}
\label{gof.sec.aic}

If you're doing  classical analysis based on likelihood then model
selection is easily accomplished using AIC \citep{burnham_anderson:2002}
which we demonstrate below. The AIC of a model is simply twice the
negative log-likelihood evaluated at the MLE,  penalized by the number of parameters
($k$) in the model:
\[
 AIC = -2 \mbox{\tt logL}({\bf y}|\hat{\bm \theta})  + 2 k
\]
Models with small values of AIC are preferred. 
It is common to use a modified AIC referred to as $AIC_{c}$ for small
sample sizes which is
\[
 AIC_{c}  = 
-2 \mbox{\tt logL}({\bf y}|\hat{\bm \theta})  + \frac{2 k
  (k+1)}{n-k-1}
\]
where $n$ is the sample size.  Two important problems with the use of
$AIC$ and $AIC_{c}$ are that they don't apply directly to hierarchical
models that contain random effects, unless they are computed directly
from the marginal likelihood (for SCR models we can do this, see
Chapt. \ref{chapt.mle}). Moreover, it is not clear what should be the
effective sample size $n$ in calculation of $AIC_{c}$, as there can be
covariates that affect individuals, that vary over time, or in space.
We do not offer strict guidelines as to when to use a small sample
size adjustment or not.

The {\bf R} package \mbox{\tt secr} computes and outputs AIC
automatically for each model fitted and it provides some capabilities
for producing a model selection table (function \mbox{\tt AIC}) and
also doing model-averaging (function \mbox{\tt model.average}), which
we recommend for obtaining estimates of density from multiple models.
We provide an example of using \mbox{\tt secr}, to analyze the
wolverine camera trapping data using 4 distinct models to accommodate
various types of sex specificity (see sec. XXXXX):
\begin{itemize}
\item[ Model 1:] model SCR0 with constant parameter
values for both male and female wolverines but with a parameter
$\psi_{sex}$ the population proportion of males;
\item[ Model 2:] sex-specific intercept
$p_{0}$ but constant $\sigma$; 
\item[ Model 3:] sex-specific $\sigma$ but constant
$p_{0}$ 
\item[ Model 4:] sex-specific $p_{0}$ {\it and} $\sigma$. 
\end{itemize}
To fit these models we use the multi-session formulation provided by
\mbox{\tt secr} (introduced in sec. \ref{mle.sec.multisession}), which
allows one to model sesssion-specific effects on density, baseline
encounter probability, $g_{0}$ in \mbox{\tt secr}, and also the scale
parameter $\sigma$ of the encounter probability model. Using this
formulation, we define the ``session'' variable to be a sex code and
thus session-specific parameters represent sex-specific parameters.
For example, if we model session-specific density, $D$, then this
corresponds to Model 1 in our list above.  We note that this suggests
one additional model which we haven't itemized above, that being the
model with {\it no} sex effect on any parameter, which is equivalent
to fixing $\psi_{sex} = 0.5$ instead of estimating it. We will label
this last model ``Model 0'' and include it in our analysis below, even
though we may not generally think of this as a natural candidate
model.

Here are the {\bf R} commands for loading the wolverine data and doing
a slight bit of formatting to prepare the data objects for analysis by
\mbox{\secr}. The key difference from our analysis in
Chapt. \ref{chapt.mle} is, here, we grab the wolverine sex information
(\mbox{\tt wolverine\$wsex}) which is a 0/1 indicator (1=male). We add
1 to that and then use it to define the ``Session'' variable based on sex.
The {\bf R} commands are as follows:
{\small
\begin{verbatim}
library("secr")
library("scrbook")
data("wolverine")
traps<-as.matrix(wolverine$wtraps)   
dimnames(traps)<-list(NULL,c("trapID","x","y",paste("day",1:165,sep="")))

trapfile2<-scr2secr(scrtraps=traps,type="proximity")

gr<-as.matrix(wolverine$grid2)
dimnames(gr)<-list(NULL,c("x","y"))
gr2<-read.mask(data=gr)

wolv.dat<-wolverine$wcaps
dimnames(wolv.dat)<-list(NULL,c("Session","ID","Occasion","trapID"))
wolv.dat[,1]<-wolverine$wsex[wolv.dat[,2]]+1
wolv.dat<-as.data.frame(wolv.dat)
wolvcapt3<-make.capthist(wolv.dat,trapfile2,fmt="trapID",noccasions=165)
\end{verbatim}
}


Once the data are have been prepared in this way we use the 
\mbox{\tt secr} model fitting function \mbox{\tt secr.fit} to fit a
number of different models and then the function \mbox{\tt AIC} to
package the models together and summarize them in the form of an AIC
table, with rows of the table ordered from best to worst. The function
\mbox{\tt model.average} performans AIC-based model-averaging of the 
parameters specified by the \mbox{\tt realnames} variable (below this
is demonstrated for the parameter density, $D$).  Because this
function defaults to averaging by AICc, we slightly modified
this function (called \mbox{\tt model.average2}) to do model averaging
by either  AIC or AICc as specified by the user. Together
these commands and resulting output (abbreviated to fit on page) look like this:
{\small 
\begin{verbatim}
XXX note to reader: I will delete some horizontal output XXXXX

model0<-secr.fit(wolvcapt3,model=list(D~1, g0~1, sigma~1), buffer=20000)
model1<-secr.fit(wolvcapt3,model=list(D~session, g0~1, sigma~1), buffer=20000)
model2<-secr.fit(wolvcapt3,model=list(D~session, g0~session, sigma~1), buffer=20000)
model3<-secr.fit(wolvcapt3,model=list(D~session, g0~1, sigma~session), buffer=20000)
model4<-secr.fit(wolvcapt3,model=list(D~session, g0~session, sigma~session), buffer=20000)

> AIC (model0,model1,model2,model3,model4)
                                    model   detectfn npar    logLik      AIC     AICc dAICc  AICwt
model0                   D~1 g0~1 sigma~1 halfnormal    3 -627.2603 1260.521 1261.932 0.000 0.5831
model2       D~session g0~session sigma~1 halfnormal    5 -624.9051 1259.810 1263.810 1.878 0.2280
model1             D~session g0~1 sigma~1 halfnormal    4 -627.2365 1262.473 1264.973 3.041 0.1275
model4 D~session g0~session sigma~session halfnormal    6 -624.6632 1261.326 1267.326 5.394 0.0393
model3       D~session g0~1 sigma~session halfnormal    5 -627.2358 1264.472 1268.472 6.540 0.0222

> model.average (model0,model1,model2,model3,model4,realnames="D")
              estimate  SE.estimate          lcl          ucl
session=1 2.707190e-05 7.913577e-06 1.544474e-05 4.745224e-05
session=2 2.927423e-05 8.270402e-06 1.700631e-05 5.039193e-05
\end{verbatim}
}

{\bf XXX we should provide parameter estimates and SE's in a table XXXXX}

The default output of estimated density is in individuals per ha, so
we have to scale this up to something more reasonable. To get into
units of per 1000 $km^2$ we need to mulitply by 100 to get to units of
$km^2$ and then 1000. This produces an estimated density of 
about 2.71 for \mbox{\tt session=1} (females) and 2.93 for
\mbox{\tt session=2} (males).

We don't agree with the use of AICc here and think its better to use
AIC, in general. This is because its not clear what the effective
sample size is. While we have 21 individuals in the data set, most of
the model structure has to do with encounter probability samples and
for that there are 100s of observations. We note that the AIC and AICc
results are not consistent.
By looking at the best model by AIC, we find that the model
with estimated sex ratio and sex specific $g_{0}$ is preferred (Model 2). This
is just slightly better than the model with a fixed sex ratio of
$\psi_{sex} = 0.50$.


We do the same things with the mask which excludes ocean. 
Results are given in Tab. XXX along with the previous models without a mask.
We see AIC is much better for the model without the mask -- it is ok
to have this because we recognize the mask as changing the random
effects distribution and the results should be sensitive to that. That
said, we don't like the non-mask model because it makes sense to
exclude the water area from the state-space  of ${\bf s}$.
For females the model-averaged density is 3.88 individuals per 1000 $km^2$ and for
females the model-averaged density estimate is 4.46 individuals per
1000 $km^2$:
\begin{verbatim}
> model.average (model0b,model1b,model2b,model3b,model4b,realnames="D")

              estimate  SE.estimate          lcl          ucl
session=1 3.876615e-05 1.189102e-05 2.153795e-05 6.977518e-05
session=2 4.459658e-05 1.323696e-05 2.523280e-05 7.882022e-05
\end{verbatim}


\begin{verbatim}
> AIC (model0b,model1b,model2b,model3b,model4b)
                                     model   detectfn npar    logLik      AIC     AICc dAICc  AICwt
model2b       D~session g0~session sigma~1 halfnormal    5 -629.0482 1268.096 1272.096 0.000 0.4421
model4b D~session g0~session sigma~session halfnormal    6 -628.3490 1268.698 1274.698 2.602 0.1204
model0b                   D~1 g0~1 sigma~1 halfnormal    3 -632.5813 1271.163 1272.574 0.478 0.3481
model1b             D~session g0~1 sigma~1 halfnormal    4 -632.5574 1273.115 1275.615 3.519 0.0761
model3b       D~session g0~1 sigma~session halfnormal    5 -632.5445 1275.089 1279.089 6.993 0.0134
\end{verbatim}





\subsection{Bayesian model selection}

Model selection is somewhat less straightforward
as a Bayesian and there is no such canned all-purpose method like AIC
As such we recommend more of a pragmatic approach, in general, for all
problems, based on a number of basic considerations:
\begin{itemize}
\item[(1)] For a small number of fixed effects we think it is
  reasonable to adopt a conventional
``hypothesis testing'' approach -- i.e., if the posterior for a parameter overlaps
zero substantially, then it is probably reasonable to discard that effect.
\item[(2)] Calculation of posterior model probabilities: In some cases we can implement
methods which allow calculation of posterior model probabilities. One
such idea is the indicator variable selection idea from
\citet{kuo_mallick:1998}.
The idea is introduce a latent variable $I \sim \mbox{Bern}(.5)$ and
expand the model to include the variable $I$ as follows:
\[
 \mbox{logit}(p_{ijk}) = \alpha_{0} + I*\alpha_{1}*x_{ijk}.
\]
The importance of the covariate $x$ is then measured by the posterior
probability that $I=1$.  
\item[(3)] DIC -- the Deviance Information Criterion:
Bayesian model selection is now routinely carried-out using the
Deviance Information
Criterion (DIC; Spiegelhalter et al. 2002) although its effectiveness
in hierarchical models depends very much on the manner in which it is
constructed \citep{millar:2009}.
We recommend using it if it leads to
sensible results but also we think it should be calibrated to the
extent possible for specific classes of models. 
\item[(4)] Logical argument: For something like M/F differences it seems
to make sense to leave an extra parameter in the model no matter what,
because of course you expect, biologically, there to be a difference in
these things. e.g., in the above example we don't really ever think
that $psi_{sex} = 0.5$ and so we might not typically admit that model
into our model set. Thus the hypothesis test itself is meaningless, a form
of gratuitious hypothesis testing \citep{johnson:1999}.
\end{itemize}
In all modeling activities, as in life itself, the use of logical argument should not be under-utilized.

\subsection{Deviance Information Criterion (DIC) }

AIC makes the use of likelihood methods  convenient for problems where
likelihood estimation is achievable. 
For Bayesian analysis, the 
development of  Deviance Information Criterion (DIC)
\citep{spiegelhalter_etal:2002} seemed like a general-purpose
equivalent for a brief 
period of time after its invention.  However, 
there seems to be too many types of DIC, and no consistent version
is reported across computing platforms, and practical calibration
has indicated highly variable effectiveness of 
DIC. Even the high-end statisticians don't have a unified front on 
practical issues related to the use of DIC.
That said, it is still widely reported and, we think, probably
reasonable for certain classes of fixed effects models. 


Deviance is negative 2 times the loglikelihood. i.e.,
for a given model with parameters $\theta$:
$Dev(\theta) = -2*log(y|\theta)$
the DIC is defined as the posterior mean of the deviance plus a 
measure of model complexity, $p_{D}$:
\[
 DIC = \overline{Dev}(\theta) + p_{D}
\]
AS $p_{D}$ does not have a clear definition in hierarchical models
with latent structure, the standard definition of $p_{D}$ is
\[
 p_{d} = \overline{Dev}(\theta) - Dev(\bar{\theta})
\]
where the 2nd term is the deviance evaluated at the posterior mean of
the parameter(s). The $p_{D}$ here is interpreted as the effective
number of parameters in the model. 
Gelman et al. (2004) suggest a different version of $p_{D}$ based on
one-half the posterior variance of the deviance:
\[
 p_{V} - Var(Dev(\theta)|{\bf y})/2.
\]
We think this is what {\bf WinBUGS} and {\bf JAGS} are doing if you
run them through \mbox{\tt R2WinBUGS} and \mbox{\tt R2jags}, respectively.
It is less easy to get DIC summaries from \mbox{\tt rjags}, so we have
used \mbox{\tt R2jags} in our analysis here. 


We did a Bayesian analysis of the 4 models 
mentioned above (models 1 - 4, but not ``Model 0'' which assumed $\psi_{sex} =
0.5$). We have an {\bf R} function called \mbox{\tt wolvSCR0ms.fn}
 in the \mbox{\tt scrbook} package which will fit each model.
The function uses {\bf JAGS} by default for the fitting, using the \mbox{\tt
  R2jags} package.  Here is how we obtain the results each model:
{\small
\begin{verbatim}
> toad1<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=1)
> toad2<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=2)
> toad3<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=3)
> toad4<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=4)
\end{verbatim}
}


{\bf
XXX provide the results from the log(sigma) parameterization in the
text but comment  on whether there is a difference.
XXXX
}

The BUGS model specification for model 4 is shown in panel XXXX. We
made a single R script that fit each of the 4 models by using a binary
indicator variable fed to BUGS as data which would turn on/off each
component of the model. Note that the baseline encounter probability
is modeled on the logit-scale and the new parameter $alpha_{sex}$ (bad
notation?) whereas we model the effect of sex on $\sigma$ not on the
scale of $\sigma$ but rather on the ``effect scale'' -- i.e., the
coefficient of distance. 
XXXXX
We should see if the other parameterization
changes things XXXXX
A final point: We have dunif(-3,3) priors. Check normal prior and
check dunif(-5,5).  Check WinBUGS too.

\begin{verbatim}
# check notation and put this in a panel?
alpha.sex ~ dunif(-3,3)
beta.sex  ~ dunif(-3,3)
sigma~dunif(0,50)
logitp0~dnorm(0,.1)
beta<- (1/(2*sigma*sigma) )
psi ~ dunif(0,1)
psi.sex  ~ dunif(0,1)
for(i in 1:M){
 wsex[i] ~ dbern(psi.sex)
 w[i]~dbern(psi)
 s[i,1]~dunif(Xl,Xu)
 s[i,2]~dunif(Yl,Yu) 
 logit(p0[i])<- logitp0 + alpha.sex*wsex[i]
 log(beta.vec[i])<- log( beta ) + beta.sex*wsex[i]
for(j in 1:ntraps){
  mu[i,j]<-w[i]*p[i,j]
  ncaps[i,j]~ dbin(mu[i,j],K[j]) 
  dd[i,j]<- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2) 
  p[i,j]  <-  p0[i]*exp( - beta.vec[i]*dd[i,j] )

}
}
\end{verbatim}


An issue is applying DIC to hierarchical models in which case the
level of the hierarchcy matters. Do you condition on the random
effects, or do you use the deviance based on the marginal likelihood?
The current Conventional Wisdom is that DIC doesn't really work so
well in hierarchical models (    ). But there doesn't really seem to be
general theory to support this. It seems maybe like its ok sometimes. So
we take a look at its use here to pick among fixed effects models.

We did this for our wolverine data. We used the same 4 models as above
and a 5th model which was the super-model containing all of them
together. We're not sure what DIC means in that case but, whatever. 

For DIC, small is better and so this suggests Model 2 is preferred. 
What is this model?

Summarize model parameters for each model...... estimates ... and
deviance, np and DIC for each model. 
The output from these models is summarized in Table XXXXX.
These are based on 21000 iterations, 3 chains, 1000 discarded, 60k
posterior samples.......
what we see is: XYZ.....
I think we get the same model by DIC as we get by AIC. This is super
encouraging!

 
\begin{verbatim}
           dev  np     dic
Model 1  441.78 78.3  520.0
Model 2  440.97 82.4  523.4  
Model 3: 443.68 80.9  524.6
Model 4: 441.07 77.3  518.3  <- better model

DIC:  4, 1, 2, 3
AIC:  2, 4, 1, 3

table
model  log(sigma) delta  alpha0  alpha.sex D  dev np DIC


> toad1b
Inference for Bugs model at "modelfile0.txt", fit using jags,
 3 chains, each with 11000 iterations (first 1000 discarded)
 n.sims = 30000 iterations saved
          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
D           5.834   1.138   3.856   5.012   5.784   6.555   8.290 1.001  9800
N          60.521  11.805  40.000  52.000  60.000  68.000  86.000 1.001  9800
X1new       9.704   2.759   5.177   7.719   9.411  11.365  15.962 1.001  6500
X1obs      13.107   3.404   7.483  10.671  12.759  15.165  20.727 1.001  4300
X3new      13.079   3.233   7.621  10.764  12.776  15.056  20.235 1.001 30000
X3obs      21.259   2.195  17.590  19.707  21.050  22.582  26.146 1.001  6900
Xnew       61.726   6.581  49.513  57.150  61.457  66.088  75.232 1.001  9900
Xobs       89.291   6.105  78.264  84.972  89.043  93.251 101.988 1.001  7100
alpha.sex  -0.014   1.730  -2.846  -1.523  -0.017   1.477   2.842 1.001 20000
beta        1.258   0.208   0.881   1.112   1.247   1.393   1.689 1.001 15000
beta.sex    0.008   1.735  -2.846  -1.491   0.013   1.521   2.845 1.001 10000
logitp0    -2.816   0.179  -3.170  -2.935  -2.815  -2.697  -2.467 1.001  6500
psi         0.305   0.067   0.188   0.258   0.300   0.347   0.448 1.001 15000
psi.sex     0.520   0.103   0.320   0.449   0.520   0.592   0.720 1.001  7300
sigma       0.637   0.054   0.544   0.599   0.633   0.671   0.753 1.001 15000
deviance  441.776  12.513 419.615 433.006 441.021 449.721 468.423 1.002  3500

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 78.3 and DIC = 520.0
DIC is an estimate of expected predictive error (lower deviance is better).
> toad2b
Inference for Bugs model at "modelfile0.txt", fit using jags,
 3 chains, each with 11000 iterations (first 1000 discarded)
 n.sims = 30000 iterations saved
          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
D           5.685   1.144   3.759   4.916   5.591   6.362   8.193 1.001  9700
N          58.980  11.868  39.000  51.000  58.000  66.000  85.000 1.001  9700
X1new       9.785   2.760   5.218   7.796   9.498  11.454  15.932 1.001 16000
X1obs      12.403   3.362   6.852  10.005  12.052  14.427  19.861 1.001 30000
X3new      13.146   3.280   7.587  10.812  12.852  15.172  20.343 1.001 30000
X3obs      21.305   2.087  17.780  19.818  21.127  22.580  25.902 1.004   710
Xnew       62.283   6.637  49.914  57.700  62.082  66.611  76.023 1.001 30000
Xobs       89.161   6.105  78.063  84.883  88.888  93.103 101.987 1.002  2200
alpha.sex  -0.755   0.348  -1.447  -0.985  -0.754  -0.522  -0.075 1.006   420
beta        1.174   0.214   0.779   1.030   1.167   1.311   1.621 1.001  4200
beta.sex    0.013   1.741  -2.846  -1.490   0.016   1.536   2.856 1.001  7400
logitp0    -2.437   0.249  -2.914  -2.607  -2.440  -2.271  -1.945 1.005   520
psi         0.297   0.067   0.182   0.250   0.292   0.339   0.442 1.001 10000
psi.sex     0.564   0.104   0.354   0.492   0.567   0.638   0.757 1.001 16000
sigma       0.661   0.063   0.555   0.618   0.654   0.697   0.801 1.001  4200
deviance  440.973  12.851 418.369 431.914 440.121 449.058 468.333 1.003  1000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 82.4 and DIC = 523.4
DIC is an estimate of expected predictive error (lower deviance is better).
> toad3b
Inference for Bugs model at "modelfile0.txt", fit using jags,
 3 chains, each with 11000 iterations (first 1000 discarded)
 n.sims = 30000 iterations saved
          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
D           5.721   1.131   3.759   4.916   5.591   6.458   8.193 1.002  2700
N          59.355  11.731  39.000  51.000  58.000  67.000  85.000 1.002  2700
X1new       9.661   2.748   5.168   7.702   9.346  11.314  15.869 1.001 30000
X1obs      13.050   3.415   7.419  10.621  12.680  15.101  20.724 1.001 16000
X3new      13.125   3.224   7.653  10.817  12.838  15.142  20.203 1.001 15000
X3obs      21.335   2.203  17.651  19.774  21.109  22.664  26.245 1.001 11000
Xnew       62.261   6.680  49.860  57.651  61.992  66.637  76.052 1.001  4600
Xobs       89.964   6.203  78.599  85.632  89.649  93.989 102.872 1.002  1700
alpha.sex  -0.017   1.730  -2.855  -1.513  -0.019   1.477   2.852 1.001 30000
beta        1.221   0.290   0.692   1.019   1.204   1.406   1.837 1.012   190
beta.sex    0.022   0.346  -0.647  -0.205   0.016   0.241   0.734 1.013   170
logitp0    -2.818   0.175  -3.163  -2.936  -2.816  -2.700  -2.477 1.003   870
psi         0.299   0.066   0.184   0.252   0.294   0.341   0.441 1.002  3300
psi.sex     0.526   0.110   0.311   0.449   0.526   0.604   0.739 1.002  1400
sigma       0.654   0.084   0.522   0.596   0.644   0.700   0.850 1.012   190
deviance  443.680  12.732 420.953 434.766 442.974 451.708 470.737 1.002  1400

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 80.9 and DIC = 524.6
DIC is an estimate of expected predictive error (lower deviance is better).
> toad4b
Inference for Bugs model at "modelfile0.txt", fit using jags,
 3 chains, each with 11000 iterations (first 1000 discarded)
 n.sims = 30000 iterations saved
          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
D           5.647   1.119   3.759   4.820   5.494   6.362   8.097 1.004   780
N          58.587  11.606  39.000  50.000  57.000  66.000  84.000 1.004   780
X1new       9.794   2.789   5.230   7.779   9.480  11.494  16.065 1.001 15000
X1obs      12.360   3.356   6.833   9.970  12.025  14.348  19.936 1.002  2600
X3new      13.162   3.233   7.621  10.864  12.883  15.163  20.274 1.001 15000
X3obs      21.378   2.070  17.890  19.920  21.181  22.639  25.949 1.001 30000
Xnew       62.472   6.595  50.278  57.914  62.204  66.754  76.131 1.001  4800
Xobs       88.559   6.070  77.548  84.319  88.227  92.445 101.274 1.003  1200
alpha.sex  -0.788   0.344  -1.471  -1.022  -0.782  -0.552  -0.123 1.003   950
beta        1.320   0.304   0.797   1.105   1.296   1.505   1.975 1.012   180
beta.sex   -0.223   0.337  -0.898  -0.447  -0.216   0.006   0.426 1.010   230
logitp0    -2.445   0.237  -2.894  -2.607  -2.449  -2.288  -1.966 1.002  1500
psi         0.295   0.065   0.181   0.249   0.290   0.336   0.435 1.004   740
psi.sex     0.535   0.110   0.317   0.459   0.536   0.612   0.743 1.001  7200
sigma       0.628   0.074   0.503   0.576   0.621   0.673   0.792 1.012   180
deviance  441.065  12.433 418.997 432.297 440.248 449.000 467.453 1.001  8300

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 77.3 and DIC = 518.3
DIC is an estimate of expected predictive error (lower deviance is better).
\end{verbatim}












\begin{verbatim}

> print(toad5,digits=2)
Inference for Bugs model at "modelfile5.txt", fit using WinBUGS,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
            mean    sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
mod[1]      0.67  0.47   0.00   0.00   1.00   1.00   1.00    1  1300
mod[2]      0.48  0.50   0.00   0.00   0.00   1.00   1.00    1 42000
beta.sex    0.05  1.71  -2.83  -1.42   0.06   1.53   2.85    1 12000
alpha.sex  -0.53  1.10  -2.53  -1.06  -0.74  -0.35   2.53    1  3000
psi         0.28  0.06   0.17   0.24   0.27   0.31   0.40    1 11000
psi.sex     0.55  0.11   0.34   0.48   0.55   0.62   0.74    1  3300
sigma      25.21 14.32   1.81  12.82  25.33  37.51  48.77    1  7200
logitp0    -2.59  0.30  -3.12  -2.83  -2.61  -2.38  -1.98    1   860
N          54.86  9.86  38.00  48.00  54.00  61.00  77.00    1 16000
D           5.29  0.95   3.66   4.63   5.21   5.88   7.42    1 16000
Xobs       91.39  5.55  81.39  87.50  91.08  94.92 103.20    1 60000
Xnew       64.69  6.10  53.39  60.50  64.49  68.71  77.23    1 60000
X1obs      12.48  3.34   6.96  10.09  12.12  14.51  19.94    1 52000
X1new       9.76  2.75   5.21   7.78   9.48  11.42  15.91    1 60000
X3obs      21.59  2.06  18.17  20.12  21.39  22.84  26.21    1  8000
X3new      13.20  3.28   7.67  10.86  12.90  15.23  20.41    1 60000
deviance  444.43 11.86 423.40 436.10 443.60 452.00 469.90    1 14000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 70.3 and DIC = 514.7
DIC is an estimate of expected predictive error (lower deviance is better).
> 
\end{verbatim}




\begin{comment}
As is pointed out in one of those posts, it's interesting to me that
both AIC and DIC are attempts at mimicking leave-one-out
cross-validation. Perhaps we could make the point that if DIC cannot
be trusted then one could always implement their own x-val
procedure... assuming it doesn't take months to complete.

Richard

On Thu, May 24, 2012 at 9:57 AM, Jeffrey Royle <jaroyle@gmail.com> wrote:
> hi all -- DIC is a hopeless black hole with, I think, no real possibility of
> making it seem reasonable to people.
> There is some good discussion on Gelman's blog, along with a number of
> papers (all cited there)
> http://andrewgelman.com/2006/07/number_of_param/
> http://andrewgelman.com/2011/06/plummer_on_dic/
> http://andrewgelman.com/2011/06/deviance_dic_ai/
>
> I gather that there are 3 version of DIC. The original one, and then
> Plummer's version which seems to be in JAGS and then the dumb version based
> on .5*var(deviance) which, I gather, you should never use -- I guess it
> sucks.
>
> I still can't compute Plummer's version using JAGS -- I think maybe I need
> to update my JAGS version or something.
\end{comment} 





\subsection{Bayesian model averaging: indicator variables}

A convenient way to deal with model selection and averaging problems in Bayesian
analysis by MCMC is to use the method of model indicator variables
\citep{kuo_mallick:1998} which allow us 
to expand the model to include the set of prescribed models as
specific reductions of a larger 
model.
This has been demonstrated in some specific capture-recapture models
in 
Royle 2009, sec. 3.xxx in \citep{royle_dorazio:2008},
and in the context of SCR by 
 Tobler et al. (in review XXXXX need to find this paper XXXXXXX).
Using this method, the model-averaged parameters are produced by
default.... we need to be careful of reporting parameters that don't
have a common interpretation in the different models because it wont
mean shit. e.g., if a regression parameter is in a model, then an
MCMC draw is from the posterior but if the regression parameter is not
in the model then it is a draw from the prior, and so we should think
carefully about whether it makes sense to report an average of such a
thing (in the vast majority of cases the answer is no). But some
parameters like $N$ or density, $D$, do have a consistent
interpretation and we support producing model-averaged results of
those parameters.

To implement the Kuo and Mallick approach,  
we expand the model to include the latent
indicator variables say $I_{m}$, for  variable $M$ in the model, 
such that 
\begin{eqnarray*}
I_{m} = \left\{ 
\begin{array}{cc} 1 &  \mbox{ linear predictor includes  covariate $m$} \\
                  0 & \mbox{ linear predictor does not
                                include covariate $m$}
 \end{array}
\right.
\end{eqnarray*}
We assume that the indicator variables $I_{m}$
are mutually independent with:
\[
I_m \sim \mbox{Bernoulli}(0.5).
\]
The
 expanded model has the linear predictor:
\[
\mbox{logit}(p_{ijk}) = \alpha_{0} + \alpha_{1}I_{1} C_{1,i} + \alpha_{2}I_{2} C_{2,ijk} 
\]
where, lets suppose, $C_{1,i}$ is an individual level covariate such
as sex, and $C_{2,ijk}$ is a behavioral response covariate which is individual- trap- and occasion-specific.
We can assume a parallel model specification on the parameter $\sigma$
which is liable to vary by individual level covariates such as sex:
\[
 log(\sigma_{i}) = \beta_{0} + \beta_{1} I_{3} C_{1,i}
\]

In this formulation of the model selection problem we can characterize
unique models by the sequence of $I$ variables. In this case, each
unique sequence $(I_{1},I_{2},I_{3})$ represents  a model, and we can
tabulate the posterior frequencies of each model by post-processing
the MCMC histories of $(I_{1},I_{2},I_{3})$ (we demonstrate this
shortly). 


Conceptually, analysis of this expanded model within the data augmentation
framework does not pose any additional difficulty. One
broader, technical consideration is that posterior model probabilities
are well known to be sensitive to priors on parameters
\citep{aitkin_1991: link_barker:2006} and see also 
\citep[][sec. 3.4.3]{royle_dorazio:2008} and \citep[][sec. 7.2.5]{link_barker:2010}.
What might normally be viewed as 
 vague or non-informative priors, 
are not usually innocuous or uninformative when evaluating
posterior model probabilities. The use of Akaike's information
criterion seems to avoid this problem largely by imposing a
specific and perhaps undesirable prior that is a function of
the sample size \citep{kadane_lazar:2004}. One solution 
 is to compute posterior model probabilities under
a model in which the prior for parameters is fixed at the
posterior distribution under the full model \citep{aitkin:1991}. At a
minimum, one should evaluate posterior model probabilities under
different vague prior specifications. 

Another approach to implementing model indicator variables is 
 to introduce a categorical model identity variable
which, i.e., so that 
 ``model identity'' is itself a parameter of the model and each
 distinct model is associated with a unique covariate combination or
 other set of model features. This is convenient especially when we
 cannot specify the linear predictor as some general model that
 reduces to various alternative sub-models simply by switching binary
 variables on or off. In the context of SCR models, choosing among
 different encounter probability models would be an example.
 For this case we do something like this:
\begin{verbatim}
 mod \sim \mbox{dcat}(probs[])
\end{verbatim}
where \mbox{\tt probs[l] = 1/(\# models)}. Then we can fill in a bigger
array of $p$ instead of $p[i,j]$ we build $p[i,j,l]$ for each of
$l=1,2,\ldots,L$ models and we define our data component like so:
\begin{verbatim}
pi[i,j]<- bigpi[i,j,mod]
y[i,j] ~ dbern(pi[i,j])
\end{verbatim}
(or something like that).
As before the posterior probabilities can be highly sensitive to
priors on the different model parameters and sometimes mixing is
really poor.  
But we give a JAGS script in the {\bf R} package
\mbox{\tt scrbook} which has an example that works.
The model specificication for the wolverine data for 3 different
encounter probability models is shown in panel XXXX: Gaussian
detection probability; Gaussian hazard model; logistic model.
The key things to note are that there are 3 intercepts and 3 different
``beta'' parameters (related to inverse of the scale parameter). The
parameters should not be regarded as equivalent across the models, so
it is important to have them seperately estimated for each
model..... Then we create a probability of encounter for each
individual, trap {\it and} model so that the holder object 'p' in the
model description is a 3-d array. (sometimes this would have to be a 4
or 5-d array in more complex models with time effects, etc..)

\begin{panel}[htp]
\centering
\rule[0.15in]{\textwidth}{.03in}
%\begin{minipage}{5in}
{\small
\begin{verbatim}
model {
for(i in 1:3){
  alpha0[i] ~ dnorm(0,.1)
  beta[i] ~ dunif(0,2)
  mod.probs[i]<- 1/3
}
psi ~ dunif(0,1)
psi.sex  ~ dunif(0,1)
catmod~dcat(mod.probs[])

for(i in 1:M){
 w[i]~dbern(psi)
 s[i,1]~dunif(Xl,Xu)
 s[i,2]~dunif(Yl,Yu) 
 logit(beta0.vec[i,1])<- alpha0[1]
 log(beta0.vec[i,2])<- alpha0[2]
 beta0.vec[i,3]<- alpha0[3]

 log(beta.vec[i,1])<- log( beta[1] ) 
 log(beta.vec[i,2])<- log( beta[2] ) 
 log(beta.vec[i,3])<- log( beta[3] ) 

 for(j in 1:ntraps){
   mu[i,j]<-w[i]*p[i,j,catmod]
   ncaps[i,j]~ dbin(mu[i,j],K[j]) 
   dd[i,j]<- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2) 
 
   p[i,j,1]  <-  beta0.vec[i,1]*exp( - beta.vec[i,1]*dd[i,j] )
   p[i,j,2] <-  1-exp(-beta0.vec[i,2]*exp( - beta.vec[i,2]*dd[i,j] ) )
   logit(p[i,j,3])<- beta0.vec[i,3] - beta.vec[i,3]*dd[i,j]
  }
}
N<-sum(w[1:M])
D<-N/area
}
\end{verbatim}
}
%\end{minipage}
\rule[-0.15in]{\textwidth}{.03in}
\caption{
Part of the BUGS specification
of the indicator variable idea to choose among
  different detection models..... this code is contained in an R
  script in scrbook package called XXXXXX.
}
\label{gof.panel.modsel}
\end{panel}




\subsection{Wolverine stuff}



We did a bunch of analysis previously with models that involved
sex-specific parameters. Here we expand the model set to include a
behavioral response. This is a little more difficult doing Bayesian
analysis because we have to do the 3-d version of the model which can
be a time-consuming task in WinBUGS. But lets do it anyway.
There are in this case 8 models (right?)


4 models with sex: DIC, model weights, AIC.

expanded model with behavioral response...... DIC , model weights, AIC......

\begin{verbatim}
Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.894	1.009	0.0171	4.18	5.822	8.062	1001	6600
	N	39.48	6.755	0.1145	28.0	39.0	54.0	1001	6600
	X1new	8.996	2.711	0.03361	4.551	8.695	15.15	1001	6600
	X1obs	11.87	3.354	0.05475	6.382	11.49	19.53	1001	6600
	X3new	13.15	3.232	0.03914	7.725	12.92	20.4	1001	6600
	X3obs	21.43	2.115	0.03256	17.8	21.24	26.07	1001	6600
	Xnew	61.78	6.517	0.1004	49.48	61.7	75.22	1001	6600
	Xobs	88.97	5.972	0.08912	78.08	88.63	101.5	1001	6600
	alpha.sex	-0.4506	1.158	0.02228	-2.571	-0.6846	2.594	1001	6600
	beta	2.423	4.111	0.3034	0.06847	1.165	17.51	1001	6600
	beta.sex	0.07415	1.761	0.1045	-2.85	0.0155	2.868	1001	6600
	deviance	439.3	12.16	0.2192	418.0	438.4	465.5	1001	6600
	logitp0	-2.577	0.2909	0.0114	-3.079	-2.597	-1.979	1001	6600
	mod[1]	0.6236	0.4845	0.01885	0.0	1.0	1.0	1001	6600
	mod[2]	0.5035	0.5	0.03697	0.0	1.0	1.0	1001	6600
	psi	0.2657	0.05634	8.482E-4	0.1653	0.2617	0.3855	1001	6600
	psi.sex	0.5449	0.1044	0.00231	0.336	0.5472	0.7417	1001	6600
	sigma	0.8442	0.6291	0.0507	0.1698	0.6551	2.704	1001	6600




run 2


Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.939	1.009	0.01712	4.18	5.822	8.211	1001	27000
	N	39.78	6.756	0.1147	28.0	39.0	55.0	1001	27000
	X1new	9.006	2.701	0.02151	4.603	8.699	15.18	1001	27000
	X1obs	11.98	3.396	0.04019	6.437	11.61	19.69	1001	27000
	X3new	13.13	3.218	0.01885	7.748	12.84	20.26	1001	27000
	X3obs	21.4	2.116	0.02102	17.83	21.2	26.11	1001	27000
	Xnew	61.54	6.483	0.0668	49.39	61.26	74.8	1001	27000
	Xobs	88.74	6.003	0.06467	77.85	88.46	101.4	1001	27000
	alpha.sex	-0.4355	1.192	0.01962	-2.624	-0.6607	2.605	1001	27000
	beta	1.257	2.088	0.1191	0.06376	1.036	6.652	1001	27000
	beta.sex	0.5605	1.675	0.06955	-2.723	0.7576	2.92	1001	27000
	deviance	438.9	12.25	0.1858	417.0	438.0	464.9	1001	27000
	logitp0	-2.598	0.287	0.01129	-3.088	-2.624	-1.998	1001	27000
	mod[1]	0.5939	0.4911	0.01784	0.0	1.0	1.0	1001	27000
	mod[2]	0.5251	0.4994	0.02614	0.0	1.0	1.0	1001	27000
	psi	0.3312	0.06925	0.001093	0.2101	0.3264	0.4776	1001	27000
	psi.sex	0.5425	0.1038	0.002119	0.3366	0.5443	0.7389	1001	27000
	sigma	1.045	0.6963	0.04014	0.2743	0.6947	2.801	1001	27000



\end{verbatim}








\section{Evaluating Goodness-of-Fit}

We estimate parameters, maybe fit a bunch of models and pick a
desireable one or perhaps average estimates of density from a few
models. An important questions is:
Are the data we have consistent with realizations from the model which
we just fitted?

Conceptually, we can think of model selection as follows: if we simulate
under that model, do the simulated realizations look like our data?
In a sense, it is that last element that is most relevant to science.
That is beyond the basic construction of the model which presumably
contains some basic scientific hypotheses and thus requires some
understanding and thinking about the system. After that it is the
assessment that provides what is essentially a basis for falsification
of the model, and hence learning..  By saying that the data are
inconsistent with a model, then we're rejecting at least one of the
hypotheses embodied by that model. This is not really true... it could
mean we're missing something, or many things. The basic model could be
correct, just not having enough stuff in it. 

For either Bayesian or classical inference, the basic strategy is to
come up with a fit statistic that depends on the parameters and the
data set, which we denote by $T({\bf y}, \theta)$, and then we compute
this for the observed data set, and then compare its value to that
computed for perfect data sets simulated under the correct model.  In
the case of classical inference, we use a parametric bootstrap where
we simulate data sets conditional on the MLE $\hat{\theta}$. For
Bayesian analysis we use the Bayesian p-value \citep{gelman_etal:2006}
in
which data sets are simulated for a posterior sample of $\theta$ which
we briefly introduced in sec. 2.XXXXX.

Using classical inference methods, it is sometimes possible to
identify a test statistic, with known asymptotic distribution, to
evaluate goodness-of-fit using a chi-square statistic. Examples from
the closed capture-recapture setting includes XXXXXX XXXXXXXX.  When
this is not the case, goodness-of-fit is ususally assessed using
bootstrap methods \citep{dixon:2002}. Using a bootstrap method
we identify a fit
statistic and we compute the value for which is more or less the
standard approach, in general.  To date, we are unaware of any
goodness-of-fit applications based on likelihood analysis of SCR
models (XXX WE NEED TO DO SOME RESEARCH ON THIS XXXXX).  We give some
ideas in sec. xxxxxx below.

One challenge with SCR models is there has not been a definitive or 
general proposal for a fit statistic for use in computing the Bayesian
p-value although a few specialized implementations of Bayesian p-values
have been provided (Royle 2009; Gardner et al. 2010 ???).
As a general matter, we recommend use of Bayesian p-values but caution
that there is not general expectation to support how well they should
do, in general. As such, one should expect to do some kind of custom
evaluation for every use of such methods if the ability to reject
under specific departures from the model is of paramount interest.
This is not a weakness of a Bayesian approach because the same issue
applies in doing parametric bootstrapping.




\subsection{The Two Components of Model Fit}

For SCR models,
there are at least two components to model fit for most models,
making an omnibus measure of fit difficult to describe.
First we can consider whether the model explains the observation
process - we can evaluate this based on the encounter frequencies of
individuals {\it conditional} on ${\bf s}_{1}, \ldots, {\bf s}_{N}$
Second, we might also be
interested in evaluating the hypothesis concerning the distribution of
the activity centers. For the simple model developed in this chapter,
this is the assumption of {\it complete spatial randomness} (CSR)
which we consider in section XXX.YYY below. Actually, as we noted in
sec. XXX this is not strictly CSR per se because of the binomial
assumption on $N$ but we refer to it as that because it is essentially
equivalent. And besides we generate the reference distribution within
the context of a Bayesian p-value.

We propose analyzing the separate elements of fit individually.
To evaluate fit of the detection component of the model, we consider
two distinct fit statistics based on the individual encounter
frequencies and also the trap encounter frequencies.

I think inference about Density is going to be insensitive to
departures from CSR but probably somewhat sensitive to bad models for
the observation process.  Why do I think this? Well spatial variation
is not important to inference about the aggregate population size, but
it is relevant to "within population" inference, such as predicting on
small areas.  Conversely, inference about total population size is
known to be highly sensitive to the observation model (Dorazio and
Royle 2003; Link 2003).



\subsection{Testing Uniformity or Spatial Randomness}

Historically, especially in ecology, there has been a huge amount of
interest in whether a realization of a point process indicates
"complete spatial randomness," i.e., that the points are distributed
uniformly and independently in space.  A good reference for such
things is Cressie (1996; ch. 8) and XYZ.XYZ and I also like Tony
Smith's lecture notes (Univ. of Penn. ESE
502)\footnote{
\url{http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf}}. In
the context of animal capture-recapture studies, the CSR hypothesis is
manifestly false, purely on biological grounds. Typically individuals
will be clustered or more uniform (for territorial species) than
expected under spatial randomness. That said, it may be a reasonable
approximation to truth in some situations and
might be largely irrelevant so far as obtaining estimates of density
is concerned (given that we do observe a portion of the population,
$n$).  Furthermore, in realistic sample sizes we might expect
relatively low power to detect departures from CSR although this is a
research question worthy of attention (see section XX.YY).

Before proceeding with the development of a framework for evaluating
the point process model - we note that if $N$ is fixed, the resulting
point process is not, strictly speaking, one of "complete spatial
randomness". This is because when $N$ is fixed, a slight bit of
correlation is induced in the number of points within any particular
subset of the state-space. That said, this is negligible for most
purposes and, besides, we use a simulation based approach to testing
in which we simulate under the appropriate model.  But the point is
that CSR is not really the conventional term - maybe we call this
"uniformity".  The basic technical framework for evaluating the CSR
hypothesis is that the cell counts i.e., precisely those we computed
to produce a density map, should have a binomial distribution and we
can use a standard chi-square goodness-of-fit test to evaluate that.
It will usually suffice to approximate the binomial cell counts by
Poisson cell counts in which case we can use the classical
"index-of-dispersion" test (e.g., Illian et al. 2008; p. 87):
\[
   I =  (cells -1)*s^2/\bar{n}
\]
which has approximately a chi-square on $(ncells - 1)$ df.  If $s^2/nbar
> 1$ this suggests clustering and if $s^2/nbar <1$ then this suggests the
point process is too regular.  I think this test is sensitive to the
number and size of the grid cells chosen and I don't know much about
that but it should be researched.
{\it The} important technical issue is that we don't observe the point
process and so the standard statistics for evaluating CSR cannot be
computed directly.  However, using Bayesian analysis, we do have a
posterior sample of the underlying point process and so we suggest
computing the posterior distribution of the chi-square statistic and
seeing how it compares to 1. As an alternative to the chi-square
statistic based on CSR there are various "nearest-neighbor"
methods. For example EXAMPLE XXXXXXXXX which is also easy to compute.

To evaluate the uniformity assumption (i.e.,
``complete spatial randomness'') for the activity centers, we use a
standard chi-square goodness-of-fit test statistic, based on gridding
the state-space of the point process into $g=1,2,\ldots,G$ cells, and
we tabulated $n(g)$ the number of activity centers in each grid
cell. A standard goodness-of-fit statistic for CSR is based on the
ratio of the variance of $n(g)$ to the mean (see Table 8.3 in Cressie
1996). In particular, in parametric likelihood theory, $I = (G
-1)*s^2/\bar{n}$ should have a chi-square on $(G - 1)$ df under the
CSR hypothesis.  However, we used this statistic as the basis for our
Bayesian p-value calculation, comparing a posterior sample of $I$
computed using each posterior realization of the point process with a
value of $I$ obtained by simulating ${\bf s}$ under CSR.


An issue is that this is probably sensitive to the size of the
state-space. As the size of the state-space increases then the cell
counts {\it are} independent binomial counts with constant density,
and so we can overwhelm the fit statistic with extraneous ``data''
simulated from the posterior, which is equal to the prior as we move
away from the data, and therefore uninformed by the actual data in the
vincinity of the trap array.
Therefore we recoommend computing these fit statistics in the vicinity
of the trap array only.

\subsection{Computing the cell counts}

In chapter 4 we talked about computing summaries of individual
locations, such as for producing a density map. We need these for the
GoF analysis...In the present context, the number of individuals
living in any well-defined polygon is a derived
parameter. Specifically, let $B(x)$ indicate a pixel
 centered at x then
$N(x)=sum_{i} I(s[i] in B(x))$ is the population size of box B(x), and
$D(x) = N(x)/||B(x)||$ is the local density. These are just "derived
parameters" (see Chapt. 2) which are estimated from MCMC output using
the appropriate Monte Carlo average. Note that we are assuming in our
illustration that N is known and so this is easily done by taking all
of the output for MCMC iterations m=1,2,, and doing this:
\[
   N(x,m) = sum_{z[i,m]=1} I(s[i,m] in B(x))
\]
Thus, N(x,1),N(x,2),, is the Markov chain for parameter N(x).

It is worth emphasizing here that density maps will not usually appear
uniform despite that we have assumed that activity centers are
uniformly distributed because the observed encounters of individuals
provide direct information about the location of the i=1,2,.,n
activity centers and thus their "estimated" locations will be affected
by the observations. In a limiting sense, were we to sample space
intensely enough, every individual would be captured a number of times
and we would have considerable information about all N point
locations. Consequently, the uniform prior would have almost no
influence at all.


\section{Omnibus and Encounter Model Testing}

For the case where there are no time-varying covariates, we can
summarize the data by individual and trap-specific counts
$y_{ij}$. Conditional on ${\bf s}_{i}$,
the expected value under any encounter model is:
\[
 E[y_{ij}] = p_{ij} K
\]
and we can define a fit statistic from the Freeman-Tukey residual
\[
 e_{ij} =  \sqrt{ y_{ij} } - \sqrt{ E(y_{ij}) }
\]
Define
\[
 T_{1}({\bf y}, \theta) = \sum_{i}\sum_{j} e_{ij}^{2}
\]
This is conditional on ${\bf s}$ and conditional on ${\bf z}$ and we
compute this for {\it each} iteration of the MCMC algorithm to provide
an assessment of model fit.
WHY FREEMAN TUKEY XXX FROM CHAPT 2 XXXXXX



\subsubsection{Statistic 2: Trap frequencies}

Thinking of SCR models  as spatial counts at trap locations it makes
sense that the SCR model is basically just a spatial model of counts
\citep{chandler_royle:2012}. So we should pick models that provide
good predictions of trap totals.
This suggests the following statistic:
\[
 bleen
\]

In this context of a spatial prediction model it
makes sense to think of 
cross-validation. 
Cross-validation is a standard
method of fitting spatial models such as kriging and splines. So we
could as well use cross-validation based on the TRAP-SPECIFIC
frequencies or some other measure.  We should try that here.


\subsubsection{Statistic 1: Individual frequencies}

SCR model is a model for heterogeneity for which individual encounter
frequencies are the sufficient statistic [not really but close]. so we
propose to use a statistic based on that.

If model has time effects then y[i,j] is binary ... we recommend
computing individual/trap or individual or trap summaries using an
expected value based on $\sum_{k} p_{ijk}$ (Russell et al. 2012).

Probably for choosing among encounter probability models there is no
ability to detect departures from fit for typical sparse data
arrays. This is because individuals are observable in only a few
traps.....

Perhaps behavioral response or other biological models we can have
some power although there needs to be some basic simulation work done
on these problems.


\subsection{Simulation}

Quick sim study with K=10 and K=40 -- N=100, large sample sizes for
sure.
For each model we fitted each of the other 5 models.
Computed each fit statistic, how often do we reject that the model fits?



\section{Some examples: Wolverine}


Wolverine study:
Model selection --- 

Goodness-of-fit
Is SCR0 adequate for the wolverine data?

We used the posterior output from the wolverine model fitted previous
to compute a relatively coarse version of a density map, using a 10 x
10 grid (Figure XXX.YYY) and using a 30 x 30 grid (Figure XXYYZZ). A
couple of things are noteworthy: First is that as we move away from
"where the data live" - away from the trap array - we see that the
density approaches the mean density (lambda = XX per grid cell 
XXX.YYY how is this computed?). This is a property of the estimator
when the "detection function" decreases sufficiently
rapidly. Relatedly, it is also a property of statistical smoothers
such as splines, kernel smoothers, and regression smoothers -
predictions tend toward the global mean as the influence of data
diminishes. Another way to think of it is that it is a consequence of
the prior - which imposes uniformity, and as you get far away from the
data, the predictions tend to the prior. The other thing to note about
this map is that density is not 0 over water. This might be perplexing
to some who are fairly certain that wolverines do not like
water. However, there is nothing about the model that recognizes water
from non-water and so the model predicts over water {\it as if} it
were habitat similar to that within which the array is nested. But,
all of this is ok as far as estimating density goes and, furthermore,
we can compute valid estimates of N over any well-defined region which
presumably wouldn't include water if we so choose.



\begin{comment}


\section{A simulation study under alternatives}

The point of this section is to evaluate whether we can effectively reject goodness of fit under various alternatives. In particular, we consider two specific modes of lack of fit:

(1)	Can we detect a lack of fit in the detection model?

(2)	Can we detect a lack of fit in the point process model?

4 or 5 detection models and 3 ponit process models.

7 x 7 grid, set to up to yield 50 individuals out of 100. Also try 49 pseudo-random or systematic points.


\subsection{Point Process Alternatives}

1.	Extreme regularity
2.	Somewhat? Regular
3.	Extreme clustering (multiple guys with same home range center)
4.	Somewhat clustered
5.	Complete randomness
6.	Spatial drift in the form of a spatial covariate or trend


We conducted a limited simulation study to evaluate the basic SCR
model under point processes that deviate from complete spatial
randomness. (To be concise we might use the term "uniformity" because
the abbreviation CSR looks too much like SCR.) Specifically, we
consider two deviations from complete spatial randomness: clustered
points, and points that are more systematically distributed than under
complete spatial randomness. We evaluate two questions: (1) how
sensitive is the density estimate to violation of complete spatial
randomness? (2) how much power does the GoF test have?


It is clear that there are many possible influences of both power and
effect of tests for CSR.  For example, N and lambda interact to affect
the expected size of the data set (n individuals) and also sigma
interacts with trap configuration and spacing to affect the number of
unique traps that individuals are captured in. It would be impossible
to catalog an exhaustive set of "what ifs" and so, instead, we focus
on the limited situation where N=100 on a state-space with parameters
set to obtain about 44 individuals on average. We simulated encounters
on a 5x5 grid of traps, unit spacing, with a 2 unit buffer defining
the state-space.

We simulated 3 point process models: Model 1 is CSR. Model 2 is a
Poisson cluster process (PCP) and Model 3 is a point process generated
to be more systematically distributed than CSR. For the Poisson
cluster process, 100 "parents" were distributed randomly over the
state space and the number of offspring for each parent was assumed to
be Poisson with mean 4. Offspring locations were generated according
to a bivariate normal distribution around the parent location, with a
standard deviation of 0.5.  This generated a list of many more than
100 final offspring locations within the state-space, and so we kept
the first 100 points within the state-space and discarded the
remaining. Effectively the last cluster is truncated if the cumulative
sum of offspring within the state-space is not precisely equal to 100.
For the systematic point process we generated 100 uniformly
distributed points and used that as a starting point to obtain the
"space-filling design" (Royle and Nychka XXXX) which is implemented in
the R package {\it fields} (Nychka et al XXXX) using the function {\it
  cover.design}.


Things to vary.

Point process. [random, clustered, regular]   3 levels

Trapping grid: 5 x 5 unit spacing. Also try random traps?   2 levels

State-space: buffered by 2 or 3 units.   2 levels

[N,lambda] fixed at a single value.

[sigma] = .75

GoF grid size = 100 cells, 225 cells, 400 cells with 0, 1 or 2 buffer?

Table:
                      Mode N  Mean N     RejectCSR   meanN(reject) modeN(reject)
CSR
PCP
SYS


4.6.2. GoF under complete spatial randomness.

4.6.3. GoF under the Poisson cluster process.

4.6.4. GoF under the inhibition process.

\end{comment}


\section{ Summary and Outlook  }


We offered some general strategies for model selection and model
checking. 
We recommend model averaging over a small set of distinct models for
estimating density. But we think using a single model or the best
model for asessing the importance of effects is probably ok despite
that the SE's will be biased. 

We think the selection ideas we talked about are probably ok, although
some more work is needed on Bayesian model selection to make it more
useful.  The GoF is pretty experimental. There is not a lot of
guidance in the literature on GoF in SCR models. 
There is essentially nothing known at all about power to detect various
departures.  There is some interesting and useful research to be done
on this. Do we have power to detect departures and does it matter in
terms of estimating $N$? Our specific analyses suggest not, but we
know of no theory or specific empirical study that has addressed this
issue. 









\subsection{What to do if CSR rejected?}

What can we do? We don't know.. depends on the laternative and how sensitive estimates are?









