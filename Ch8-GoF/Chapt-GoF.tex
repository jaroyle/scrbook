\chapter{
Model Selection and Assessment
}
\markboth{Model Assessment}{}
\label{chapt.gof}

\vspace{.3in}

Our purpose in life is to analyze models. By that, we mean one 
or more of the following basic 4 tasks: estimate parameters, make
predictions of unobserved random variables (e.g., of density),
evaluate the relative merits of different models or choosing a best
model (model selection), and we check whether a specific model appears
to provide a reasonable description of our data or not (assessment).
In previous chapters we have addressed the problem of estimation
extensively, and also making predictions of latent variables either
${\bf s}$ or density or population size.  In this chapter, we focus on
the last two of these inference tasks: model selection (which model or
models should we favor), and model assessment (do the data appear to
be consistent with a particular model?).

In the context of SCR models we discuss some specific methods of model
selection and assessment, and focus on a few specific application
contexts such as choosing among certain covariates, or other aspects
of model structure.  We discuss the use of AIC and DIC for this
purpose, and also the ``indicator model selection'' approach of
\citet{kuo_mallick:1998}.  To check model adequacy, or whether a specific model provides
a satisfactory description of our data set (i.e., ``goodness-of-fit''), we rely 
exclusively on Bayesian p-values \citep{gelman_etal:2006}.
For checking  adequacy of SCR models, 
part of the challenge is coming up with good summaries of model fit, 
and there does not appear to be any guidance on
this in the literature.  Following \citet{royle_etal:2011mee}, we break the problem up into 2 components
which we attack separately: (1) Does the encounter 
model fit? (2) Does the uniformity assumption appear adequate? The latter
component of model fit has a huge amount of precedent in the ecological literature as it is
analogous to the classical 
problem of testing  ``complete spatial randomness.'' 
% Really this is a test only of a
%form of ``spatial randomness'' because CSR implies the use of a
%Poisson point process which, as we discussed, is not universally
%adopated in our implementation of SCR models.

A basic problem with these two objectives of model selection and model
assessment is their simultaneous use implies a kind of contradiction
which we call the
{\it model selectors paradox}: Inferences are always achieved using
standard paradigms of parametric inference (Bayesian or frequentist)
which asssert that the model is properly specified. That is, we assume
that the model is 
truth. This is paradoxical because we all know that ``all models are
wrong'' but possibly, ``some are useful'' In fact, the notion that an
``assumption'' could even be correct is itself something of an oxymoron.
Therefore we don't expect
or hope to make assumptions that are ``correct'' in any way. Gelman
and Shalizi (2010) say it this way: ``there is general agreement that,
in this domain, all models in use are wrong -- not just merely
falsifiable, but actually false.''
We should therefore refrain from over-stating the relevance of any model. 

In this chapter we develop basic strategies of model selection and
model assessment or checking using both likelihood methods (as
implemented in the \mbox{\tt secr} package) and also Bayesian
analysis.  We apply these methods to the wolverine camera trapping
data to investigate sex specificity of model parameters and whether
there is a behavioral response to encounter. We note that individuals
are drawn to the camera trap devices by food bait and therefore it
stands to reason that once an individual discovers a trap, it might
return subsequently for that benefit, a response commonly referred to
as ``trap happiness.''







\begin{comment}
A few specific questions to resolve:
\begin{itemize}
\item[(1)]  Can AIC or DIC choose among detection models and does it matter?
\item[(2)]  Can AIC or DIC choose among more substantial models?  We should
 assume it does. Can we rig up a test?  Suggest: (1) trap level
 covariate and trying DIC; (2) Model Mb vs. not
\item[(3)] Simultaneous to above, see if we can do that.......
Posterior model probabilities: Can we demo this?   With detection models?
 Don't do much analysis of the situation, just demo it maybe?
\item[(4)]  Basic framework for Bayesian GoF. Power under alternatives.
(clustering, regularity, uniformity) and misspecified encounter models
-- do they show a lack of fit?
\end{itemize}
\end{comment}



\begin{comment}
\subsection{The Role of model assumptions}

Referees are quick to point out that bivariate normal distributions as
models for home range are simply wrong. As if using wrong models
somehow invalidates something. (is that a statistical paradigm that I
missed?). The issues are whether its an adequate model, whether we can
refute it, whether resulting inferences are sensitive to it, and
whether the model is useful for its intended purpose regardless.
If  we really wanted a great description of a home range then we would do
something different besides conduct an SCR study. The purpose of most
SCR models is not to study home range geometry and morphology but, rather,
to estimate density and possibly other vital rates such as survival and
recruitment. In addition, it is my experience that the same people who
criticize models as implying overly simple models actually cannot describe
alternative models except using jargon and procedures like "kernel
smoothing", "utilization distributions" and "neural networks". So, on
the one hand, the bivariate normal distribution is overly simple, and
therefore we should use "neural networks"?  What irritates me even more
is referees who emphatically assert that "real home ranges aren't bivariate
normal" and then they go on to cite references who somehow "proved" they
are not. These people have no clue what statistics is good for and what
the point of SCR models is, nor can they grasp the relevance of the home
range model - namely, as a model for explaining heterogeneity in
capture-probability. To be sure, the bivariate normal model is an
over-simplification but so far it has not been shown to be ineffective
in SCR problems, and besides  substantially more flexible models have been
developed \citep{royle_etal:2012ecol}.

What we care about in models of capture-recapture data is whether the
bivariate normal (or any model) provides an adequate description of the
encounter process. That is the question we will evaluate here.
\end{comment}


\section{General Strategies for Model Selection}

We review a number of standard methods of model selection that apply
to ``variable selection'' problems. That is, when our set of models
consists of distinct covariate effects and they represent constraints
of some larger model achieved by setting some of the parameter values
to 0.  For classical analysis based on likelihood, model selection by
AIC is the standard approach \citep{burnham_anderson:2002}.  For
Bayesian analysis we rely on a number of different methods.  We
demonstrate the use of DIC here for variable selection problems
although we recommend against its general use (see below).  We use the
Kuo and Mallick indicator variable selection approach
\citep{kuo_mallick:1998} which, although its implementation in the
{\bf BUGS} packages can be tempramental, it produces direct statemtns
of posterior model probabilities which we think are the most useful,
and leads directly to model-averaged estimates of density.  There is a
good review paper recently by \citet{ohara_sillanpaa:2009} that hits
on these and many more related ideas for variable selection.  We have
not done a comprehensive evaluation of these different methods for
effect and efficiency.  In addition to \citet{ohara_sillanpaa:2009} we
also recommend \citet[][Chapt. 7]{link_barker:2010} for general
information on model selection and assessment.


\subsection{Scope of the model selection problem}

There are two distinct classes of problems that we encounter in SCR
models which might require some type of model selection or ranking
effort: (1) Choosing among models that represent distinct, meaningful
biological hypotheses; and, (2) choosing among different parametric
encounter probability models. We believe that the importance of model
selection depends on which type of problem we have.

{\flushleft {\bf Choosing among biological models:}}
SCR models that represent extensions of the basic null model by
including specific covariates or other effects often represent
explicit biological hypotheses. Examples include models with a
behavioral response, or seasonal variation in encounter probability,
or sex-specificity of model parameters.
We anticipate that such basic biological factors 
could be important, and therefore it can be useful to choose among (or
rank) a set of models that represent these hypotheses. Indeed, they
might sometimes be of direct interest although usually only
secondary to estimating or modeling density.

{\flushleft {\bf Choosing among models for encounter probability:}}
In sec. \ref{chapt.scr0.implied} we introduced the notion that
encounter probability models imply specific models of space usage, an
idea we expand on and generalize in Chapt. \ref{chapt.rsf}. Because of
this linkage it is tempting to want to choose among them so that we
can proclaim something reasonable about space usage. Our feeling about
choosing among encounter probability models is determined by the
following two considerations:  (1) 
usually trap density is low and so we imagine one would have little or
no power to effectively choose among models and (2) the model itself
is not a biological construct, just the implied home range area is. As
stationary and isotropic models, they are all equally dumb as models
for space usage and so it seems to us that choosing among a dozen or
more arbitrary parametric forms that have no biological motivation
should tend to lead to an over-fitting.
So we will apply ideas of model selection to  some problems below (and
elsewhere in this book) but we avoid the problem of choosing among
detections functions and we discourage people from doing that. 



\subsection{Model selection by AIC}
\label{gof.sec.aic}

If you're doing  classical analysis based on likelihood then model
selection is easily accomplished using AIC \citep{burnham_anderson:2002}
which we demonstrate below. The AIC of a model is simply twice the
negative log-likelihood evaluated at the MLE,  penalized by the number of parameters
($k$) in the model:
\[
 AIC = -2 \mbox{\tt logL}({\bf y}|\hat{\bm \theta})  + 2 k
\]
Models with small values of AIC are preferred. 
It is common to use a modified AIC referred to as $AIC_{c}$ for small
sample sizes which is
\[
 AIC_{c}  = 
-2 \mbox{\tt logL}({\bf y}|\hat{\bm \theta})  + \frac{2 k
  (k+1)}{n-k-1}
\]
where $n$ is the sample size.  Two important problems with the use of
$AIC$ and $AIC_{c}$ are that they don't apply directly to hierarchical
models that contain random effects, unless they are computed directly
from the marginal likelihood (for SCR models we can do this, see
Chapt. \ref{chapt.mle}). Moreover, it is not clear what should be the
effective sample size $n$ in calculation of $AIC_{c}$, as there can be
covariates that affect individuals, that vary over time, or in space.
We do not offer strict guidelines as to when to use a small sample
size adjustment or not.

The {\bf R} package \mbox{\tt secr} computes and outputs AIC
automatically for each model fitted and it provides some capabilities
for producing a model selection table (function \mbox{\tt AIC}) and
also doing model-averaging (function \mbox{\tt model.average}), which
we recommend for obtaining estimates of density from multiple models.
We provide an example of using \mbox{\tt secr}, to analyze the
wolverine camera trapping data using 4 distinct models to accommodate
various types of sex specificity (see sec. XXXXX):
\begin{itemize}
\item[ Model 1:] model SCR0 with constant parameter
values for both male and female wolverines but with a parameter
$\psi_{sex}$ the population proportion of males;
\item[ Model 2:] sex-specific intercept
$p_{0}$ but constant $\sigma$; 
\item[ Model 3:] sex-specific $\sigma$ but constant
$p_{0}$ 
\item[ Model 4:] sex-specific $p_{0}$ {\it and} $\sigma$. 
\end{itemize}
The default in \mbox{\tt secr} is to model covariates on $p_{0}$ on
the logit-scale, and covariates on $\sigma$ on the log-scale according
to:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \alpha_{sex} \mbox{\tt sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \beta_{sex} \mbox{\tt sex}_{i}
\]

To fit these models we use the multi-session formulation provided by
\mbox{\tt secr} (introduced in sec. \ref{mle.sec.multisession}), which
allows one to model sesssion-specific effects on density, baseline
encounter probability, $p_{0}$ ($g_{0}$ in \mbox{\tt secr}), and also the scale
parameter $\sigma$ of the encounter probability model. Using this
formulation, we define the ``Session'' variable to be a sex code and
thus session-specific parameters represent sex-specific parameters.
For example, if we model session-specific density, $D$, then this
corresponds to Model 1 in our list above.  We note that the use of
multi-session models like this suggests 
one additional model which we haven't itemized above, that being the
model with {\it no} sex effect on any parameter, which is equivalent
to fixing $\psi_{sex} = 0.5$ instead of estimating it. We will label
this last model ``Model 0'' and include it in our analysis below, even
though we may not generally think of it as a natural candidate
model.

Here are the {\bf R} commands for loading the wolverine data and doing
a slight bit of formatting to prepare the data objects for analysis by
\mbox{\secr}. The key difference from our analysis in
Chapt. \ref{chapt.mle} is, here, we grab the wolverine sex information
(\mbox{\tt wolverine\$wsex}) which is a 0/1 indicator (1=male). We add
1 to that and then use it to define the ``Session'' variable based on sex.
The {\bf R} commands are as follows:
{\small
\begin{verbatim}
library("secr")
library("scrbook")
data("wolverine")
traps<-as.matrix(wolverine$wtraps)   
dimnames(traps)<-list(NULL,c("trapID","x","y",paste("day",1:165,sep="")))

trapfile2<-scr2secr(scrtraps=traps,type="proximity")

gr<-as.matrix(wolverine$grid2)
dimnames(gr)<-list(NULL,c("x","y"))
gr2<-read.mask(data=gr)

wolv.dat<-wolverine$wcaps
dimnames(wolv.dat)<-list(NULL,c("Session","ID","Occasion","trapID"))
wolv.dat[,1]<-wolverine$wsex[wolv.dat[,2]]+1
wolv.dat<-as.data.frame(wolv.dat)
wolvcapt3<-make.capthist(wolv.dat,trapfile2,fmt="trapID",noccasions=165)
\end{verbatim}
}


Once the data are have been prepared in this way we use the 
\mbox{\tt secr} model fitting function \mbox{\tt secr.fit} to fit a
number of different models and then the function \mbox{\tt AIC} to
package the models together and summarize them in the form of an AIC
table, with rows of the table ordered from best to worst. The function
\mbox{\tt model.average} performans AIC-based model-averaging of the 
parameters specified by the \mbox{\tt realnames} variable (below this
is demonstrated for the parameter density, $D$).  Because this
function defaults to averaging by AICc, we slightly modified
this function (called \mbox{\tt model.average2}) to do model averaging
by either  AIC or AICc as specified by the user. Together
these commands and resulting output (abbreviated to fit on page) look like this:
{\small 
\begin{verbatim}
XXX note to reader: I will delete some horizontal output XXXXX

model0<-secr.fit(wolvcapt3,model=list(D~1, g0~1, sigma~1), buffer=20000)
model1<-secr.fit(wolvcapt3,model=list(D~session, g0~1, sigma~1), buffer=20000)
model2<-secr.fit(wolvcapt3,model=list(D~session, g0~session, sigma~1), buffer=20000)
model3<-secr.fit(wolvcapt3,model=list(D~session, g0~1, sigma~session), buffer=20000)
model4<-secr.fit(wolvcapt3,model=list(D~session, g0~session, sigma~session), buffer=20000)

> AIC (model0,model1,model2,model3,model4)
                                    model   detectfn npar    logLik      AIC     AICc dAICc  AICwt
model0                   D~1 g0~1 sigma~1 halfnormal    3 -627.2603 1260.521 1261.932 0.000 0.5831
model2       D~session g0~session sigma~1 halfnormal    5 -624.9051 1259.810 1263.810 1.878 0.2280
model1             D~session g0~1 sigma~1 halfnormal    4 -627.2365 1262.473 1264.973 3.041 0.1275
model4 D~session g0~session sigma~session halfnormal    6 -624.6632 1261.326 1267.326 5.394 0.0393
model3       D~session g0~1 sigma~session halfnormal    5 -627.2358 1264.472 1268.472 6.540 0.0222

> model.average (model0,model1,model2,model3,model4,realnames="D")
              estimate  SE.estimate          lcl          ucl
session=1 2.707190e-05 7.913577e-06 1.544474e-05 4.745224e-05
session=2 2.927423e-05 8.270402e-06 1.700631e-05 5.039193e-05
\end{verbatim}
}
As usual, estimates and standard errors of the individual model
parameters can be obtained from the \mbox{\tt secr.fit} summary output
of any of the \mbox{\tt modelX} objects shown above. 
The default output of estimated density is in individuals per ha, so
we have to scale this up to something more reasonable. To get into
units of per 1000 $km^2$ we need to mulitply by 100 to get to units of
$km^2$ and then 1000. This produces an estimated density of 
about 2.71 for \mbox{\tt session=1} (females) and 2.93 for
\mbox{\tt session=2} (males).  We can use the generic {\bf R} function
\mbox{\tt predict} applied to the \mbox{\tt secr.fit} output to obtain
 specific information about the MLEs on the natural scale.

We don't necessarily agree with the use of AICc here and think its
better to use AIC, in general. This is because its not clear what the
effective sample size is for most capture-recapture problems. While we
have 21 individuals in the data set, most of the model structure has
to do with encounter probability samples and for that there are 100s
of observations. We note that the AIC and AICc results are not
entirely consistent.  By looking at the best model by AIC, we find
that the model with estimated sex ratio and sex specific $g_{0}$ is
preferred (Model 2). This is just slightly better than the model with
a fixed sex ratio of $\psi_{sex} = 0.50$.

We do the same things with the mask which excludes ocean. 
Results are given in Tab. \ref{gof.tab.aic} 
along with the previous models without a mask. {\bf XXX WE SEE THIS AND
THAT XXXXX }
We see AIC is better for the model without the mask -- it is to
compare these by AIC
 because we recognize the mask as changing the random
effects distribution and the results should be sensitive to that. That
said, we may not like the non-mask model because it makes sense to
exclude the water area from the state-space  of ${\bf s}$.
For females the model-averaged density is 3.88 individuals per 1000 $km^2$ and for
males the model-averaged density estimate is 4.46 individuals per
1000 $km^2$ as we see here:
\begin{verbatim}
> model.average (model0b,model1b,model2b,model3b,model4b,realnames="D")

              estimate  SE.estimate          lcl          ucl
session=1 3.876615e-05 1.189102e-05 2.153795e-05 6.977518e-05
session=2 4.459658e-05 1.323696e-05 2.523280e-05 7.882022e-05
\end{verbatim}
This is a bit higher than that based on the rectangular state-space
(i.e., not specifying a habitat mask). 

\begin{verbatim}
more compact model notation:
D,g0,sigma
D(sex),g0,sigma
D(sex),g0(sex),sigma
D(sex),g0, sigma(sex)
D(sex),g0(sex),sigma(sex)

models 0-4 without and with mask 

> round(out1,2)
       D    p0   sigma   D   p0   sigma
[1,] 2.83 0.06 6298.66 2.83 0.06 6298.66
[2,] 2.69 0.06 6298.69 2.96 0.06 6298.69
[3,] 2.45 0.08 6435.51 3.16 0.04 6435.51
[4,] 2.70 0.06 6280.49 2.95 0.06 6319.03
[5,] 2.59 0.08 6080.70 2.99 0.04 6833.16
> round(out2,2)
     [,1] [,2]    [,3] [,4] [,5]    [,6]
[1,] 4.18 0.05 6282.62 4.18 0.05 6282.62
[2,] 3.98 0.05 6282.65 4.38 0.05 6282.65
[3,] 3.64 0.07 6382.88 4.73 0.03 6382.88
[4,] 3.93 0.05 6357.26 4.41 0.05 6220.22
[5,] 3.87 0.07 5859.40 4.41 0.03 7039.09


\end{verbatim}



\begin{table}[ht]
\centering
\caption{TABLE: AIC Results wolverine data with/without habitat mask. fitted
in secr. half-normal encounter probability model. Models ordered by
AIC within each class of models (no mask or with mask).  Density, D,
reported in units of individuals per 1000 $km^2$. 
}
\hline \hline
\hspace{1in} NO HABITAT MASK \\
\begin{tabular}{lcccc}
  model & npar & AIC & AICc & D \\ \hline
D~1 g0~1 sigma~1                &    3&  1260.521& 1261.932 &\\
D~session g0~session sigma~1    &    5&  1259.810& 1263.810 &\\
D~session g0~1 sigma~1          &    4&  1262.473& 1264.973 &\\
D~session g0~session sigma~session&  6&  1261.326& 1267.326 &\\
D~session g0~1 sigma~session      &  5&  1264.472& 1268.472 &\\
\end{tabular}
\hline \hline
\hspace{1in} WITH HABITAT MASK \\
\begin{tabular}{lcccc}
  model & npar & AIC & AICc & D \\ \hline
D~session g0~session sigma~1       &    5& 1268.096& 1272.096 & \\
D~session g0~session sigma~session &    6& 1268.698& 1274.698 &\\
D~1       g0~1       sigma~1       &    3& 1271.163& 1272.574 &\\
D~session g0~1       sigma~1       &    4& 1273.115& 1275.615 &\\
D~session g0~1       sigma~session &    5& 1275.089& 1279.089 &\\
\end{tabular}
\hline
\label{gof.tab.aic}
\end{table}





\subsection{Bayesian model selection}

Model selection is somewhat less straightforward
as a Bayesian and there is no such canned all-purpose method like AIC
As such we recommend more of a pragmatic approach, in general, for all
problems, based on a number of basic considerations:
\begin{itemize}
\item[(1)] For a small number of fixed effects we think it is
  reasonable to adopt a conventional ``hypothesis testing'' approach
  -- i.e., if the posterior for a parameter overlaps zero
  substantially, then it is probably reasonable to discard that
  effect.
\item[(2)] Calculation of posterior model probabilities: In some cases
  we can implement methods which allow calculation of posterior model
  probabilities. One such idea is the indicator variable selection
  idea from \citet{kuo_mallick:1998}.  The idea is introduce a latent
  variable $I \sim \mbox{Bern}(.5)$ and expand the model to include
  the variable $I$ as follows:
\[
 \mbox{logit}(p_{ijk}) = \alpha_{0} + I*\alpha_{1}*C_{ijk}.
\]
The importance of the covariate $C$ is then measured by the posterior
probability that $I=1$.  
\item[(3)] DIC -- the Deviance Information Criterion: Bayesian model
  selection is now routinely carried-out using the Deviance
  Information Criterion (DIC; Spiegelhalter et al. 2002) although its
  effectiveness in hierarchical models depends very much on the manner
  in which it is constructed \citep{millar:2009}.  We recommend using
  it if it leads to sensible results but also we think it should be
  calibrated to the extent possible for specific classes of models.
\item[(4)] Logical argument: For something like M/F differences it
  seems to make sense to leave an extra parameter in the model no
  matter what, because of course you expect, biologically, there to be
  a difference in these things. e.g., in the above example we don't
  really ever think that $psi_{sex} = 0.5$ and so we might not
  typically admit that model into our model set. Thus the hypothesis
  test itself is meaningless, a form of gratuitious hypothesis testing
  \citep{johnson:1999}.
\end{itemize}
In all modeling activities, as in life itself, the use of logical argument should not be under-utilized.

\subsection{Deviance Information Criterion (DIC) }

AIC makes the use of likelihood methods convenient for problems where
likelihood estimation is achievable.  For Bayesian analysis, the
development of Deviance Information Criterion (DIC)
\citep{spiegelhalter_etal:2002} seemed like a general-purpose
equivalent for a brief period of time after its invention.  However,
there seems to be many types of DIC, and no consistent version is
reported across computing platforms, and our own experience with
 calibration has indicated highly variable effectiveness of DIC. Even
 academic 
statisticians don't have a unified front on practical issues related
to the use of DIC \citep{millar:2009}.  That said, it is still widely reported and, we
think, probably reasonable for certain classes of fixed effects
models.

Model deviance is negative 2 times the loglikelihood. i.e.,
for a given model with parameters $\theta$:
$Dev(\theta) = -2*logL({\bf y}|\theta)$.
The DIC is defined as the posterior mean of the deviance plus a 
measure of model complexity, $p_{D}$:
\[
 DIC = \overline{Dev}(\theta) + p_{D}
\]
AS $p_{D}$ does not have a clear definition in hierarchical models
with latent structure, the standard definition of $p_{D}$ is
\[
 p_{d} = \overline{Dev}(\theta) - Dev(\bar{\theta})
\]
where the 2nd term is the deviance evaluated at the posterior mean of
the parameter(s). The $p_{D}$ here is interpreted as the effective
number of parameters in the model. 
Gelman et al. (2004) suggest a different version of $p_{D}$ based on
one-half the posterior variance of the deviance:
\[
 p_{V} = \mbox{Var}(\mbox{Dev}(\theta)|{\bf y})/2.
\]
This is what is produced from {\bf WinBUGS} and {\bf JAGS} if they are
run
from \mbox{\tt R2WinBUGS} and \mbox{\tt R2jags}, respectively.
It is less easy to get DIC summaries from \mbox{\tt rjags}, so we have
used \mbox{\tt R2jags} in our analysis here. 

We did a Bayesian analysis of the 4 models 
mentioned above (models 1 - 4, but not ``Model 0'' which assumed $\psi_{sex} =
0.5$).
Following our analysis in \mbox{\tt secr} above, we used the
$logit(p_{0}), log(\sigma)$ parameterization of the models:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + \alpha_{sex} \mbox{\tt sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + \beta_{sex} \mbox{\tt sex}_{i}
\]
In addition, unlike the multi-session model in \mbox{\tt secr}, we
explicitly account for the covariate 'sex' in the model by assigning
it a Bernoulli prior distribution with $\psi_{sex}$ being the
proportion of males in the population. As usual, handling of missing
values of 'sex' is done seamlessly which might be a practical
advantage of doing a Bayesian analysis in situations where sex is
difficult to record in the field.  
The {\bf BUGS} model specification for model 4 is shown in Panel
\ref{gof.panel.sexmodel}. 

 We have an {\bf R} function called \mbox{\tt wolvSCR0ms.fn}
 in the \mbox{\tt scrbook} package which will fit each model. The
 kernel of the BUGS model specification is shown in Panel XXXX below.
The function uses {\bf JAGS} by default for the fitting, using the \mbox{\tt
  R2jags} package.  Here is how we obtain the results each model:
{\small
\begin{verbatim}
> toad1<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=1)
> toad2<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=2)
> toad3<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=3)
> toad4<-wolvSCR0ms.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=4)
\end{verbatim}
}

The R script in \mbox{\tt wolvSCR0ms.fn}
can fit each of the 4 models by using a binary
indicator variable to turn 'on' or 'off' each effect. The binary
indicator variable is fed into the bugs or jags call as data.

\begin{panel}[htp]
\centering
\rule[0.15in]{\textwidth}{.03in}
%\begin{minipage}{5in}
{\small
\begin{verbatim}
alpha.sex ~ dunif(-3,3)
beta.sex  ~ dunif(-3,3)
sigma0~dunif(0,50)
alpha0~dnorm(0,.1)
beta<- (1/(2*sigma*sigma) )
psi ~ dunif(0,1)
psi.sex  ~ dunif(0,1)
for(i in 1:M){
  wsex[i] ~ dbern(psi.sex)
  w[i]~dbern(psi)
  s[i,1]~dunif(Xl,Xu)
  s[i,2]~dunif(Yl,Yu) 
  logit(p0[i])<- alpha0 + alpha.sex*wsex[i]
  log(sigma.vec[i])<- log(sigma0) + beta.sex*wsex[i]
  beta.vec[i]<- 1/(2*sigma.vec[i]*sigma.vec[i])
for(j in 1:ntraps){
  mu[i,j]<-w[i]*p[i,j]
  ncaps[i,j]~ dbin(mu[i,j],K[j]) 
  dd[i,j]<- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2) 
  p[i,j]  <-  p0[i]*exp( - beta.vec[i]*dd[i,j] )

}
}
\end{verbatim}
}
%\end{minipage}
\rule[-0.15in]{\textwidth}{.03in}
\caption{
Part of the BUGS specification for a complete sex-specificity of model
parameters.   XXXXX probably requires some notation changes XXXXXXX
}
\label{gof.panel.sexmodel}
\end{panel}


Summarize model parameters for each model...... estimates ... and
deviance, np and DIC for each model. 
The output from these models is summarized in Table XXXXX.
These are based on 21000 iterations, 3 chains, 1000 discarded, 60k
posterior samples.......
what we see is: XYZ.....
I think we get the same model by DIC as we get by AIC. This is super
encouraging!

 
\begin{verbatim}
          beta param
                                     sigma param
           dev  np     dic         dev    np     dic 
Model 1  441.78 78.3  520.0       441.73 78.1   519.8
Model 2  440.97 82.4  523.4       439.81 78.3   518.1 
Model 3: 443.68 80.9  524.6       443.29 79.9   523.2 
Model 4: 441.07 77.3  518.3       441.27 81.1   522.4 

DIC1:   4, 1, 2, 3
AIC:    2, 4, 1, 3
DIC2:   2, 1, 4, 3
AICc    2, 1, 4, 3
AICmask 4  2  3  1
AICcmsk 2  4  1  3
\end{verbatim}



\begin{table}[htp]
\centering
\caption{XXX this will be in a table XXXXX
The following model outputs report the log(sigma) model............................
21k iters, 1k burn, 3 chains = 60k iters
all Rhat < 1.01
}
\begin{tabular}
parameter     model 1          model 2      model 3           model 4
            mean    sd       mean    sd       mean    sd      mean    sd
D           5.776   1.121   5.723   1.136    5.714   1.145   5.677   1.156 
N          59.925  11.627  59.370  11.790   59.275  11.874  58.892  11.995 
alpha0     -2.816   0.175  -2.439   0.248   -2.813   0.176  -2.420   0.259 
alpha.sex   0.003   1.735  -0.743   0.343    0.001   1.732  -0.810   0.358 
beta        1.246   0.208   1.190   0.206    1.222   0.296   1.295   0.334 
beta.sex   -0.013   1.733   0.009   1.741   -0.012   0.172   0.087   0.187 
sigma       0.640   0.054   0.656   0.059    0.654   0.083   0.639   0.093 
psi         0.302   0.066   0.299   0.066    0.298   0.067   0.297   0.068 
psi.sex     0.520   0.102   0.561   0.102    0.524   0.108   0.543   0.112 
deviance  441.731  12.499 439.814  12.516  443.285  12.661 441.272  12.743 
pD = 78.1 D               pD = 78.3         pD = 79.9         pD = 81.1 
DIC = 519.8               DIC = 518.1       DIC = 523.2       DIC = 522.4
\end{tabular}
\label{blahblahblah}
\end{table}


\subsubsection{Checking some things}


XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
check priors
A final point: We have dunif(-3,3) priors. Check normal prior and
check dunif(-5,5).  

status: running dunif(0,.1)


XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Check WinBUGS too.  Got this job queued up ....
RESULT: Pretty similar overall and in terms of DIC. Probably things
are working right.
model selection results:
a
   00    01    10    11 
22599  1328 33535  2538 
> table(a)/length(a)
a
        00         01         10         11 
0.37665000 0.02213333 0.55891667 0.04230000 


XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX




\begin{comment}
As is pointed out in one of those posts, it's interesting to me that
both AIC and DIC are attempts at mimicking leave-one-out
cross-validation. Perhaps we could make the point that if DIC cannot
be trusted then one could always implement their own x-val
procedure... assuming it doesn't take months to complete.

Richard

On Thu, May 24, 2012 at 9:57 AM, Jeffrey Royle <jaroyle@gmail.com> wrote:
> hi all -- DIC is a hopeless black hole with, I think, no real possibility of
> making it seem reasonable to people.
> There is some good discussion on Gelman's blog, along with a number of
> papers (all cited there)
> http://andrewgelman.com/2006/07/number_of_param/
> http://andrewgelman.com/2011/06/plummer_on_dic/
> http://andrewgelman.com/2011/06/deviance_dic_ai/
>
> I gather that there are 3 version of DIC. The original one, and then
> Plummer's version which seems to be in JAGS and then the dumb version based
> on .5*var(deviance) which, I gather, you should never use -- I guess it
> sucks.
>
> I still can't compute Plummer's version using JAGS -- I think maybe I need
> to update my JAGS version or something.
\end{comment} 





\subsection{Bayesian model averaging: indicator variables}

A convenient way to deal with model selection and averaging problems
in Bayesian analysis by MCMC is to use the method of model indicator
variables \citep{kuo_mallick:1998}. Using this approach, we expand the
model to include a set of prescribed models as specific reductions of
a larger model.  This has been demonstrated in some specific
capture-recapture models in \citep[][sec. 3.xxx]{royle_dorazio:2008},
\citep{royle:2009} and in the context of SCR by
\citep{tobler_etal:2012}.  A useful aspect of this method is that
model-averaged parameters are produced by default. We emphasize the
need to be careful of reporting model-averaged parameters that don't
have a common interpretation in the different models because they are
meaningless.  For example, if a regression parameter is in a specific
model then the posterior is informed by the data and a specific MCMC
draw is from the appropriate posterior distribution. On the other
hand, if the regression parameter is not in the model then the MCMC
draw is obtained directly from the prior distribution, and so we need
to think carefully about whether it makes sense to report an average
of such a thing (in the vast majority of cases the answer is no). But
some parameters like $N$ or density, $D$, do have a consistent
interpretation and we support producing model-averaged results of
those parameters. Model averaging is therefore a useful endeavor in
spatial capture-recapture problem.s

To implement the Kuo and Mallick approach, we expand the model to
include the latent indicator variables say $I_{m}$, for variable $M$
in the model, such that
\begin{eqnarray*}
I_{m} = \left\{ 
\begin{array}{cc} 1 &  \mbox{ linear predictor includes  covariate $m$} \\
                  0 & \mbox{ linear predictor does not
                                include covariate $m$}
 \end{array}
\right.
\end{eqnarray*}
We assume that the indicator variables $I_{m}$ are mutually
independent with:
\[
I_m \sim \mbox{Bernoulli}(0.5).
\]
The expanded model has the linear predictor:
\[
\mbox{logit}(p_{ijk}) = \alpha_{0} + \alpha_{1}I_{1} C_{1,i} + \alpha_{2}I_{2} C_{2,ijk} 
\]
where, lets suppose, $C_{1,i}$ is an individual level covariate such
as sex, and $C_{2,ijk}$ is a behavioral response covariate which is
individual- trap- and occasion-specific.  We can assume a parallel
model specification on the parameter $\sigma$ which is liable to vary
by individual level covariates such as sex:
\[
 log(\sigma_{i}) = \beta_{0} + \beta_{1} I_{3} C_{1,i}
\]

In this formulation of the model selection problem we can characterize
unique models by the sequence of $I$ variables. In this case, each
unique sequence $(I_{1},I_{2},I_{3})$ represents  a model, and we can
tabulate the posterior frequencies of each model by post-processing
the MCMC histories of $(I_{1},I_{2},I_{3})$ (we demonstrate this
shortly). 

Conceptually, analysis of this expanded model within the data
augmentation framework does not pose any additional difficulty. One
broader, technical consideration is that posterior model probabilities
are well known to be sensitive to priors on parameters
\citep{aitkin_1991: link_barker:2006} and see also
\citep[][sec. 3.4.3]{royle_dorazio:2008} and
\citep[][sec. 7.2.5]{link_barker:2010}.  What might normally be viewed
as vague or non-informative priors, are not usually innocuous or
uninformative when evaluating posterior model probabilities. The use
of Akaike's information criterion seems to avoid this problem largely
by imposing a specific and perhaps undesirable prior that is a
function of the sample size \citep{kadane_lazar:2004}. One solution is
to compute posterior model probabilities under a model in which the
prior for parameters is fixed at the posterior distribution under the
full model \citep{aitkin:1991}. At a minimum, one should evaluate
posterior model probabilities under different vague prior
specifications.


\subsubsection{Wolverine data}

Our {\bf R} script \mbox{\tt wolvSCR0ms.fn} in \mbox{\tt scrbook} also
provides the model indicator variable implementation for the fully
sex-specific SCR0 model.  It is run by setting \mbox{\tt model=5}, and
the results are summarized in Table XXXX below. The variable 'mod'
contains the two binary indicator variables ($I$ above) which pre-multiply the 'sex'
term in each of the $p_{0}$ and $\sigma$ model components, like this:
\[
 \mbox{logit}(p_{0,i}) = \alpha_{0} + mod[1]*\alpha_{sex}*\mbox{\tt sex}_{i}
\]
and
\[
 \mbox{log}(\sigma_{i}) = \log(\sigma_{0}) + mod[2]*\beta_{sex}*\mbox{\tt sex}_{i}
\]
The MCMC output for 'mod' was post-processed to obtain the
model-weights. The {\bf R} commands are as follows:
\begin{verbatim}
> mod<-toad5$BUGSoutput$sims.list$mod
> mod<-paste(mod[,1],mod[,2])
> table(mod)
mod
  0 0   0 1   1 0   1 1 
20599  1488 35026  2887 
> table(mod)/length(mod)
mod
       0 0        0 1        1 0        1 1 
0.34331667 0.02480000 0.58376667 0.04811667 
\end{verbatim}
We see that th emodel with sex-specific baseline encounter probability
$p_{0}$ has posterior model weight of $0.584$, the model with no sex
effect has posterior probability $0.34$ and the remaining posterior mass is distributed
over the other two models. 


\begin{table}[htp]
\centering
\caption{Inference for Bugs model at "modelfile5.txt", fit using jags,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
pD = 77.9 and DIC = 518.6
{\bf XXXX I'm thinking this table isn't needed
  XXXXXXXXXXXXXXXXXXXX }
}
\begin{tabular}{lrrrrrrrrr}
          mu.vect sd.vect    2.5\%     25\%     50\%     75\%   97.5\%  Rhat n.eff
D           5.726   1.136   3.759   4.916   5.591   6.458   8.193 1.003   870
N          59.401  11.781  39.000  51.000  58.000  67.000  85.000 1.003   870
alpha0    -2.579   0.285  -3.082  -2.791  -2.595  -2.380  -2.004 1.001  5000
alpha.sex  -0.472   1.138  -2.574  -1.016  -0.689  -0.270   2.586 1.001 60000
sigma       0.650   0.059   0.547   0.609   0.645   0.687   0.777 1.005   510
beta.sex    0.016   1.671  -2.841  -1.356   0.028   1.403   2.840 1.001 60000
mod[1]      0.632   0.482   0.000   0.000   1.000   1.000   1.000 1.001  6300
mod[2]      0.073   0.260   0.000   0.000   0.000   0.000   1.000 1.006  1300
psi         0.299   0.066   0.184   0.252   0.294   0.341   0.443 1.003  1000
psi.sex     0.542   0.104   0.338   0.471   0.544   0.614   0.741 1.002  3300
deviance  440.660  12.494 418.328 431.871 439.912 448.644 467.077 1.003  1100
\end{tabular}
\end{table}


\subsubsection{Choosing among detection functions}

Another approach to implementing model indicator variables is 
 to introduce a categorical model identity variable
which, i.e., so that 
 ``model identity'' is itself a parameter of the model and each
 distinct model is associated with a unique covariate combination or
 other set of model features. This is convenient especially when we
 cannot specify the linear predictor as some general model that
 reduces to various alternative sub-models simply by switching binary
 variables on or off. In the context of SCR models, choosing among
 different encounter probability models would be an example.
 For this case we do something like this:
\begin{verbatim}
 mod \sim \mbox{dcat}(probs[])
\end{verbatim}
where \mbox{\tt probs[l] = 1/(\# models)}. Then we can fill in a bigger
array of $p$ instead of $p[i,j]$ we build $p[i,j,l]$ for each of
$l=1,2,\ldots,L$ models and we define our data component like so:
\begin{verbatim}
pi[i,j]<- bigpi[i,j,mod]
y[i,j] ~ dbern(pi[i,j])
\end{verbatim}
(or something like that).
As before the posterior probabilities can be highly sensitive to
priors on the different model parameters and sometimes mixing is
really poor.  
But we give a {\bf JAGS} script in the {\bf R} package
\mbox{\tt scrbook} which has an example that works.
The model specificication for the wolverine data for 3 different
encounter probability models is shown in Panel 
\ref{gof.panel.modsel}: The Gaussian encounter probability model;
Gaussian hazard model; and logistic model. 
The key things to note are that there are 3 intercepts and 3 different
'\mbox{\tt beta}' parameters (the coefficient on distance). The 
parameters should not be regarded as equivalent across the models, so
it is important to have them seperately estimated for each
model. Then,  we create a probability of encounter for each
individual, trap {\it and} model so that the holder object '\mbox{\tt p}' in the
model description is a 3-dimensional array. (sometimes this would have to be a 4
or 5-d array in more complex models with time effects, etc..)
This is in the scrbook library called XXXXX XXXXXXX
run this code and report the results XXXXX XXXXXXXX


\begin{panel}[htp]
\centering
\rule[0.15in]{\textwidth}{.03in}
%\begin{minipage}{5in}
{\small
\begin{verbatim}
model {
for(i in 1:3){
  alpha0[i] ~ dnorm(0,.1)
  beta[i] ~ dunif(0,2)
  mod.probs[i]<- 1/3
}
psi ~ dunif(0,1)
psi.sex  ~ dunif(0,1)
catmod~dcat(mod.probs[])

for(i in 1:M){
 w[i]~dbern(psi)
 s[i,1]~dunif(Xl,Xu)
 s[i,2]~dunif(Yl,Yu) 
 logit(beta0.vec[i,1])<- alpha0[1]
 log(beta0.vec[i,2])<- alpha0[2]
 beta0.vec[i,3]<- alpha0[3]

 log(beta.vec[i,1])<- log( beta[1] ) 
 log(beta.vec[i,2])<- log( beta[2] ) 
 log(beta.vec[i,3])<- log( beta[3] ) 

 for(j in 1:ntraps){
   mu[i,j]<-w[i]*p[i,j,catmod]
   ncaps[i,j]~ dbin(mu[i,j],K[j]) 
   dd[i,j]<- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2) 
 
   p[i,j,1]  <-  beta0.vec[i,1]*exp( - beta.vec[i,1]*dd[i,j] )
   p[i,j,2] <-  1-exp(-beta0.vec[i,2]*exp( - beta.vec[i,2]*dd[i,j] ) )
   logit(p[i,j,3])<- beta0.vec[i,3] - beta.vec[i,3]*dd[i,j]
  }
}
N<-sum(w[1:M])
D<-N/area
}
\end{verbatim}
}
%\end{minipage}
\rule[-0.15in]{\textwidth}{.03in}
\caption{
Part of the BUGS specification
of the indicator variable idea to choose among
  different detection models..... this code is contained in an R
  script in scrbook package called XXXXXX.
}
\label{gof.panel.modsel}
\end{panel}







\subsection{Further analysis of the wolverine data}



We did a bunch of analysis previously with models that involved
sex-specific parameters. Here we expand the model set to include a
behavioral response. This is a little more difficult doing Bayesian
analysis because we have to do the 3-d version of the model which can
be a time-consuming task in WinBUGS. But lets do it anyway.
There are in this case 8 models (right?)


4 models with sex: DIC, model weights, AIC.

expanded model with behavioral response...... DIC , model weights, AIC......

\begin{verbatim}
Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.894	1.009	0.0171	4.18	5.822	8.062	1001	6600
	N	39.48	6.755	0.1145	28.0	39.0	54.0	1001	6600
	X1new	8.996	2.711	0.03361	4.551	8.695	15.15	1001	6600
	X1obs	11.87	3.354	0.05475	6.382	11.49	19.53	1001	6600
	X3new	13.15	3.232	0.03914	7.725	12.92	20.4	1001	6600
	X3obs	21.43	2.115	0.03256	17.8	21.24	26.07	1001	6600
	Xnew	61.78	6.517	0.1004	49.48	61.7	75.22	1001	6600
	Xobs	88.97	5.972	0.08912	78.08	88.63	101.5	1001	6600
	alpha.sex	-0.4506	1.158	0.02228	-2.571	-0.6846	2.594	1001	6600
	beta	2.423	4.111	0.3034	0.06847	1.165	17.51	1001	6600
	beta.sex	0.07415	1.761	0.1045	-2.85	0.0155	2.868	1001	6600
	deviance	439.3	12.16	0.2192	418.0	438.4	465.5	1001	6600
	logitp0	-2.577	0.2909	0.0114	-3.079	-2.597	-1.979	1001	6600
	mod[1]	0.6236	0.4845	0.01885	0.0	1.0	1.0	1001	6600
	mod[2]	0.5035	0.5	0.03697	0.0	1.0	1.0	1001	6600
	psi	0.2657	0.05634	8.482E-4	0.1653	0.2617	0.3855	1001	6600
	psi.sex	0.5449	0.1044	0.00231	0.336	0.5472	0.7417	1001	6600
	sigma	0.8442	0.6291	0.0507	0.1698	0.6551	2.704	1001	6600




run 2


Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.939	1.009	0.01712	4.18	5.822	8.211	1001	27000
	N	39.78	6.756	0.1147	28.0	39.0	55.0	1001	27000
	X1new	9.006	2.701	0.02151	4.603	8.699	15.18	1001	27000
	X1obs	11.98	3.396	0.04019	6.437	11.61	19.69	1001	27000
	X3new	13.13	3.218	0.01885	7.748	12.84	20.26	1001	27000
	X3obs	21.4	2.116	0.02102	17.83	21.2	26.11	1001	27000
	Xnew	61.54	6.483	0.0668	49.39	61.26	74.8	1001	27000
	Xobs	88.74	6.003	0.06467	77.85	88.46	101.4	1001	27000
	alpha.sex	-0.4355	1.192	0.01962	-2.624	-0.6607	2.605	1001	27000
	beta	1.257	2.088	0.1191	0.06376	1.036	6.652	1001	27000
	beta.sex	0.5605	1.675	0.06955	-2.723	0.7576	2.92	1001	27000
	deviance	438.9	12.25	0.1858	417.0	438.0	464.9	1001	27000
	logitp0	-2.598	0.287	0.01129	-3.088	-2.624	-1.998	1001	27000
	mod[1]	0.5939	0.4911	0.01784	0.0	1.0	1.0	1001	27000
	mod[2]	0.5251	0.4994	0.02614	0.0	1.0	1.0	1001	27000
	psi	0.3312	0.06925	0.001093	0.2101	0.3264	0.4776	1001	27000
	psi.sex	0.5425	0.1038	0.002119	0.3366	0.5443	0.7389	1001	27000
	sigma	1.045	0.6963	0.04014	0.2743	0.6947	2.801	1001	27000



\end{verbatim}








\section{Evaluating Goodness-of-Fit}

In practical settings, we estimate parameters of a desirable model, or
maybe fit a bunch of models and report estimates from all of them or a
model-averaged summary of density.  An important question is: Is our
model worth a shit?  In other words, are the data we have consistent
with realizations from the model which we just fitted and upon which
we are basing our inferences?

There is some interesting stuff here. If we do model selection should
all of the models have to fit? Or should the averaged model have to
fit? We don't know the answers to these questions. We suggest that if
you have a model that you really like, a single model, then it is a
sensible thing to check that the model should fit. 

Conceptually, we can think of model selection as follows: if we simulate
under that model, do the simulated realizations look like our data?
In a sense, it is that last element that is most relevant to science.
That is beyond the basic construction of the model which presumably
contains some basic scientific hypotheses and thus requires some
understanding and thinking about the system. After that it is the
assessment that provides what is essentially a basis for falsification
of the model, and hence learning..  By saying that the data are
inconsistent with a model, then we're rejecting at least one of the
hypotheses embodied by that model. This is not really true... it could
mean we're missing something, or many things. The basic model could be
correct, just not having enough stuff in it. 

For either Bayesian or classical inference, the basic strategy is to
come up with a fit statistic that depends on the parameters and the
data set, which we denote by $T({\bf y}, \theta)$, and then we compute
this for the observed data set, and then compare its value to that
computed for perfect data sets simulated under the correct model.  In
the case of classical inference, we use a parametric bootstrap where
we simulate data sets conditional on the MLE $\hat{\theta}$. For
Bayesian analysis we use the Bayesian p-value \citep{gelman_etal:2006}
in which data sets are simulated for a posterior sample of $\theta$
which we briefly introduced in sec. \ref{glms.sec.gov}.  Using
classical inference methods, it is sometimes possible to identify a
test statistic of theoretical merit, perhaps with a known asymptotic
distribution.  Examples from the closed capture-recapture setting
includes XXXXXX XXXXXXXX.  When this is not the case, goodness-of-fit
is ususally assessed using bootstrap methods \citep{dixon:2002}. Using
a bootstrap method we identify a fit statistic and we compute the
value for which is more or less the standard approach, in general.  To
date, we are unaware of any goodness-of-fit applications based on
likelihood analysis of SCR models (XXX WE NEED TO DO SOME RESEARCH ON
THIS XXXXX) although the approach is standard in distance sampling XXX
REF HERE ??? XXXX. For example, the package \mbox{\tt unmarked}
\citep{fiske_chandler:2012} contains generic bootstrapping methods for
all of the hierarchical models fitted, including distance sampling
\citep[e.g.,][]{sillett_etal:2012}.


One challenge with SCR models is there has not been a definitive or 
general proposal for a fit statistic for use in computing the Bayesian
p-value although a few specialized implementations of Bayesian p-values
have been provided \citep{royle_etal:2009,royle_etal:2011mee, gopalaswamy_etal:2012ecol,gopalaswamy_etal:2012mee,russell_etal:2012}. 
As a general matter, we recommend use of Bayesian p-values but caution
that there is not general expectation to support how well they should
do, in general. As such, one might consider doing some kind of custom
evaluation or calibration when using such methods,  if the power of
the test (ability to reject
under specific departures from the model) is of paramount interest.
This is not a weakness of a Bayesian approach because the same issue
applies in using bootstrap approaches.




\subsection{The Two Components of Model Fit}

For SCR models,
there are at least two distinct components to model fit for most models,
making an omnibus measure of fit difficult to describe.
First, we consider whether the model explains the observation
process, which we can evaluate  based on the encounter frequencies of
individuals {\it conditional} on the underlying point process ${\bf s}_{1}, \ldots, {\bf s}_{N}$
Second, we can 
evaluate whether the specific assumptions about the point process
appear consistent with the data. 
 For the simple model of independence and uniformity, 
this is the assumption of {\it complete spatial randomness} (CSR)
which we consider in section XXX.YYY below. Actually, 
this is not strictly CSR per se because of the binomial
assumption on $N$ but we refer to it as that because it is practically
equivalent in most cases.
We propose analyzing the two elements of fit individually.



\subsection{Testing Uniformity or Spatial Randomness}

Historically, especially in ecology, there has been a huge amount of
interest in whether a realization of a point process indicates
"complete spatial randomness," i.e., that the points are distributed
uniformly and independently in space.  A good reference for such
things is Cressie (1996; ch. 8) and XYZ.XYZ and I also like Tony
Smith's lecture notes (Univ. of Penn. ESE
502)\footnote{
\url{http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf}}. In
the context of animal capture-recapture studies, the CSR hypothesis is
manifestly false, purely on biological grounds. Typically individuals
will be clustered or more uniform (for territorial species) than
expected under spatial randomness. That said, it may be a reasonable
approximation to truth in some situations and
might be largely irrelevant so far as obtaining estimates of density
is concerned (given that we do observe a portion of the population,
$n$).  Furthermore, in realistic sample sizes we might expect
relatively low power to detect departures from CSR although this is a
research question worthy of attention (see section XX.YY).

Before proceeding with the development of a framework for evaluating
the point process model - we note that if $N$ is fixed, the resulting
point process is not, strictly speaking, one of "complete spatial
randomness". This is because when $N$ is fixed, a slight bit of
correlation is induced in the number of points within any particular
subset of the state-space. That said, this is negligible for most
purposes and, besides, we use a simulation based approach to testing
in which we simulate under the appropriate model.  But the point is
that CSR is not really the conventional term - maybe we call this
"uniformity".  The basic technical framework for evaluating the CSR
hypothesis is that the cell counts i.e., precisely those we computed
to produce a density map, should have a binomial distribution and we
can use a standard chi-square goodness-of-fit test to evaluate that.
It will usually suffice to approximate the binomial cell counts by
Poisson cell counts in which case we can use the classical
"index-of-dispersion" test (e.g., Illian et al. 2008; p. 87):
\[
   I =  (cells -1)*s^2/\bar{n}
\]
which has approximately a chi-square on $(ncells - 1)$ df.  If $s^2/nbar
> 1$ this suggests clustering and if $s^2/nbar <1$ then this suggests the
point process is too regular.  I think this test is sensitive to the
number and size of the grid cells chosen and I don't know much about
that but it should be researched.
{\it The} important technical issue is that we don't observe the point
process and so the standard statistics for evaluating CSR cannot be
computed directly.  However, using Bayesian analysis, we do have a
posterior sample of the underlying point process and so we suggest
computing the posterior distribution of the chi-square statistic and
seeing how it compares to 1. As an alternative to the chi-square
statistic based on CSR there are various "nearest-neighbor"
methods. For example EXAMPLE XXXXXXXXX which is also easy to compute.

To evaluate the uniformity assumption (i.e.,
``complete spatial randomness'') for the activity centers, we use a
standard chi-square goodness-of-fit test statistic, based on gridding
the state-space of the point process into $g=1,2,\ldots,G$ cells, and
we tabulated $n(g)$ the number of activity centers in each grid
cell. A standard goodness-of-fit statistic for CSR is based on the
ratio of the variance of $n(g)$ to the mean (see Table 8.3 in Cressie
1996). In particular, in parametric likelihood theory, $I = (G
-1)*s^2/\bar{n}$ should have a chi-square on $(G - 1)$ df under the
CSR hypothesis.  However, we used this statistic as the basis for our
Bayesian p-value calculation, comparing a posterior sample of $I$
computed using each posterior realization of the point process with a
value of $I$ obtained by simulating ${\bf s}$ under CSR.


An issue is that this is probably sensitive to the size of the
state-space. As the size of the state-space increases then the cell
counts {\it are} independent binomial counts with constant density,
and so we can overwhelm the fit statistic with extraneous ``data''
simulated from the posterior, which is equal to the prior as we move
away from the data, and therefore uninformed by the actual data in the
vincinity of the trap array.
Therefore we recoommend computing these fit statistics in the vicinity
of the trap array only.

\subsubsection{Computing the cell counts}

In chapter 4 we talked about computing summaries of individual
locations, such as for producing a density map. We need these for the
GoF analysis...In the present context, the number of individuals
living in any well-defined polygon is a derived
parameter. Specifically, let $B(x)$ indicate a pixel
 centered at x then
$N(x)=sum_{i} I(s[i] in B(x))$ is the population size of box B(x), and
$D(x) = N(x)/||B(x)||$ is the local density. These are just "derived
parameters" (see Chapt. 2) which are estimated from MCMC output using
the appropriate Monte Carlo average. Note that we are assuming in our
illustration that N is known and so this is easily done by taking all
of the output for MCMC iterations m=1,2,, and doing this:
\[
   N(x,m) = sum_{z[i,m]=1} I(s[i,m] in B(x))
\]
Thus, N(x,1),N(x,2),, is the Markov chain for parameter N(x).

It is worth emphasizing here that density maps will not usually appear
uniform despite that we have assumed that activity centers are
uniformly distributed because the observed encounters of individuals
provide direct information about the location of the i=1,2,.,n
activity centers and thus their "estimated" locations will be affected
by the observations. In a limiting sense, were we to sample space
intensely enough, every individual would be captured a number of times
and we would have considerable information about all N point
locations. Consequently, the uniform prior would have almost no
influence at all.


\subsection{Assessing  Fit of the Encounter Probability Model}

We first consider the case 
 where there are no time-varying covariates. In this case, we can 
summarize the data by individual and trap-specific counts
$y_{ij}$. Conditional on ${\bf s}_{i}$,
the expected value under any encounter model is:
\[
 E[y_{ij}] = p_{ij} K
\]
(or $K_{j}$ if the traps are operational for variable periods). Then
we can define a fit statistic from the Freeman-Tukey residuals
according to:
\[
 T_{1}({\bf y}, \theta) = \sum_{i}\sum_{j} (\sqrt{ y_{ij} } - \sqrt{ E(y_{ij}) })^2
\]
This is conditional on ${\bf s}$ as well as on the data
augmentation variables ${\bf z}$. We 
compute this statistic for {\it each} iteration of the MCMC algorithm
for the observed data set and also for a data set simulated from the
posterior distribution, say ${\bf y}^{new}$.


\begin{comment}
- GoF of Bernoulli response is not really possible, because "the deviance is not informative about fit in the Bernoulli case". You can find something like the latter in the GLM bible (McCulllagh and Nelder).
- Then, I think I did a Bayesian p-value approach in an occupancy model with simulated data and found that all models fit (using Chisquare or Freeman-Tukey, can't remember), which fits in.
- I think I grasped somewhere that to test GOF of a Bernoulli response, it would have to be aggregated. For a dynamic occupancy model with three within-season reps, I tried that out with a distribution model for the Alpine lynx population, by taking as reponse the observed number of detections per year (i.e., the sum of the Bernoulli response over the three seasons per year). That indicated (slight) non-fit, so that seems to work.
- Re. fitting a mis-specified Poisson I don't know. Sounds too much of a dirty trick to me. And if it worked, certainly somebody would have had the idea before. But, why not try it out in a simple simulation ?
 
hi Marc,
 a few months ago you sent around some comments on the basic problem of doing GoF or something with a Bernoulli model -- how it couldn't be done.
Did you have a reference for that, or can you recapitulate the basic problem for me?
 
 I guess the issue is it was a feature of the bernoulli likelihood but I wonder if you could do GoF using a misspecified Poisson likelihood and it should be ok?  As long as E[y] is small, which it usually is, seems like that would be a good approximation to truth.
\end{comment} 


\subsubsection{Statistic 2: Individual frequencies}

SCR model is a model for heterogeneity for which individual encounter
frequencies are the sufficient statistic [not really but close]. so we
propose to use a statistic based on that.

If model has time effects then y[i,j] is binary ... we recommend
computing individual/trap or individual or trap summaries using an
expected value based on $\sum_{k} p_{ijk}$ (Russell et al. 2012).

Probably for choosing among encounter probability models there is no
ability to detect departures from fit for typical sparse data
arrays. This is because individuals are observable in only a few
traps.....

Perhaps behavioral response or other biological models we can have
some power although there needs to be some basic simulation work done
on these problems.


\subsubsection{Statistic 2: Trap frequencies}

Thinking of SCR models  as spatial counts at trap locations it makes
sense that the SCR model is basically just a spatial model of counts
\citep{chandler_royle:2012}. So we should pick models that provide
good predictions of trap totals.
This suggests the following statistic:
\[
 bleen
\]

In this context of a spatial prediction model it
makes sense to think of 
cross-validation. 
Cross-validation is a standard
method of fitting spatial models such as kriging and splines. So we
could as well use cross-validation based on the TRAP-SPECIFIC
frequencies or some other measure.  We should try that here.








\section{Some examples: Wolverine}


Wolverine study:
Model selection --- 

Goodness-of-fit
Is SCR0 adequate for the wolverine data?

We used the posterior output from the wolverine model fitted previous
to compute a relatively coarse version of a density map, using a 10 x
10 grid (Figure XXX.YYY) and using a 30 x 30 grid (Figure XXYYZZ). A
couple of things are noteworthy: First is that as we move away from
"where the data live" - away from the trap array - we see that the
density approaches the mean density (lambda = XX per grid cell 
XXX.YYY how is this computed?). This is a property of the estimator
when the "detection function" decreases sufficiently
rapidly. Relatedly, it is also a property of statistical smoothers
such as splines, kernel smoothers, and regression smoothers -
predictions tend toward the global mean as the influence of data
diminishes. Another way to think of it is that it is a consequence of
the prior - which imposes uniformity, and as you get far away from the
data, the predictions tend to the prior. The other thing to note about
this map is that density is not 0 over water. This might be perplexing
to some who are fairly certain that wolverines do not like
water. However, there is nothing about the model that recognizes water
from non-water and so the model predicts over water {\it as if} it
were habitat similar to that within which the array is nested. But,
all of this is ok as far as estimating density goes and, furthermore,
we can compute valid estimates of N over any well-defined region which
presumably wouldn't include water if we so choose.



\begin{comment}


\section{A simulation study under alternatives}

The point of this section is to evaluate whether we can effectively reject goodness of fit under various alternatives. In particular, we consider two specific modes of lack of fit:

(1)	Can we detect a lack of fit in the detection model?

(2)	Can we detect a lack of fit in the point process model?

4 or 5 detection models and 3 ponit process models.

7 x 7 grid, set to up to yield 50 individuals out of 100. Also try 49 pseudo-random or systematic points.


\subsection{Point Process Alternatives}

1.	Extreme regularity
2.	Somewhat? Regular
3.	Extreme clustering (multiple guys with same home range center)
4.	Somewhat clustered
5.	Complete randomness
6.	Spatial drift in the form of a spatial covariate or trend


We conducted a limited simulation study to evaluate the basic SCR
model under point processes that deviate from complete spatial
randomness. (To be concise we might use the term "uniformity" because
the abbreviation CSR looks too much like SCR.) Specifically, we
consider two deviations from complete spatial randomness: clustered
points, and points that are more systematically distributed than under
complete spatial randomness. We evaluate two questions: (1) how
sensitive is the density estimate to violation of complete spatial
randomness? (2) how much power does the GoF test have?


It is clear that there are many possible influences of both power and
effect of tests for CSR.  For example, N and lambda interact to affect
the expected size of the data set (n individuals) and also sigma
interacts with trap configuration and spacing to affect the number of
unique traps that individuals are captured in. It would be impossible
to catalog an exhaustive set of "what ifs" and so, instead, we focus
on the limited situation where N=100 on a state-space with parameters
set to obtain about 44 individuals on average. We simulated encounters
on a 5x5 grid of traps, unit spacing, with a 2 unit buffer defining
the state-space.

We simulated 3 point process models: Model 1 is CSR. Model 2 is a
Poisson cluster process (PCP) and Model 3 is a point process generated
to be more systematically distributed than CSR. For the Poisson
cluster process, 100 "parents" were distributed randomly over the
state space and the number of offspring for each parent was assumed to
be Poisson with mean 4. Offspring locations were generated according
to a bivariate normal distribution around the parent location, with a
standard deviation of 0.5.  This generated a list of many more than
100 final offspring locations within the state-space, and so we kept
the first 100 points within the state-space and discarded the
remaining. Effectively the last cluster is truncated if the cumulative
sum of offspring within the state-space is not precisely equal to 100.
For the systematic point process we generated 100 uniformly
distributed points and used that as a starting point to obtain the
"space-filling design" (Royle and Nychka XXXX) which is implemented in
the R package {\it fields} (Nychka et al XXXX) using the function {\it
  cover.design}.


Things to vary.

Point process. [random, clustered, regular]   3 levels

Trapping grid: 5 x 5 unit spacing. Also try random traps?   2 levels

State-space: buffered by 2 or 3 units.   2 levels

[N,lambda] fixed at a single value.

[sigma] = .75

GoF grid size = 100 cells, 225 cells, 400 cells with 0, 1 or 2 buffer?

Table:
                      Mode N  Mean N     RejectCSR   meanN(reject) modeN(reject)
CSR
PCP
SYS


4.6.2. GoF under complete spatial randomness.

4.6.3. GoF under the Poisson cluster process.

4.6.4. GoF under the inhibition process.

\end{comment}


\section{ Summary and Outlook  }


We offered some general strategies for model selection and model
checking. 
We recommend model averaging over a small set of distinct models for
estimating density. But we think using a single model or the best
model for asessing the importance of effects is probably ok despite
that the SE's will be biased. 

We think the selection ideas we talked about are probably ok, although
some more work is needed on Bayesian model selection to make it more
useful.  The GoF is pretty experimental. There is not a lot of
guidance in the literature on GoF in SCR models. 
There is essentially nothing known at all about power to detect various
departures.  There is some interesting and useful research to be done
on this. Do we have power to detect departures and does it matter in
terms of estimating $N$? Our specific analyses suggest not, but we
know of no theory or specific empirical study that has addressed this
issue. 

Effect of non-fitting models ..... 
I think inference about Density is going to be insensitive to
departures from CSR but probably somewhat sensitive to bad models for
the observation process.  Why do I think this? Well spatial variation
is not important to inference about the aggregate population size, but
it is relevant to "within population" inference, such as predicting on
small areas.  Conversely, inference about total population size is
known to be highly sensitive to the observation model (Dorazio and
Royle 2003; Link 2003).

What if CSR is rejected?
What can we do? We don't know.. depends on the laternative and how sensitive estimates are?

Power of the test?







