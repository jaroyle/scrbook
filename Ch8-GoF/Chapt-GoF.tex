\chapter{
Model Selection and Assessment
}
\markboth{Model Assessment}{}
\label{chapt.gof}

\vspace{.3in}

Our purpose in life is to analyze models. By ``analyze'' we mean one
or more of the following basic 4 tasks: estimate parameters, make
predictions of unobserved random variables (e.g., of density),
evaluate the relative merits of different models or choosing a best
model (model selection), and we check whether a specific model appears
to provide a reasonable description of our data or not (assessment).
In previous chapters we have addressed the problem of estimation
extensively, and also making predictions of latent variables either
${\bf s}$ or density or population size.  In this chapter, we focus on
the last two of these inference tasks: model selection (which model or
models should we favor), and model assessment (do the data appear to
be consistent with a particular model?).

In the context of SCR models we discuss some specific methods of model
selection and assessment, and focus on a few specific application
contexts such as choosing among certain covariates, or other aspects
of model structure.  We discuss the use of AIC and DIC for this
purpose, and also the ``indicator model selection'' approach of
\citet{kuo_mallick:1998}.  For model checking or assessment we rely
exclusively on Bayesian p-values (REF XYZ) to evaluate
"goodness-of-fit". Part of the challenge thus far is coming up with
good fit statistics, and there does not appear to be any guidance on
this in the literature.  We break the problem up into 2 components
which we attack separately (Royle et al. 2011): Does the observation
model fit?  Does the uniformity assumption appear adequate? The latter
has a huge amount of precedent in the ecological literature as it is
equivalent to the test of ``complete spatial randomness'' which Royle
et al. (2011) applied to SCR models. Really this is a test only of a
form of ``spatial randomness'' because CSR implies the use of a
Poisson point process which, as we discussed, is not universally
adopated in our implementation of SCR models.

A basic problem with these two objectives of model selection and model
assessment is their simultaneous use implies a kind of contradiction
which we call the
{\it model selectors paradox}: Inferences are always achieved using
standard paradigms of parametric inference (Bayesian or frequentist)
which asssert that the model is properly specified. That is, we assume
that the model is 
truth. This is paradoxical because we all know that ``all models are
wrong'' but possibly, ``some are useful'' In fact, the notion that an
``assumption'' could even be correct is itself something of an oxymoron.
Therefore we don't expect
or hope to make assumptions that are ``correct'' in any way. Gelman
and Shalizi (2010) say it this way: ``there is general agreement that,
in this domain, all models in use are wrong -- not just merely
falsifiable, but actually false.''
We should therefore refrain from over-stating the relevance of any model. 

In this chapter we develop basic strategies of model selection and
model assessment or checking using both likelihood methods (as
implemented in the \mbox{\tt secr} package) and also Bayesian
analysis.  We apply these methods to the wolverine camera trapping
data to investigate sex specificity of model parameters and whether
there is a behavioral response to encounter. We note that individuals
are drawn to the camera trap devices by food bait and therefore it
stands to reason that once an individual discovers a trap, it might
return subsequently for that benefit, a response commonly referred to
as ``trap happiness.''







\begin{comment}
A few specific questions to resolve:
\begin{itemize}
\item[(1)]  Can AIC or DIC choose among detection models and does it matter?
\item[(2)]  Can AIC or DIC choose among more substantial models?  We should
 assume it does. Can we rig up a test?  Suggest: (1) trap level
 covariate and trying DIC; (2) Model Mb vs. not
\item[(3)] Simultaneous to above, see if we can do that.......
Posterior model probabilities: Can we demo this?   With detection models?
 Don't do much analysis of the situation, just demo it maybe?
\item[(4)]  Basic framework for Bayesian GoF. Power under alternatives.
(clustering, regularity, uniformity) and misspecified encounter models
-- do they show a lack of fit?
\end{itemize}
\end{comment}



\begin{comment}
\subsection{The Role of model assumptions}

Referees are quick to point out that bivariate normal distributions as
models for home range are simply wrong. As if using wrong models
somehow invalidates something. (is that a statistical paradigm that I
missed?). The issues are whether its an adequate model, whether we can
refute it, whether resulting inferences are sensitive to it, and
whether the model is useful for its intended purpose regardless.
If  we really wanted a great description of a home range then we would do
something different besides conduct an SCR study. The purpose of most
SCR models is not to study home range geometry and morphology but, rather,
to estimate density and possibly other vital rates such as survival and
recruitment. In addition, it is my experience that the same people who
criticize models as implying overly simple models actually cannot describe
alternative models except using jargon and procedures like "kernel
smoothing", "utilization distributions" and "neural networks". So, on
the one hand, the bivariate normal distribution is overly simple, and
therefore we should use "neural networks"?  What irritates me even more
is referees who emphatically assert that "real home ranges aren't bivariate
normal" and then they go on to cite references who somehow "proved" they
are not. These people have no clue what statistics is good for and what
the point of SCR models is, nor can they grasp the relevance of the home
range model - namely, as a model for explaining heterogeneity in
capture-probability. To be sure, the bivariate normal model is an
over-simplification but so far it has not been shown to be ineffective
in SCR problems, and besides  substantially more flexible models have been
developed \citep{royle_etal:2012ecol}.

What we care about in models of capture-recapture data is whether the
bivariate normal (or any model) provides an adequate description of the
encounter process. That is the question we will evaluate here.
\end{comment}


\section{General Strategies for Model Selection}

We review a number of standard methods of model selection that apply
to ``variable selection'' problems. That is, when our set of models
consists of distinct covariate effects and they represent constraints
of some larger model achieved by setting some of the parameter values
to 0. 
For classical analysis based on likelihood,  model selection by 
AIC is the standard approach \citep{burnham_anderson:2002}.
For Bayesian analysis we rely on a number of different methods.
We demonstrate the use of DIC here for variable selection problems although we recommend against its
general use (see below).
We use the Kuo and
Mallick indicator variable selection approach
 \citep{kuo_mallick:1998} which, although its implementation in the
 {\bf BUGS} packages can be tempramental, it produces direct statemtns of
 posterior model probabilities which we think are the most useful, and
 leads directly to model-averaged estimates of density.
There is a good review paper recently by \citet{ohara_sillanpaa:2009}
that hits on these and many more related ideas for variable selection.
We have not done a comprehensive evaluation of these different methods
for effect and efficiency.
In addition to \citet{ohara_sillanpaa:2009} we also recommend
\citet[][Chapt. 7]{link_barker:2010}
for general information on model selection and assessment.


\subsection{Scope of the model selection problem}

There are two distinct classes of problems that we encounter in SCR
models which might require some type of model selection or ranking
effort: (1) Choosing among models that represent distinct, meaningful
biological hypotheses; and, (2) choosing among different parametric
encounter probability models. We believe that the importance of model
selection depends on which type of problem we have.

{\flushleft {\bf Choosing among biological models:}}
SCR models that represent extensions of the basic null model by
including specific covariates or other effects often represent
explicit biological hypotheses. Examples include models with a
behavioral response, or seasonal variation in encounter probability,
or sex-specificity of model parameters.
We anticipate that such basic biological factors 
could be important, and therefore it can be useful to choose among (or
rank) a set of models that represent these hypotheses. Indeed, they
might sometimes be of direct interest although usually only
secondary to estimating or modeling density.

{\flushleft {\bf Choosing among models for encounter probability:}}
In sec. \ref{chapt.scr0.implied} we introduced the notion that
encounter probability models imply specific models of space usage, an
idea we expand on and generalize in Chapt. \ref{chapt.rsf}. Because of
this linkage it is tempting to want to choose among them so that we
can proclaim something reasonable about space usage. Our feeling about
choosing among encounter probability models is determined by the
following two considerations:  (1) 
usually trap density is low and so we imagine one would have little or
no power to effectively choose among models and (2) the model itself
is not a biological construct, just the implied home range area is. As
stationary and isotropic models, they are all equally dumb as models
for space usage and so it seems to us that choosing among a dozen or
more arbitrary parametric forms that have no biological motivation
should tend to lead to an over-fitting.
So we will apply ideas of model selection to  some problems below (and
elsewhere in this book) but we avoid the problem of choosing among
detections functions and we discourage people from doing that. 



\subsection{Model selection by AIC}
\label{gof.sec.aic}

If you're doing  classical analysis based on likelihood then model
selection is easily accomplished using AIC \citep{burnham_anderson:2002}
which we demonstrate below. The AIC of a model is simply twice the
negative log-likelihood evaluated at the MLE,  penalized by the number of parameters
($k$) in the model:
\[
 AIC = -2 \mbox{\tt logL}({\bf y}|\hat{\bm \theta})  + 2 k
\]
Models with small values of AIC are preferred. 
It is common to use a modified AIC referred to as $AIC_{c}$ for small
sample sizes which is
\[
 AIC_{c}  = 
-2 \mbox{\tt logL}({\bf y}|\hat{\bm \theta})  + \frac{2 k
  (k+1)}{n-k-1}
\]
where $n$ is the sample size. 
Two important problems with the use of $AIC$ and $AIC_{c}$ are that
they don't apply directly to hierarchical models that contain random
effects, unless they are computed directly from the marginal
likelihood (for SCR models we can do this, see
Chapt. \ref{chapt.mle}). Moreover, it is not clear what should be the
effective sample size $n$ in calculation of $AIC_{c}$, as there can be
covariates that affect individuals, that vary over time, or in space.
We do not offer 
strict guidelines
as to when to use a small sample size adjustment or not.

The {\bf R} package \mbox{\tt secr} computes and outputs AIC
automatically for each model fitted and it provides some capabilities
for producing a model selection table (function \mbox{\tt AIC})  and also doing model-averaging
(function \mbox{\tt model.average}),  which we recommend for obtaining estimates of density from multiple models.
We provide  an example of using \mbox{\tt secr}, to analyze the
wolverine camera trapping data using 
 4 distinct models:
\begin{itemize}
\item[ Model 1:] model SCR0 with constant parameter
values for both male and female wolverines but with a parameter
$\psi_{sex}$ the population proportion of males;
\item[ Model 2:] sex-specific intercept
$p_{0}$ but constant $\sigma$; 
\item[ Model 3:] sex-specific $\sigma$ but constant
$p_{0}$ 
\item[ Model 4:] sex-specific $p_{0}$ {\it and} $\sigma$. 
\end{itemize}
To fit these models we use the multi-session formulation provided by
\mbox{\tt secr} (introduced in sec. \ref{mle.sec.multisession}), which
allows one to model sesssion-specific effects on density, baseline
encounter probability, $g_{0}$ in \mbox{\tt secr}, and also the scale
parameter $\sigma$ of the encounter probability model. Using this
formulation, we define the ``session'' variable to be a sex code and
thus session-specific parameters represent sex-specific parameters.
For example, if we model session-specific density, $D$, then this
corresponds to Model 1 in our list above.  We note that this suggests
one additional model which we haven't itemized above, that being the
model with {\it no} sex effect on any parameter, which is equivalent
to fixing $\psi_{sex} = 0.5$ instead of estimating it. We will label
this last model ``Model 0'' and include it in our analysis below, even
though we may not generally think of this as a natural candidate
model. The specific formulation of sex effect models is discussed in
sec. XXXXX.

Here are the {\bf R} commands for loading the wolverine data and doing
a slight bit of formatting to prepare the data objects for analysis by
\mbox{\secr}. The key difference from our analysis in
Chapt. \ref{chapt.mle} is, here, we grab the wolverine sex information
(\mbox{\tt wolverine\$wsex}) which is a 0/1 indicator (1=male). We add
1 to that and then use it to define the ``Session'' variable based on sex.
The {\bf R} commands are as follows:
{\small
\begin{verbatim}
library("secr")
library("scrbook")
data("wolverine")
traps<-as.matrix(wolverine$wtraps)   
dimnames(traps)<-list(NULL,c("trapID","x","y",paste("day",1:165,sep="")))

trapfile2<-scr2secr(scrtraps=traps,type="proximity")

gr<-as.matrix(wolverine$grid2)
dimnames(gr)<-list(NULL,c("x","y"))
gr2<-read.mask(data=gr)

wolv.dat<-wolverine$wcaps
dimnames(wolv.dat)<-list(NULL,c("Session","ID","Occasion","trapID"))
wolv.dat[,1]<-wolverine$wsex[wolv.dat[,2]]+1
wolv.dat<-as.data.frame(wolv.dat)
wolvcapt3<-make.capthist(wolv.dat,trapfile2,fmt="trapID",noccasions=165)
\end{verbatim}
}

{\bf XXXX use model.average2 XXXXXX }

Once the data are have been prepared in this way we use the 
\mbox{\secr} model fitting function \mbox{\tt secr.fit} to fit a
number of different models and then the function \mbox{\tt AIC} to
package the models together and summarize them in the form of an AIC
table, with rows of the table ordered from best to worst. The function
\mbox{\tt model.average} performans AIC-based model-averaging of the 
parameters specified by the \mbox{\tt realnames} variable (below this
is demonstrated for density, $D$).  Together
these commands and resulting output (abbreviated to fit on page) look like this:
{\small 
\begin{verbatim}

XXX note to reader: I will delete some horizontal output XXXXX

model0<-secr.fit(wolvcapt3,model=list(D~1, g0~1, sigma~1), buffer=20000)
model1<-secr.fit(wolvcapt3,model=list(D~session, g0~1, sigma~1), buffer=20000)
model2<-secr.fit(wolvcapt3,model=list(D~session, g0~session, sigma~1), buffer=20000)
model3<-secr.fit(wolvcapt3,model=list(D~session, g0~1, sigma~session), buffer=20000)
model4<-secr.fit(wolvcapt3,model=list(D~session, g0~session, sigma~session), buffer=20000)

> AIC (model0,model1,model2,model3,model4)
                                    model   detectfn npar    logLik      AIC     AICc dAICc  AICwt
model0                   D~1 g0~1 sigma~1 halfnormal    3 -627.2603 1260.521 1261.932 0.000 0.5831
model2       D~session g0~session sigma~1 halfnormal    5 -624.9051 1259.810 1263.810 1.878 0.2280
model1             D~session g0~1 sigma~1 halfnormal    4 -627.2365 1262.473 1264.973 3.041 0.1275
model4 D~session g0~session sigma~session halfnormal    6 -624.6632 1261.326 1267.326 5.394 0.0393
model3       D~session g0~1 sigma~session halfnormal    5 -627.2358 1264.472 1268.472 6.540 0.0222

> model.average (model0,model1,model2,model3,model4,realnames="D")
              estimate  SE.estimate          lcl          ucl
session=1 2.707190e-05 7.913577e-06 1.544474e-05 4.745224e-05
session=2 2.927423e-05 8.270402e-06 1.700631e-05 5.039193e-05
\end{verbatim}
}

{\bf XXX we should provide parameter estimates and SE's in a table XXXXX}

The default output of estimated density is in individuals per ha, so
we have to scale this up to something more reasonable. To get into
units of per 1000 $km^2$ we need to mulitply by 100 to get to units of
$km^2$ and then 1000. This produces an estimated density of 
about 2.71 for \mbox{\tt session=1} (females) and 2.93 for
\mbox{\tt session=2} (males).

We don't agree with the use of AICc here and think its better to use
AIC, in general. This is because its not clear what the effective
sample size is. While we have 21 individuals in the data set, most of
the model structure has to do with encounter probability samples and
for that there are 100s of observations. We note that the AIC and AICc
results are not consistent.
By looking at the best model by AIC, we find that the model
with estimated sex ratio and sex specific $g_{0}$ is preferred (Model 2). This
is just slightly better than the model with a fixed sex ratio of
$\psi_{sex} = 0.50$.


We do the same things with the mask which excludes ocean. 
Results are given in Tab. XXX along with the previous models without a mask.
We see AIC is much better for the model without the mask -- it is ok
to have this because we recognize the mask as changing the random
effects distribution and the results should be sensitive to that. That
said, we don't like the non-mask model because it makes sense to
exclude the water area from the state-space  of ${\bf s}$.
For females the model-averaged density is 3.88 individuals per 1000 $km^2$ and for
females the model-averaged density estimate is 4.46 individuals per
1000 $km^2$:
\begin{verbatim}
> model.average (model0b,model1b,model2b,model3b,model4b,realnames="D")

              estimate  SE.estimate          lcl          ucl
session=1 3.876615e-05 1.189102e-05 2.153795e-05 6.977518e-05
session=2 4.459658e-05 1.323696e-05 2.523280e-05 7.882022e-05
\end{verbatim}


\begin{verbatim}
> AIC (model0b,model1b,model2b,model3b,model4b)
                                     model   detectfn npar    logLik      AIC     AICc dAICc  AICwt
model2b       D~session g0~session sigma~1 halfnormal    5 -629.0482 1268.096 1272.096 0.000 0.4421
model4b D~session g0~session sigma~session halfnormal    6 -628.3490 1268.698 1274.698 2.602 0.1204
model0b                   D~1 g0~1 sigma~1 halfnormal    3 -632.5813 1271.163 1272.574 0.478 0.3481
model1b             D~session g0~1 sigma~1 halfnormal    4 -632.5574 1273.115 1275.615 3.519 0.0761
model3b       D~session g0~1 sigma~session halfnormal    5 -632.5445 1275.089 1279.089 6.993 0.0134
\end{verbatim}



\subsection{Bayesian model selection}

Model selection is somewhat less straightforward
as a Bayesian and there is no such canned all-purpose method like AIC
As such we recommend more of a pragmatic approach, in general, for all
problems, based on a number of basic considerations:
\begin{itemize}
\item[(1)] For a small number of fixed effects we think it is
  reasonable to adopt a conventional
``hypothesis testing'' approach -- i.e., if the posterior for a parameter overlaps
zero substantially, then it is probably reasonable to discard that effect.
\item[(2)] Calculation of posterior model probabilities: In some cases we can implement
methods which allow calculation of posterior model probabilities. One
such idea is the indicator variable selection idea from
\citet{kuo_mallick:1998}.
The idea is introduce a latent variable $I \sim \mbox{Bern}(.5)$ and
expand the model to include the variable $I$ as follows:
\[
 \mbox{logit}(p_{ijk}) = \alpha_{0} + I*\alpha_{1}*x_{ijk}.
\]
The importance of the covariate $x$ is then measured by the posterior
probability that $I=1$.  
\item[(3)] DIC -- the Deviance Information Criterion:
Bayesian model selection is now routinely carried-out using the
Deviance Information
Criterion (DIC; Spiegelhalter et al. 2002) although its effectiveness
in hierarchical models depends very much on the manner in which it is
constructed \citep{millar:2009}.
We recommend using it if it leads to
sensible results but also we think it should be calibrated to the
extent possible for specific classes of models. 
\item[(4)] Logical argument: For something like M/F differences it seems
to make sense to leave the extra parameters in the model no matter what,
because of course you expect , biologically, there to be a difference in
these things. Thus the hypothesis test itself is meaningless (Doug Johnson
wrote a paper on this kind of stuff once) and you would only reject the
null because of low power anyhow.
\end{itemize}

In all modeling activities, as in life itself, the use of logical argument should not be under-utilized.

\subsection{Deviance Information Criterion}

AIC makes the use of likelihood methods  convenient for problems where
likelihood estimation is achievable. 
For Bayesian analysis, the 
development of  Deviance Information Criterion (DIC)
\citep{spiegelhalter_etal:2002} seemed like a general-purpose
equivalent for a brief 
period of time after its invention.  However, 
there seems to be too many types of DIC, and no consistent version
is reported across computing platforms, and practical calibration
has indicated highly variable effectiveness of 
DIC. Even the high-end statisticians don't have a unified front on 
practical issues related to the use of DIC.
That said, it is still widely reported and, we think, probably
reasonable for certain classes of fixed effects models. 

XXXX DEFINE DIC HERE XXXXXXXXXX


An issue is applying DIC to hierarchical models in which case the
level of the hierarchcy matters. Do you condition on the random
effects, or do you use the deviance based on the marginal likelihood?
The current Conventional Wisdom is that DIC doesn't really work so
well in hierarchical models (    ). But there doesn't really seem to be
general theory to support this. It seems maybe like its ok sometimes. So
we take a look at its use here to pick among fixed effects models.

We did this for our wolverine data. We used the same 4 models as above
and a 5th model which was the super-model containing all of them
together. We're not sure what DIC means in that case but, whatever. 
 
\begin{verbatim}
model0 = no covariates , sex effect only (variable density of males/females)
model1 = same + g0(sex)
model2 = sigma(sex)
model3 = g0(sex)+sigma(sex)
## SHOULD RENAME THIS FUNCTION
> toad0<-wolvSCR0.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=0)
> toad1<-wolvSCR0.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=1)
> toad2<-wolvSCR0.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=2)
> toad3<-wolvSCR0.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=3)
> toad5<-wolvSCR0.fn(y3d,traps,wsex=wsex,nb=1000,ni=21000,delta=2,M=200,model=5)
> 

 is an estimate of expected predictive error (lower deviance is better).
> toad0$DIC
[1] 515.5896
> toad1$DIC
[1] 512.7036
> toad2$DIC
[1] 514.2595
> toad3$DIC
[1] 511.5824   <- BEST MODEL HAS BOTH
> toad4$DIC
Error: object 'toad4' not found
> toad5$DIC
[1] 514.695
> 
\end{verbatim}

The output from these models is summarized in Table XXXXX.
These are based on 21000 iterations, 3 chains, 1000 discarded, 60k
posterior samples.......
what we see is: XYZ.....

\begin{verbatim}

Model 0:
> print(toad0,digits=2)
Inference for Bugs model at "modelfile0.txt", fit using WinBUGS,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
            mean    sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
beta.sex    0.00  1.74  -2.85  -1.50   0.01   1.51   2.85    1 56000
alpha.sex   0.00  1.73  -2.85  -1.50   0.00   1.51   2.86    1 60000
psi         0.27  0.06   0.17   0.23   0.27   0.31   0.39    1 60000
psi.sex     0.52  0.10   0.32   0.45   0.52   0.59   0.72    1  9000
sigma      24.34 14.59   1.72  11.34  24.07  36.94  48.71    1 12000
logitp0    -2.87  0.17  -3.21  -2.99  -2.88  -2.76  -2.53    1  2800
N          54.11  9.53  38.00  47.00  53.00  60.00  75.00    1 53000
D           5.22  0.92   3.66   4.53   5.11   5.78   7.23    1 52000
deviance  446.58 11.75 425.90 438.30 445.80 454.10 471.70    1  6100
DIC info (using the rule, pD = var(deviance)/2)
pD = 69.0 and DIC = 515.6

> print(toad1,digits=2)
Inference for Bugs model at "modelfile0.txt", fit using WinBUGS,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
            mean    sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
beta.sex   -0.01  1.73  -2.85  -1.51  -0.02   1.49   2.85    1 60000
alpha.sex  -0.79  0.35  -1.48  -1.02  -0.79  -0.56  -0.11    1  8100
psi         0.28  0.06   0.18   0.24   0.27   0.31   0.40    1  3600
psi.sex     0.56  0.10   0.36   0.49   0.56   0.63   0.75    1  1700
sigma      24.67 14.42   1.87  11.93  24.44  37.19  48.71    1 60000
logitp0    -2.46  0.26  -2.95  -2.63  -2.46  -2.29  -1.94    1 13000
N          54.87  9.72  38.00  48.00  54.00  61.00  76.00    1  2900
D           5.29  0.94   3.66   4.63   5.21   5.88   7.33    1  2900
deviance  443.21 11.79 422.20 434.90 442.50 450.70 468.20    1 10000
DIC info (using the rule, pD = var(deviance)/2)
pD = 69.5 and DIC = 512.7

> print(toad2,digits=2)
Inference for Bugs model at "modelfile0.txt", fit using WinBUGS,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
            mean    sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
beta.sex    0.01  1.71  -2.83  -1.46   0.01   1.49   2.84    1 46000
alpha.sex  -0.01  1.73  -2.85  -1.51  -0.01   1.49   2.84    1 60000
psi         0.28  0.06   0.17   0.24   0.27   0.31   0.39    1  3000
psi.sex     0.53  0.10   0.32   0.45   0.53   0.60   0.72    1 12000
sigma      25.64 14.00   2.19  13.80  25.67  37.68  48.75    1 15000
logitp0    -2.87  0.17  -3.21  -2.99  -2.87  -2.75  -2.52    1  1500
N          54.54  9.52  38.00  48.00  54.00  60.00  75.00    1  3200
D           5.26  0.92   3.66   4.63   5.21   5.78   7.23    1  3200
deviance  446.36 11.66 425.80 438.10 445.60 453.80 471.20    1  4200
DIC info (using the rule, pD = var(deviance)/2)
pD = 67.9 and DIC = 514.3
DIC is an estimate of expected predictive error (lower deviance is better).

> print(toad3,digits=2)
Inference for Bugs model at "modelfile0.txt", fit using WinBUGS,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
            mean    sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
beta.sex    0.16  1.67  -2.76  -1.22   0.21   1.60   2.86    1 60000
alpha.sex  -0.80  0.34  -1.48  -1.03  -0.80  -0.57  -0.13    1  3700
psi         0.28  0.06   0.18   0.24   0.27   0.31   0.40    1  3000
psi.sex     0.56  0.10   0.36   0.49   0.57   0.64   0.76    1  5200
sigma      26.33 13.93   2.20  14.71  26.62  38.33  48.88    1  7300
logitp0    -2.45  0.25  -2.94  -2.62  -2.45  -2.28  -1.94    1  1300
N          55.04  9.67  38.00  48.00  54.00  61.00  76.00    1  2400
D           5.31  0.93   3.66   4.63   5.21   5.88   7.33    1  2400
deviance  443.04 11.71 422.30 434.80 442.30 450.50 468.10    1  9000
DIC info (using the rule, pD = var(deviance)/2)
pD = 68.5 and DIC = 511.6
DIC is an estimate of expected predictive error (lower deviance is better).




> print(toad5,digits=2)
Inference for Bugs model at "modelfile5.txt", fit using WinBUGS,
 3 chains, each with 21000 iterations (first 1000 discarded)
 n.sims = 60000 iterations saved
            mean    sd   2.5%    25%    50%    75%  97.5% Rhat n.eff
mod[1]      0.67  0.47   0.00   0.00   1.00   1.00   1.00    1  1300
mod[2]      0.48  0.50   0.00   0.00   0.00   1.00   1.00    1 42000
beta.sex    0.05  1.71  -2.83  -1.42   0.06   1.53   2.85    1 12000
alpha.sex  -0.53  1.10  -2.53  -1.06  -0.74  -0.35   2.53    1  3000
psi         0.28  0.06   0.17   0.24   0.27   0.31   0.40    1 11000
psi.sex     0.55  0.11   0.34   0.48   0.55   0.62   0.74    1  3300
sigma      25.21 14.32   1.81  12.82  25.33  37.51  48.77    1  7200
logitp0    -2.59  0.30  -3.12  -2.83  -2.61  -2.38  -1.98    1   860
N          54.86  9.86  38.00  48.00  54.00  61.00  77.00    1 16000
D           5.29  0.95   3.66   4.63   5.21   5.88   7.42    1 16000
Xobs       91.39  5.55  81.39  87.50  91.08  94.92 103.20    1 60000
Xnew       64.69  6.10  53.39  60.50  64.49  68.71  77.23    1 60000
X1obs      12.48  3.34   6.96  10.09  12.12  14.51  19.94    1 52000
X1new       9.76  2.75   5.21   7.78   9.48  11.42  15.91    1 60000
X3obs      21.59  2.06  18.17  20.12  21.39  22.84  26.21    1  8000
X3new      13.20  3.28   7.67  10.86  12.90  15.23  20.41    1 60000
deviance  444.43 11.86 423.40 436.10 443.60 452.00 469.90    1 14000

For each parameter, n.eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor (at convergence, Rhat=1).

DIC info (using the rule, pD = var(deviance)/2)
pD = 70.3 and DIC = 514.7
DIC is an estimate of expected predictive error (lower deviance is better).
> 
\end{verbatim}


For DIC, small is better and so this suggests Model 2 is preferred. 
What is this model?


\begin{comment}
As is pointed out in one of those posts, it's interesting to me that
both AIC and DIC are attempts at mimicking leave-one-out
cross-validation. Perhaps we could make the point that if DIC cannot
be trusted then one could always implement their own x-val
procedure... assuming it doesn't take months to complete.

Richard

On Thu, May 24, 2012 at 9:57 AM, Jeffrey Royle <jaroyle@gmail.com> wrote:
> hi all -- DIC is a hopeless black hole with, I think, no real possibility of
> making it seem reasonable to people.
> There is some good discussion on Gelman's blog, along with a number of
> papers (all cited there)
> http://andrewgelman.com/2006/07/number_of_param/
> http://andrewgelman.com/2011/06/plummer_on_dic/
> http://andrewgelman.com/2011/06/deviance_dic_ai/
>
> I gather that there are 3 version of DIC. The original one, and then
> Plummer's version which seems to be in JAGS and then the dumb version based
> on .5*var(deviance) which, I gather, you should never use -- I guess it
> sucks.
>
> I still can't compute Plummer's version using JAGS -- I think maybe I need
> to update my JAGS version or something.
\end{comment} 

\subsection{Model selection indicator variables}

A convenient way to deal with model selection problems in Bayesian
analysis by MCMC is to use the method of model indicator variables
\citep{kuo_mallick:1998} which allow us 
to expand the model to include the set of prescribed models as
specific reductions of a larger 
model.
This has been demonstrated in some specific capture-recapture models
in 
Royle 2009, sec. 3.xxx in \citep{royle_dorazio:2008},
and in the context of SCR by 
 Tobler et al. (in review XXXXX need to find this paper XXXXXXX).

To implement this approach, 
we expand the model to include the latent
indicator variables say $I_{m}$, for  variable $M$ in the model, 
such that 
\begin{eqnarray*}
I_{m} = \left\{ 
\begin{array}{cc} 1 &  \mbox{linear predictor includes  covariate $m$} \\
                  0 & \mbox{linear predictor does not
                                include covariate $m$}
 \end{array}
\right.
\end{eqnarray*}
We assume that the indicator variables $I_{m}$
are mutually independent with:
\[
I_m \sim \mbox{Bernoulli}(0.5).
\]
The
 expanded model has the linear predictor:
\[
logit(p_{ijk}) = \alpha_{0} + \alpha_{1}I_{1} C_{1,i} + \alpha_{2}I_{2} C_{2,ijk} 
\]
where, lets suppose, $C_{1,i}$ is an individual level covariate such
as sex, and $C_{2,ijk}$ is a behavioral response covariate which is individual- trap- and occasion-specific.
We can assume a parallel model specification on the parameter $\sigma$
which is liable to vary by individual level covariates such as sex:
\[
 log(\sigma_{i}) = \beta_{0} + \beta_{1} I_{3} C_{1,i}
\]

In this formulation of the model selection problem we can characterize
unique models by the sequence of $I$ variables. In this case, each
unique sequence $(I_{1},I_{2},I_{3})$ represents  a model, and we can
tabulate the posterior frequencies of each model by post-processing
the MCMC histories of $(I_{1},I_{2},I_{3})$ (we demonstrate this shortly).


Conceptually, analysis of this expanded model within the data augmentation
framework does not pose anyadditional difficulty. One
broader, technical consideration is that posterior model probabilities
are well known to be sensitive to priors on parameters
(e.g., Aitkin, 1991; Link and Barker, 2006; Link and Barker 2010,
sec. 7.2.5) and vague priors
are not usually innocuous or uninformative when evaluating
posterior model probabilities. The use of Akaike's information
criterion seems to avoid this problem largely by imposing a
specific and perhaps undesirable prior that is a function of
the sample size (Kadane and Lazar, 2004). One solution to
this problem is to compute posterior model probabilities under
a model in which the prior for parameters is fixed at the
posterior distribution under the full model (Aitkin, 1991).





The other way is to introduce a categorical model identity variable
which, so that 
 ``model identity'' is itself a parameter of the model.
  For example, for
 different basic encounter probability models, we cannot specify the
linear predictor of some general model in a manner that reduces to the
various alternative models simply by switching binary variables on and
off. For this case we do something like this:
\begin{verbatim}
 mod \sim \mbox{dcat}(probs[])
\end{verbatim}
where \mbox{\tt probs[l] = 1/(\# models)}. Then we can fill in a bigger
array of $p$ instead of $p[i,j]$ we build $p[i,j,l]$ for each of
$l=1,2,\ldots,L$ models and we define our data component like so:
\begin{verbatim}
pi[i,j]<- bigpi[i,j,mod]
y[i,j] ~ dbern(pi[i,j])
\end{verbatim}
(or something like that).
As before the posterior probabilities can be highly sensitive to
priors on the different model parameters and sometimes mixing is
really poor.  
But we give a JAGS script in the {\bf R} package
\mbox{\tt scrbook} which has an example that works.
The model specificication for the wolverine data for 3 different
encounter probability models is shown in panel XXXX: Gaussian
detection probability; Gaussian hazard model; logistic model.



model {
for(i in 1:3){
  alpha0[i] ~ dnorm(0,.1)
  beta[i] ~ dunif(0,2)
  mod.probs[i]<- 1/3
}
psi ~ dunif(0,1)
psi.sex  ~ dunif(0,1)
catmod~dcat(mod.probs[])

mod[1] ~ dbern(.5)
mod[2] ~ dbern(.5)

for(i in 1:M){
 w[i]~dbern(psi)
 s[i,1]~dunif(Xl,Xu)
 s[i,2]~dunif(Yl,Yu) 
 logit(beta0.vec[i,1])<- alpha0[1]
 log(beta0.vec[i,2])<- alpha0[2]
 beta0.vec[i,3]<- alpha0[3]

log(beta.vec[i,1])<- log( beta[1] ) 
log(beta.vec[i,2])<- log( beta[2] ) 
log(beta.vec[i,3])<- log( beta[3] ) 

for(j in 1:ntraps){
  mu[i,j]<-w[i]*p[i,j,catmod]
  ncaps[i,j]~ dbin(mu[i,j],K[j]) 
  dd[i,j]<- pow(s[i,1] - traplocs[j,1],2)  + pow(s[i,2] - traplocs[j,2],2) 
 
  p[i,j,1]  <-  beta0.vec[i,1]*exp( - beta.vec[i,1]*dd[i,j] )
  p[i,j,2] <-  1-exp(-beta0.vec[i,2]*exp( - beta.vec[i,2]*dd[i,j] ) )
  logit(p[i,j,3])<- beta0.vec[i,3] - beta.vec[i,3]*dd[i,j]
 }
}
N<-sum(w[1:M])
D<-N/area
}





\subsection{Wolverine stuff}

We did a bunch of analysis previously with models that involved
sex-specific parameters. Here we expand the model set to include a
behavioral response. This is a little more difficult doing Bayesian
analysis because we have to do the 3-d version of the model which can
be a time-consuming task in WinBUGS. But lets do it anyway.
There are in this case 8 models (right?)


4 models with sex: DIC, model weights, AIC.

expanded model with behavioral response...... DIC , model weights, AIC......

\begin{verbatim}
Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.894	1.009	0.0171	4.18	5.822	8.062	1001	6600
	N	39.48	6.755	0.1145	28.0	39.0	54.0	1001	6600
	X1new	8.996	2.711	0.03361	4.551	8.695	15.15	1001	6600
	X1obs	11.87	3.354	0.05475	6.382	11.49	19.53	1001	6600
	X3new	13.15	3.232	0.03914	7.725	12.92	20.4	1001	6600
	X3obs	21.43	2.115	0.03256	17.8	21.24	26.07	1001	6600
	Xnew	61.78	6.517	0.1004	49.48	61.7	75.22	1001	6600
	Xobs	88.97	5.972	0.08912	78.08	88.63	101.5	1001	6600
	alpha.sex	-0.4506	1.158	0.02228	-2.571	-0.6846	2.594	1001	6600
	beta	2.423	4.111	0.3034	0.06847	1.165	17.51	1001	6600
	beta.sex	0.07415	1.761	0.1045	-2.85	0.0155	2.868	1001	6600
	deviance	439.3	12.16	0.2192	418.0	438.4	465.5	1001	6600
	logitp0	-2.577	0.2909	0.0114	-3.079	-2.597	-1.979	1001	6600
	mod[1]	0.6236	0.4845	0.01885	0.0	1.0	1.0	1001	6600
	mod[2]	0.5035	0.5	0.03697	0.0	1.0	1.0	1001	6600
	psi	0.2657	0.05634	8.482E-4	0.1653	0.2617	0.3855	1001	6600
	psi.sex	0.5449	0.1044	0.00231	0.336	0.5472	0.7417	1001	6600
	sigma	0.8442	0.6291	0.0507	0.1698	0.6551	2.704	1001	6600




run 2


Node statistics
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	D	5.939	1.009	0.01712	4.18	5.822	8.211	1001	27000
	N	39.78	6.756	0.1147	28.0	39.0	55.0	1001	27000
	X1new	9.006	2.701	0.02151	4.603	8.699	15.18	1001	27000
	X1obs	11.98	3.396	0.04019	6.437	11.61	19.69	1001	27000
	X3new	13.13	3.218	0.01885	7.748	12.84	20.26	1001	27000
	X3obs	21.4	2.116	0.02102	17.83	21.2	26.11	1001	27000
	Xnew	61.54	6.483	0.0668	49.39	61.26	74.8	1001	27000
	Xobs	88.74	6.003	0.06467	77.85	88.46	101.4	1001	27000
	alpha.sex	-0.4355	1.192	0.01962	-2.624	-0.6607	2.605	1001	27000
	beta	1.257	2.088	0.1191	0.06376	1.036	6.652	1001	27000
	beta.sex	0.5605	1.675	0.06955	-2.723	0.7576	2.92	1001	27000
	deviance	438.9	12.25	0.1858	417.0	438.0	464.9	1001	27000
	logitp0	-2.598	0.287	0.01129	-3.088	-2.624	-1.998	1001	27000
	mod[1]	0.5939	0.4911	0.01784	0.0	1.0	1.0	1001	27000
	mod[2]	0.5251	0.4994	0.02614	0.0	1.0	1.0	1001	27000
	psi	0.3312	0.06925	0.001093	0.2101	0.3264	0.4776	1001	27000
	psi.sex	0.5425	0.1038	0.002119	0.3366	0.5443	0.7389	1001	27000
	sigma	1.045	0.6963	0.04014	0.2743	0.6947	2.801	1001	27000



\end{verbatim}








\section{Evaluating Goodness-of-Fit}

We estimate parameters, maybe fit a bunch of models and pick a
desireable one or perhaps average estimates of density from a few
models. An important questions is:
Are the data we have consistent with realizations from the model which
we just fitted?

Conceptually, we can think of model selection as follows: if we simulate
under that model, do the simulated realizations look like our data?
In a sense, it is that last element that is most relevant to science.
That is beyond the basic construction of the model which presumably
contains some basic scientific hypotheses and thus requires some
understanding and thinking about the system. After that it is the
assessment that provides what is essentially a basis for falsification
of the model, and hence learning..  By saying that the data are
inconsistent with a model, then we're rejecting at least one of the
hypotheses embodied by that model. This is not really true... it could
mean we're missing something, or many things. The basic model could be
correct, just not having enough stuff in it. 

For either Bayesian or classical inference, the basic strategy is to
come up with a fit statistic that depends on the parameters and the
data set, which we denote by $T({\bf y}, \theta)$, and then we compute
this for the observed data set, and then compare its value to that
computed for perfect data sets simulated under the correct model.  In
the case of classical inference, we use a parametric bootstrap where
we simulate data sets conditional on the MLE $\hat{\theta}$. For
Bayesian analysis we use the Bayesian p-value (Gelman et al. XXXXX) in
which data sets are simulated for a posterior sample of $\theta$ which
we briefly introduced in sec. 2.XXXXX.

Using classical inference methods, it is sometimes possible to
identify a test statistic, with known asymptotic distribution, to
evaluate goodness-of-fit using a chi-square statistic. Examples from
the closed capture-recapture setting includes XXXXXX XXXXXXXX.  When
this is not the case, goodness-of-fit is ususally assessed using
bootstrap methods \citep{dixon:2002}. Using a bootstrap method
we identify a fit
statistic and we compute the value for which is more or less the
standard approach, in general.  To date, we are unaware of any
goodness-of-fit applications based on likelihood analysis of SCR
models (XXX WE NEED TO DO SOME RESEARCH ON THIS XXXXX).  We give some
ideas in sec. xxxxxx below.

One challenge with SCR models is there has not been a definitive or 
general proposal for a fit statistic for use in computing the Bayesian
p-value although a few specialized implementations of Bayesian p-values
have been provided (Royle 2009; Gardner et al. 2010 ???).
As a general matter, we recommend use of Bayesian p-values but caution
that there is not general expectation to support how well they should
do, in general. As such, one should expect to do some kind of custom
evaluation for every use of such methods if the ability to reject
under specific departures from the model is of paramount interest.
This is not a weakness of a Bayesian approach because the same issue
applies in doing parametric bootstrapping.




\subsection{The Two Components of Model Fit}

For SCR models,
there are at least two components to model fit for most models,
making an omnibus measure of fit difficult to describe.
First we can consider whether the model explains the observation
process - we can evaluate this based on the encounter frequencies of
individuals {\it conditional} on ${\bf s}_{1}, \ldots, {\bf s}_{N}$
Second, we might also be
interested in evaluating the hypothesis concerning the distribution of
the activity centers. For the simple model developed in this chapter,
this is the assumption of {\it complete spatial randomness} (CSR)
which we consider in section XXX.YYY below. Actually, as we noted in
sec. XXX this is not strictly CSR per se because of the binomial
assumption on $N$ but we refer to it as that because it is essentially
equivalent. And besides we generate the reference distribution within
the context of a Bayesian p-value.

We propose analyzing the separate elements of fit individually.
To evaluate fit of the detection component of the model, we consider
two distinct fit statistics based on the individual encounter
frequencies and also the trap encounter frequencies.

I think inference about Density is going to be insensitive to
departures from CSR but probably somewhat sensitive to bad models for
the observation process.  Why do I think this? Well spatial variation
is not important to inference about the aggregate population size, but
it is relevant to "within population" inference, such as predicting on
small areas.  Conversely, inference about total population size is
known to be highly sensitive to the observation model (Dorazio and
Royle 2003; Link 2003).



\subsection{Testing Uniformity or Spatial Randomness}

Historically, especially in ecology, there has been a huge amount of
interest in whether a realization of a point process indicates
"complete spatial randomness," i.e., that the points are distributed
uniformly and independently in space.  A good reference for such
things is Cressie (1996; ch. 8) and XYZ.XYZ and I also like Tony
Smith's lecture notes (Univ. of Penn. ESE
502)\footnote{
\url{http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf}}. In
the context of animal capture-recapture studies, the CSR hypothesis is
manifestly false, purely on biological grounds. Typically individuals
will be clustered or more uniform (for territorial species) than
expected under spatial randomness. That said, it may be a reasonable
approximation to truth in some situations and
might be largely irrelevant so far as obtaining estimates of density
is concerned (given that we do observe a portion of the population,
$n$).  Furthermore, in realistic sample sizes we might expect
relatively low power to detect departures from CSR although this is a
research question worthy of attention (see section XX.YY).

Before proceeding with the development of a framework for evaluating
the point process model - we note that if $N$ is fixed, the resulting
point process is not, strictly speaking, one of "complete spatial
randomness". This is because when $N$ is fixed, a slight bit of
correlation is induced in the number of points within any particular
subset of the state-space. That said, this is negligible for most
purposes and, besides, we use a simulation based approach to testing
in which we simulate under the appropriate model.  But the point is
that CSR is not really the conventional term - maybe we call this
"uniformity".  The basic technical framework for evaluating the CSR
hypothesis is that the cell counts i.e., precisely those we computed
to produce a density map, should have a binomial distribution and we
can use a standard chi-square goodness-of-fit test to evaluate that.
It will usually suffice to approximate the binomial cell counts by
Poisson cell counts in which case we can use the classical
"index-of-dispersion" test (e.g., Illian et al. 2008; p. 87):
\[
   I =  (cells -1)*s^2/\bar{n}
\]
which has approximately a chi-square on $(ncells - 1)$ df.  If $s^2/nbar
> 1$ this suggests clustering and if $s^2/nbar <1$ then this suggests the
point process is too regular.  I think this test is sensitive to the
number and size of the grid cells chosen and I don't know much about
that but it should be researched.
{\it The} important technical issue is that we don't observe the point
process and so the standard statistics for evaluating CSR cannot be
computed directly.  However, using Bayesian analysis, we do have a
posterior sample of the underlying point process and so we suggest
computing the posterior distribution of the chi-square statistic and
seeing how it compares to 1. As an alternative to the chi-square
statistic based on CSR there are various "nearest-neighbor"
methods. For example EXAMPLE XXXXXXXXX which is also easy to compute.

To evaluate the uniformity assumption (i.e.,
``complete spatial randomness'') for the activity centers, we use a
standard chi-square goodness-of-fit test statistic, based on gridding
the state-space of the point process into $g=1,2,\ldots,G$ cells, and
we tabulated $n(g)$ the number of activity centers in each grid
cell. A standard goodness-of-fit statistic for CSR is based on the
ratio of the variance of $n(g)$ to the mean (see Table 8.3 in Cressie
1996). In particular, in parametric likelihood theory, $I = (G
-1)*s^2/\bar{n}$ should have a chi-square on $(G - 1)$ df under the
CSR hypothesis.  However, we used this statistic as the basis for our
Bayesian p-value calculation, comparing a posterior sample of $I$
computed using each posterior realization of the point process with a
value of $I$ obtained by simulating ${\bf s}$ under CSR.


An issue is that this is probably sensitive to the size of the
state-space. As the size of the state-space increases then the cell
counts {\it are} independent binomial counts with constant density,
and so we can overwhelm the fit statistic with extraneous ``data''
simulated from the posterior, which is equal to the prior as we move
away from the data, and therefore uninformed by the actual data in the
vincinity of the trap array.
Therefore we recoommend computing these fit statistics in the vicinity
of the trap array only.

\subsection{Computing the cell counts}

In chapter 4 we talked about computing summaries of individual
locations, such as for producing a density map. We need these for the
GoF analysis...In the present context, the number of individuals
living in any well-defined polygon is a derived
parameter. Specifically, let $B(x)$ indicate a pixel
 centered at x then
$N(x)=sum_{i} I(s[i] in B(x))$ is the population size of box B(x), and
$D(x) = N(x)/||B(x)||$ is the local density. These are just "derived
parameters" (see Chapt. 2) which are estimated from MCMC output using
the appropriate Monte Carlo average. Note that we are assuming in our
illustration that N is known and so this is easily done by taking all
of the output for MCMC iterations m=1,2,…, and doing this:
\[
   N(x,m) = sum_{z[i,m]=1} I(s[i,m] in B(x))
\]
Thus, N(x,1),N(x,2),, is the Markov chain for parameter N(x).

It is worth emphasizing here that density maps will not usually appear
uniform despite that we have assumed that activity centers are
uniformly distributed because the observed encounters of individuals
provide direct information about the location of the i=1,2,….,n
activity centers and thus their "estimated" locations will be affected
by the observations. In a limiting sense, were we to sample space
intensely enough, every individual would be captured a number of times
and we would have considerable information about all N point
locations. Consequently, the uniform prior would have almost no
influence at all.


\section{Omnibus and Encounter Model Testing}

For the case where there are no time-varying covariates, we can
summarize the data by individual and trap-specific counts
$y_{ij}$. Conditional on ${\bf s}_{i}$,
the expected value under any encounter model is:
\[
 E[y_{ij}] = p_{ij} K
\]
and we can define a fit statistic from the Freeman-Tukey residual
\[
 e_{ij} =  \sqrt{ y_{ij} } - \sqrt{ E(y_{ij}) }
\]
Define
\[
 T_{1}({\bf y}, \theta) = \sum_{i}\sum_{j} e_{ij}^{2}
\]
This is conditional on ${\bf s}$ and conditional on ${\bf z}$ and we
compute this for {\it each} iteration of the MCMC algorithm to provide
an assessment of model fit.
WHY FREEMAN TUKEY XXX FROM CHAPT 2 XXXXXX

Statistic 3: We look also at Trap frequencies

Statistic 2: Individual frequencies

If model has time effects then y[i,j] is binary ... we recommend
computing individual/trap or individual or trap summaries using an
expected value based on $\sum_{k} p_{ijk}$ (Russell et al. 2012).

Probably for choosing among encounter probability models there is no
ability to detect departures from fit for typical sparse data
arrays. This is because individuals are observable in only a few
traps.....

Perhaps behavioral response or other biological models we can have
some power although there needs to be some basic simulation work done
on these problems.


\subsection{Simulation}

Quick sim study with K=10 and K=40 -- N=100, large sample sizes for
sure.
For each model we fitted each of the other 5 models.
Computed each fit statistic, how often do we reject that the model fits?



\section{Some examples: Wolverine}


Wolverine study:
Model selection --- 

Goodness-of-fit
Is SCR0 adequate for the wolverine data?

We used the posterior output from the wolverine model fitted previous
to compute a relatively coarse version of a density map, using a 10 x
10 grid (Figure XXX.YYY) and using a 30 x 30 grid (Figure XXYYZZ). A
couple of things are noteworthy: First is that as we move away from
"where the data live" - away from the trap array - we see that the
density approaches the mean density (lambda = XX per grid cell …
XXX.YYY how is this computed?). This is a property of the estimator
when the "detection function" decreases sufficiently
rapidly. Relatedly, it is also a property of statistical smoothers
such as splines, kernel smoothers, and regression smoothers -
predictions tend toward the global mean as the influence of data
diminishes. Another way to think of it is that it is a consequence of
the prior - which imposes uniformity, and as you get far away from the
data, the predictions tend to the prior. The other thing to note about
this map is that density is not 0 over water. This might be perplexing
to some who are fairly certain that wolverines do not like
water. However, there is nothing about the model that recognizes water
from non-water and so the model predicts over water {\it as if} it
were habitat similar to that within which the array is nested. But,
all of this is ok as far as estimating density goes and, furthermore,
we can compute valid estimates of N over any well-defined region which
presumably wouldn't include water if we so choose.



\begin{comment}


\section{A simulation study under alternatives}

The point of this section is to evaluate whether we can effectively reject goodness of fit under various alternatives. In particular, we consider two specific modes of lack of fit:

(1)	Can we detect a lack of fit in the detection model?

(2)	Can we detect a lack of fit in the point process model?

4 or 5 detection models and 3 ponit process models.

7 x 7 grid, set to up to yield 50 individuals out of 100. Also try 49 pseudo-random or systematic points.


\subsection{Point Process Alternatives}

1.	Extreme regularity
2.	Somewhat? Regular
3.	Extreme clustering (multiple guys with same home range center)
4.	Somewhat clustered
5.	Complete randomness
6.	Spatial drift in the form of a spatial covariate or trend


We conducted a limited simulation study to evaluate the basic SCR
model under point processes that deviate from complete spatial
randomness. (To be concise we might use the term "uniformity" because
the abbreviation CSR looks too much like SCR.) Specifically, we
consider two deviations from complete spatial randomness: clustered
points, and points that are more systematically distributed than under
complete spatial randomness. We evaluate two questions: (1) how
sensitive is the density estimate to violation of complete spatial
randomness? (2) how much power does the GoF test have?


It is clear that there are many possible influences of both power and
effect of tests for CSR.  For example, N and lambda interact to affect
the expected size of the data set (n individuals) and also sigma
interacts with trap configuration and spacing to affect the number of
unique traps that individuals are captured in. It would be impossible
to catalog an exhaustive set of "what ifs" and so, instead, we focus
on the limited situation where N=100 on a state-space with parameters
set to obtain about 44 individuals on average. We simulated encounters
on a 5x5 grid of traps, unit spacing, with a 2 unit buffer defining
the state-space.

We simulated 3 point process models: Model 1 is CSR. Model 2 is a
Poisson cluster process (PCP) and Model 3 is a point process generated
to be more systematically distributed than CSR. For the Poisson
cluster process, 100 "parents" were distributed randomly over the
state space and the number of offspring for each parent was assumed to
be Poisson with mean 4. Offspring locations were generated according
to a bivariate normal distribution around the parent location, with a
standard deviation of 0.5.  This generated a list of many more than
100 final offspring locations within the state-space, and so we kept
the first 100 points within the state-space and discarded the
remaining. Effectively the last cluster is truncated if the cumulative
sum of offspring within the state-space is not precisely equal to 100.
For the systematic point process we generated 100 uniformly
distributed points and used that as a starting point to obtain the
"space-filling design" (Royle and Nychka XXXX) which is implemented in
the R package {\it fields} (Nychka et al XXXX) using the function {\it
  cover.design}.


Things to vary….

Point process…. [random, clustered, regular]   3 levels

Trapping grid: 5 x 5 unit spacing. Also try random traps?   2 levels

State-space: buffered by 2 or 3 units.   2 levels

[N,lambda] fixed at a single value.

[sigma] = .75

GoF grid size = 100 cells, 225 cells, 400 cells with 0, 1 or 2 buffer?

Table:
                      Mode N  Mean N     RejectCSR   meanN(reject) modeN(reject)
CSR
PCP
SYS


4.6.2. GoF under complete spatial randomness.

4.6.3. GoF under the Poisson cluster process.

4.6.4. GoF under the inhibition process.

\end{comment}


\section{ Summary and Outlook  }


There is nothing known at all about power to detect various
departures.  There is some interesting and useful research to be done
on this. Do we have power to detect departures and does it matter in
terms of estimating $N$?

Model fit: other ideas:
Richard brings up a good point which is related to using
cross-validation.  Given that SCR models are a type of model for
spatialy correlation, modeling spatial dependence in counts, similar
to the Wolpert and Ickstadt idea, why not use techniques commonly used
in spatial models for SCR models?  Cross-validation is a standard
method of fitting spatial models such as kriging and splines. So we
could as well use cross-validation based on the TRAP-SPECIFIC
frequencies or some other measure.  We should try that here.

\subsection{What to do if CSR rejected?}

What can we do? We don't know….. depends on the laternative and how sensitive estimates are?









