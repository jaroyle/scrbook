\chapter{
Model Selection and Assessment
}
\markboth{Model Assessment}{}
\label{chapt.gof}

\vspace{.3in}

There are 4 main inference tasks that we do with a model: estimate 
parameters, make predictions of unobserved random variables
(e.g., of density), evaluate the relative 
merits of different models or choose a best model (model selection), 
and we check whether a specific model is reasonable or not (assessment). 
Conceptually, we can think of this last thing as follows: if we simulate 
under that model, do the simulated realizations look like our data? 


In a sense, it is that last element that is most relevant to science.
That is beyond the basic construction of the model which presumably
contains some basic scientific hypotheses and thus requires some
understanding and thinking about the system. After that it is the
assessment that provides what is essentially a basis for falsification
of the model, and hence learning..

The Gelman and Shalizi (2001?)  paper requires extensive citation
here. It is not just that models are wrong but they are manifestly
so. - good quote on that in his paper.  Also the fact that
falsification is the main core of science here.

In this chapter, we focus on the last two of these inference tasks:
model selection (which model or models seem preferred), and model
assessment (do the data seem consistent with a particular model?).
Bayesian model assessment has not been terribly popular in the
literature because it is a bit more tedious computationally and,
furthermore, with complex hierarchical models it is not always so
obvious what should be a decent fit statistic. Conversely, in
classical inference it is sometimes possible to identify a single test
statistic for assessing goodness-of-fit.  While this is more typical
for the most basic models (linear models, ANOVA, frequency data,
etc..), it is also easy, in classical statistics, to assess goodness
of fit using bootstrap methods which is more or less the standard
approach, in general.

In the context of SCR models we focus in this chapter on a couple of 
specific model selection and assessment problems

here on selecting among various models of the detection process which 
could involve different "detection functions", link functions related 
p to s, or additional covariates or other model structure such as a 
behavioral response. 

For model checking or assessment we rely exclusively on Bayesian p-values 
(REF XYZ) to evaluate "goodness-of-fit". Part of the challenge thus far is 
coming up with good fit statistics, we suggest a few here and evaluate them 
by simulation. We break the problem up into 2 components which we attack 
separately (Royle et al. 2011): Does the observation model fit? Does the 
uniformity assumption appear adequate? The latter has a huge amount of 
precedent in the ecological literature as it is equivalent to the test of 
``complete spatial randomness'' which Royle et al. (2011) 
applied to SCR models. Really this is a test of ``spatial randomness''
because CSR really means Poisson point process.

What are the key elements of SCR models?
(1)	Link function 
(2)	Stationary, isotropic, symmetric home range model
(3)	Movement? [sometimes] - "iid" of things
(4)	Covariates or other model structure
(5)	State-space covariates?
(6)	Point process model




\subsection{Referees are pin-heads. Fuck them.}

Referees are quick to point out that bivariate normal distributions aren't 
very realistic models of home range. Whether they are or are not is probably 
immaterial compared to what we can estimate from available SCR data. If 
we really wanted a great description of a home range then we would do 
something different besides conduct an SCR study. The purpose of most 
SCR models is not to study home range geometry and morphology but, rather, 
to estimate density and possibly other vital rates such as survival and 
recruitment. In addition, it is my experience that the same people who 
criticize models as implying overly simple models actually cannot describe 
alternative models except using jargon and procedures like "kernel 
smoothing", "utilization distributions" and "neural networks". So, on 
the one hand, the bivariate normal distribution is overly simple, and 
therefore we should use "neural networks"?  What irritates me even more 
is referees who emphatically assert that "real home ranges aren't bivariate 
normal" and then they go on to cite references who somehow "proved" they 
are not. These people have no clue what statistics is good for and what 
the point of SCR models is, nor can they grasp the relevance of the home 
range model - namely, as a model for explaining heterogeneity in 
capture-probability. To be sure, the bivariate normal model is an 
over-simplification but so far it has not been shown to be ineffective 
in SCR problems, and besides  substantially more flexible models have been 
developed \citep{royle_etal:2012ecol}. 

What we care about in models of capture-recapture data is whether the 
bivariate normal (or any model) provides an adequate description of the 
encounter process. That is the question we will evaluate here. 

We're not saying that we disagree with complex models, just that they don't 
really exist in a usable form. This seems like a good research area. 

Secondly, with typical SCR data sets, we will typically have insufficient 
power to fit or choose among models that contain more than several 
parameters.



\section{Model Selection}

If you're doing a classical analysis based on likelihood then model
selection is easily accomplished using AIC \citep{burnham_anderson:xxxx}
which we demonstrate below. Model selection is somewhat more difficult 
as a Bayesian and there is no such canned all-purpose method like AIC.  
As such we recommend more of a pragmatic approach, in general, for all
problems, based on a number of basic considerations:
\begin{itemize}
\item[(1)] For a small number of fixed effects we recommend a standard 
hypothesis testing ideas -- i.e., if the posterior for a parameter overlaps 
zero or whatever then toss out the effect
\item[(2)] Posterior model probabilities: In some cases we can implement 
the "variable weights" idea [not really sure who to credit for this]. 
The idea is introduce a latent variable $w \sim Bern(.5)$ and then 
construct  your model according to:  
\[
 logit(p) = a + w*b*covariate  
\]
There are examples in Royle and Dorazio (2008) in chapter 3 and later in 
the CJS chapter (perhaps Marc has an example in the new BPA book?)
\item[(3)] DIC: The official word on DIC is that it might not work so 
well, but its hard to say in general. I recommend using it if it leads to 
sensible results but also calibrate it for specific problems.
As always, we 
think a small simulation study would be valuable for any particular case. 
We need to do that here.
\item[(4)] Logical argument: For something like M/F differences it seems 
to make sense to leave the extra parameters in the model no matter what, 
because of course you expect , biologically, there to be a difference in 
these things. Thus the hypothesis test itself is meaningless (Doug Johnson 
wrote a paper on this kind of stuff once) and you would only reject the 
null because of low power anyhow.
\end{itemize}

In all modeling activities, the use of logical argument should not be under-utilized.


\subsection{AIC and DIC}

What is AIC? What is the DIC for a SCR model? The models are just 
binomial or Poisson regressions and so we can define the DIC as
 -loglikelihood().  But what about the data augmentation part?
The current Conventional Wisdom is that DIC doesn't really work so 
well in hierarchical models (    ). But there doesn't really seem to be 
general theory to support this. It seems maybe like its ok sometimes. So 
we take a look at its use here to pick among fixed effects models. 




\subsection{Choosing detection models}

The idea is that maybe we should do a very directed simulation 
study to evaluate selection based on DIC, AIC and also the "variable 
weights".
I have a 2x2 factorial arrangement in mind for a 7 x 7 grid of traps
with unit spacing:
\begin{verbatim}
                     N=100              N=200

sigma = 0.50   low spatial recaps      low spatial recaps
               low n                   high n
sigma = 1.00   high spatial recaps     high/high
               low n
\end{verbatim}

There could be two dimensions to this study:

(1)	Selection from among different detection models. 
E.g., Comparison of logit vs. cloglog and linear vs quadratic vs. 
log() distance functions.  That's 6 total models, for 
delta = 2, 3 and 4.  Why should DIC work?

(3)	For each unique detection model, we simulate data under that model 
and fit all of the other models and compute DIC, variable weights and 
perhaps use likelihood to compute AIC and see how the results differ.

Since all the detection models give about the same answer there's no
point in acting like we're engaging in some profound act of pure
science by doing model selection on detection functions. 


\subsection{Choosing Biological models}

Behavior, time of year, trap type (biated or not), things that make a
biological difference we should be able to choose among

Different model structures: e.g., with/without behavioral response. 
With/without some trap covariate. Other things?

The expectation is that it should make a big difference for covariate
effects such as Mb and so forth which has been demonstrated in
practice repeatedly. So can we choose among those models effectively?


\subsection{Model selection using posterior model probabilities}

This is ok for covariates. Do an example. 

Could also do the indicator function for different models if the 
w[i] = (0,0,1,0) a multinomial draw.









\section{Evaluating Goodness-of-Fit}


It is sometimes useful to provide an evaluation of model fit. For SCR 
models it is usually possible to devise such an evaluation using the 
so-called Bayesian p-value (Gelman XYZ.XYZ) approach (see section 2.XYZ). 
However, to the best of our knowledge there has not been a definitive or 
general proposal for a fit statistic for use in computing the Bayesian 
p-value although a few specialized implementations of Bayesian p-values 
have been provided (Royle 2009; Gardner et al. 2010 ???).  

One reason that this is challenging in the context of SCR models is that 
there are at least two components to model fit for most models (but 
sometimes more), making an omnibus measure of fit difficult to describe. 
First we can consider whether the model explains the observation 
process - we can evaluate this based on the encounter frequencies of
individuals {\it conditional} on s[1], …, s[N]. Second, we might also be 
interested in evaluating the hypothesis concerning the distribution of the activity centers. For the simple model developed in this chapter, this is the assumption of {\it complete spatial randomness} (CSR) which we consider in section XXX.YYY below.  We propose analyzing the separate elements of fit individually.  
To evaluate fit of the detection component of the model, we consider two distinct fit statistics based on the individual encounter frequencies and also the trap encounter frequencies……
DETAILS HERE……
I think inference about Density is going to be insensitive to departures from CSR but probably somewhat sensitive to bad models for the observation process.  Why do I think this? Well spatial variation is not important to inference about the aggregate population size, but it is relevant to "within population" inference, such as predicting on small areas.  Conversely, inference about total population size is known to be highly sensitive to the observation model (Dorazio and Royle 2003; Link 2003).
4.5. Testing Complete Spatial Randomness (CSR)
Historically, especially in ecology, there has been a huge amount of interest in whether a realization of a point process indicates "complete spatial randomness," i.e., that the points are distributed uniformly and independently in space.  A good reference for such things is Cressie (1996; ch. 8) and XYZ.XYZ and I also like Tony Smith's lecture notes (Univ. of Penn. ESE 502)\footnote{http://www.seas.upenn.edu/~ese502/NOTEBOOK/Part_I/3_Testing_Spatial_Randomness.pdf}. In the context of animal capture-recapture studies, the CSR hypothesis is manifestly false, purely on biological grounds. Typically individuals will be clustered or more uniform (for territorial species) than expected under spatial randomness. That said, it may be a reasonable approximation to truth in some situations and, as we noted above, might be largely irrelevant so far as obtaining estimates of density is concerned. Furthermore, in realistic sample sizes we might expect relatively low power to detect departures from CSR although this is a research question worthy of attention (see section XX.YY).
Before proceeding with the development of a framework for evaluating the point process model - we note that if $N$ is fixed, the resulting point process is not, strictly speaking, one of "complete spatial randomness". This is because when $N$ is fixed, a slight bit of correlation is induced in the number of points within any particular subset of the state-space. That said, this is negligible for most purposes and, besides, we use a simulation based approach to testing in which we simulate under the appropriate model.  But the point is that CSR is not really the conventional term - maybe we call this "uniformity".
The basic technical framework for evaluating the CSR hypothesis is that the cell counts i.e., precisely those we computed to produce a density map, should have a binomial distribution and we can use a standard chi-square goodness-of-fit test to evaluate that.  It will usually suffice to approximate the binomial cell counts by Poisson cell counts in which case we can use the classical "index-of-dispersion" test (e.g., Illian et al. 2008; p. 87):
\[
   I =  (cells -1)*s^2/nbar 
\]
which has approximately a chi-square on (ncells - 1) df.  If s^2/nbar > 1 this suggests clustering and if s^2/nbar <1 then this suggests the point process is too regular.  I think this test is sensitive to the number and size of the grid cells chosen and I don't know much about that but it should be researched.

{\it The} important technical issue is that we don't observe the point process and so the standard statistics for evaluating CSR cannot be computed directly.  However, using Bayesian analysis, we do have a posterior sample of the underlying point process and so we suggest computing the posterior distribution of the chi-square statistic and seeing how it compares to 1. As an alternative to the chi-square statistic based on CSR there are various "nearest-neighbor" methods. For example EXAMPLE XXXXXXXXX which is also easy to compute. 

To evaluate the uniformity assumption (i.e.,
``complete spatial randomness'') for the activity centers, we used a
standard chi-square goodness-of-fit test statistic, based on gridding
the state-space of the point process into $g=1,2,\ldots,G$ cells, and
we tabulated $n(g)$ the number of activity centers in each grid
cell. A standard goodness-of-fit statistic for CSR is based on the
ratio of the variance of $n(g)$ to the mean (see Table 8.3 in Cressie
1996). In particular, in parametric likelihood theory, $I = (G
-1)*s^2/\bar{n}$ should have a chi-square on $(G - 1)$ df under the
CSR hypothesis.  However, we used this statistic as the basis for our
Bayesian p-value calculation, comparing a posterior sample of $I$
computed using each posterior realization of the point process with a
value of $I$ obtained by simulating ${\bf s}$ under CSR.

\subsection{Computing the cell counts}
In chapter 4 we talked about computing summaries of individual locations, such as for producing a density map. We need these for the GoF analysis…..In the present context, the number of individuals living in any well-defined polygon is a derived parameter. Specifically, let B(x) indicate a box centered at x then N(x)=sum_{i} I(s[i] in B(x)) is the population size of box B(x), and D(x) = N(x)/||B(x)|| is the local density. These are just "derived parameters" (see Chapt. 2) which are estimated from MCMC output using the appropriate Monte Carlo average. Note that we are assuming in our illustration that N is known and so this is easily done by taking all of the output for MCMC iterations m=1,2,…, and doing this:
\[
   N(x,m) = sum_{z[i,m]=1} I(s[i,m] in B(x))
\]
Thus, N(x,1),N(x,2),…, is the Markov chain for parameter N(x).  

It is worth emphasizing here that density maps will not usually appear uniform despite that we have assumed that activity centers are uniformly distributed because the observed encounters of individuals provide direct information about the location of the i=1,2,….,n activity centers and thus their "estimated" locations will be affected by the observations. In a limiting sense, were we to sample space intensely enough, every individual would be captured a number of times and we would have considerable information about all N point locations. Consequently, the uniform prior would have almost no influence at all. 

\subsection{What to do if CSR rejected?}
What can we do? We don't know….. depends on the laternative and how sensitive estimates are?
\section{Is SCR0 adequate for the wolverine data?} 

We used the posterior output from the wolverine model fitted previous to compute a relatively coarse version of a density map, using a 10 x 10 grid (Figure XXX.YYY) and using a 30 x 30 grid (Figure XXYYZZ). A couple of things are noteworthy: First is that as we move away from "where the data live" - away from the trap array - we see that the density approaches the mean density (lambda = XX per grid cell … XXX.YYY how is this computed?). This is a property of the estimator when the "detection function" decreases sufficiently rapidly. Relatedly, it is also a property of statistical smoothers such as splines, kernel smoothers, and regression smoothers - predictions tend toward the global mean as the influence of data diminishes. Another way to think of it is that it is a consequence of the prior - which imposes uniformity, and as you get far away from the data, the predictions tend to the prior. The other thing to note about this map is that density is not 0 over water. This might be perplexing to some who are fairly certain that wolverines do not like water. However, there is nothing about the model that recognizes water from non-water and so the model predicts over water {\it as if} it were habitat similar to that within which the array is nested. But, all of this is ok as far as estimating density goes and, furthermore, we can compute valid estimates of N over any well-defined region which presumably wouldn't include water if we so choose. 
 


\section{Is SCR0 adequate for the Fort Drum black bear data?} 

\section{A simulation study under alternatives}
The point of this section is to evaluate whether we can effectively reject goodness of fit under various alternatives. In particular, we consider two specific modes of lack of fit:
(1)	Can we detect a lack of fit in the detection model?
(2)	Can we detect a lack of fit in the point process model?
4 or 5 detection models and 3 ponit process models.
7 x 7 grid, set to up to yield 50 individuals out of 100. Also try 49 pseudo-random or systematic points.


\subsection{Point Process Alternatives}
1.	Extreme regularity
2.	Somewhat? Regular
3.	Extreme clustering (multiple guys with same home range center)
4.	Somewhat clustered
5.	Complete randomness
6.	Spatial drift in the form of a spatial covariate or trend


We conducted a limited simulation study to evaluate the basic SCR model under point processes that deviate from complete spatial randomness. (To be concise we might use the term "uniformity" because the abbreviation CSR looks too much like SCR.) Specifically, we consider two deviations from complete spatial randomness: clustered points, and points that are more systematically distributed than under complete spatial randomness. We evaluate two questions:  (1) how sensitive is the density estimate to violation of complete spatial randomness? (2) how much power does the GoF test have? 
It is clear that there are many possible influences of both power and effect of tests for CSR.  For example, N and lambda interact to affect the expected size of the data set (n individuals) and also sigma interacts with trap configuration and spacing to affect the number of unique traps that individuals are captured in. It would be impossible to catalog an exhaustive set of "what ifs" and so, instead, we focus on the limited situation where N=100 on a state-space with parameters set to obtain about 44 individuals on average. We simulated encounters on a 5x5 grid of traps, unit spacing, with a 2 unit buffer defining the state-space.  We simulated 3 point process models: Model 1 is CSR. Model 2 is a Poisson cluster process (PCP) and Model 3 is a point process generated to be more systematically distributed than CSR. For the Poisson cluster process, 100 "parents" were distributed randomly over the state space and the number of offspring for each parent was assumed to be Poisson with mean 4. Offspring locations were generated according to a bivariate normal distribution around the parent location, with a standard deviation of 0.5.  This generated a list of many more than 100 final offspring locations within the state-space, and so we kept the first 100 points within the state-space and discarded the remaining. Effectively the last cluster is truncated if the cumulative sum of offspring within the state-space is not precisely equal to 100.  For the systematic point process we generated 100 uniformly distributed points and used that as a starting point to obtain the "space-filling design" (Royle and Nychka XXXX) which is implemented in the R package {\it fields} (Nychka et al XXXX) using the function {\it cover.design}.
Things to vary….
Point process…. [random, clustered, regular]   3 levels 
Trapping grid: 5 x 5 unit spacing. Also try random traps?   2 levels 
State-space: buffered by 2 or 3 units.   2 levels
[N,lambda] fixed at a single value.
[sigma] = .75 
GoF grid size = 100 cells, 225 cells, 400 cells with 0, 1 or 2 buffer?

Table:
                      Mode N  Mean N     RejectCSR   meanN(reject) modeN(reject)
CSR
PCP
SYS


4.6.2. GoF under complete spatial randomness.
4.6.3. GoF under the Poisson cluster process.
4.6.4. GoF under the inhibition process.














4.7. Summary and Outlook  
We saw that the basic SCR model is a type of binomial (logistic regression) with an individual "random effect" that has a uniform distribution over space. Thus, this model is not much more than an ordinary capture-recapture model for closed populations -- it is simply that model augmented with a set of "individual effects", s[i], which relate some sense of individual location to encounter probability. Formal consideration of the collection of individual locations (s[1], …,s[N]) in the model is fundamental to all of the models considered in this book. 
In statistical terminology, we think of the collection of points { s[i] } as a realization of a point process and part of the promise, and ongoing challenge, of SCR models is to develop interest point process models. For SCR0, we considered the simplest possible point process model - the points are independent and uniformly ("randomly") distributed over space. Despite the simplicity of this assumption, it should suffice in many applications of SCR models although we do address generalizations of this model in later chapters. Moreover, even though the {\it prior} distribution on the point locations is uniform, the realized pattern may deviate markedly from uniformity and the observed encounter data provide information to impart deviations from uniformity. Thus, the estimated density map will typically appear distinctly non-uniform.  We showed how to conduct inference about the underlying point process including calculation of density maps, and evaluating "complete spatial randomness" from posterior output. We can do other things we normally do with spatial point processes such as compute K-functions, although we have not pursued these things here. Modifying and applying such methods to SCR problems seems to use like a fruitful area of research.
A point we tried to emphasize in this chapter is that the basic SCR model is a type of individual covariate model - but with imperfect information about the individual covariate. They are GLMM type models when N is known or, when N is unknown, they are zero-inflated GLMMs (see Royle 2006).  These models are really quite easy to analyze by likelihood methods, based on the integrated likelihood, and they are also very easy to analyze using existing MCMC black boxes such as WinBUGS or JAGS and possibly other packages.  We will consider likelihood analysis of such models sparingly in this book because our emphasis is on Bayesian analysis which we think is more flexible and, in practical problems, inferences are more rigorous. 
An obvious question that might be floating around in your mind is why should we ever go through all of this trouble when we could just MARK or CAPTURE to get an estimate of N and apply ½ MMDM methods?  That's a good question. The main reason is that these methods are predicated on models that are blatent misspecifications - they are wrong! Or perhaps more charitably, they are models of the wrong system. They do not account for trap identity. They don't account for spatial organization or "clustering" of individual encounters. And, "density" is not a parameter of those models because density has no meaning absent an explicit representation of space. Conversely, the SCR model is a model for trap-specific encounter data - how individuals are organized in space and interact with traps. SCR models provide a coherent framework for inference about density or population size and also, because of the formality of their derivation, can be extended and generalized to a large variety of different situations, as we demonstrate in subsequent chapters.
	In the next few chapters we continue to work with this basic SCR design and model but consider some important extensions of the basic model. We consider technical details of Bayesian and maximum likelihood estimation in the following chapter, and then extensions to include covariates that vary by individual, trap, or over time (chapter XXX.YYY).
Technical Things of some importance:
We can think of SCR models as a type of model with heterogeneous detection probability, p, where the heterogeneity derives from distance, d, between individual and trap. The marginal pdf of the random variable "p" is therefore
Pr(p) = Pr(p|d)Pr(d)
Obviously Pr(d) depends on geometry of S. Thus choice of S is equivalent to choice of a mixing distribution for p. Actually do this in terms of Pr(p|s)Pr(s) instead of d - d is too indirect and doesn't add anything.
Link's result... [p|captureable]...... obviously as S increases then
you get a point mass developng near 0......... is this relevant to anything?
The 2nd thing to talk about here (related) is that D is invariant to the state-space as long as p(d) is well-behaved.
